from __future__ import annotations

import os
from typing import Any

import evaluate

from flexeval.core.utils.jinja2_env import JINJA2_ENV

from .base import Metric, MetricResult
from .normalizer import Normalizer

# by default, the program is not allowed to execute code and we need to set this environment variable
os.environ["HF_ALLOW_CODE_EVAL"] = "1"


class CodeEval(Metric):
    """
    A metric that evaluates generated code with test cases.

    Args:
        code_prompt_template: A Jinja2 template string that will prepend the generated code.
            The template should contain variables that will be replaced with the values in `task_inputs_list`.
            If `None`, the code prompt will be the generated code itself.
        normalizer: A normalizer applied to model outputs before evaluation.
    """

    def __init__(self, code_prompt_template: str | None = None, normalizer: Normalizer | None = None) -> None:
        self._code_prompt_template = None
        if code_prompt_template is not None:
            self._code_prompt_template = JINJA2_ENV.from_string(
                code_prompt_template,
            )
        self._code_eval = evaluate.load("code_eval")
        self._normalizer = normalizer

    def evaluate(
        self,
        lm_outputs: list[str],
        references_list: list[list[str]],
        task_inputs_list: list[dict[str, str]] | None = None,
    ) -> MetricResult:
        if task_inputs_list is None:
            task_inputs_list = [{} for _ in lm_outputs]

        generated_functions: list[str] = []
        test_case_list: list[str] = []
        # in code generation tasks, references_list contains the test cases
        for lm_output, task_inputs, test_cases in zip(
            lm_outputs,
            task_inputs_list,
            references_list,
        ):
            if self._normalizer is not None:
                lm_output = self._normalizer.normalize(lm_output)  # noqa: PLW2901

            generated_function = lm_output
            if self._code_prompt_template is not None:
                generated_function = self._code_prompt_template.render(**task_inputs) + lm_output

            generated_functions.append(generated_function)
            test_case_list.append("\n".join(test_cases))
        pass_at_k, results = self._code_eval.compute(
            references=test_case_list,
            predictions=[[f] for f in generated_functions],
            k=[1],
        )

        # `results` contain the detailed results for each test case
        # e.g., {0: [(0, {'task_id': 0, 'passed': False, 'result': "failed", 'completion_id': 0})]}
        results: dict[int, list[tuple[int, dict[str, Any]]]]

        instance_details: list[dict[str, Any]] = []
        for i in range(len(lm_outputs)):
            first_result = results[i][0]  # we only assume one candidate code per instance, so we take the first result
            _, detail_result = first_result  # the first element is just the index so we ignore it
            # remove unnecessary fields to save space
            detail_result.pop("completion_id")
            detail_result.pop("task_id")
            instance_details.append(detail_result)

        return MetricResult(pass_at_k, instance_details=instance_details)
