{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FlexEval","text":"<p>Flexible evaluation tool for language models. Easy to extend, highly customizable!</p> <p>With FlexEval, you can evaluate language models with:</p> <ul> <li>Zero/few-shot prompt tasks</li> <li>Open-ended text-generation benchmarks such as MT-Bench with automatic evaluation using GPT-4</li> <li>Log-probability-based multiple-choice tasks</li> <li>Computing perplexity of text data</li> </ul> <p>... and more!</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Flexibility: <code>flexeval</code> is flexible in terms of the evaluation setup and the language model to be evaluated.</li> <li>Modularity: The core components of <code>flexeval</code> are easily extensible and replaceable.</li> <li>Clarity: The results of evaluation are clear and all the details are saved.</li> <li>Reproducibility: <code>flexeval</code> should be reproducible, with the ability to save and load configurations and results.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install flexeval\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>The following minimal example evaluates the hugging face model <code>sbintuitions/tiny-lm</code> with the <code>commonsense_qa</code> task.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa\" \\\n  --save_dir \"results/commonsense_qa\"\n</code></pre> <p>(The model used in the example is solely for debugging purposes and does not perform well. Try switching to your favorite model!)</p> <p>The results saved in <code>--saved_dir</code> contain:</p> <ul> <li><code>config.json</code>: The configuration of the evaluation, which can be used to replicate the evaluation.</li> <li><code>metrics.json</code>: The evaluation metrics.</li> <li><code>outputs.jsonl</code>: The outputs of the language model that comes with instance-level metrics.</li> </ul> <p>You can flexibly customize the evaluation by specifying command-line arguments or configuration files. Besides the Transformers model, you can also evaluate models via OpenAI ChatGPT and vLLM, and other models can be readily added!</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Run <code>flexeval_presets</code> to check the list of off-the-shelf presets in addition to <code>commonsense_qa</code>. You can find the details in the Preset Configs section.</li> <li>See Getting Started to check the tutorial examples for other kinds of tasks.</li> <li>See the Configuration Guide to set up your evaluation.</li> </ul>"},{"location":"configuration_guide/","title":"How to configure your own evaluation","text":""},{"location":"configuration_guide/#overview","title":"Overview","text":"<p><code>flexeval</code> allows you to evaluate any language model with any task, any prompt, and any metric via the <code>flexeval_lm</code> command. The CLI command is implemented on jsonargparse, which allows a flexible configuration either by CLI arguments or by a configuration file.</p> <p>There are many ways to write configuration files, but for now let's see how to define a config for the argument <code>--eval_setup</code>. You can check the configuration for preset setups by running the following command:</p> <pre><code>flexeval_presets commonsense_qa\n</code></pre> <p>This command will show the configuration for the <code>commonsense_qa</code> setup. The content is written in the jsonnet format, which is a superset of JSON.</p> <p>Tip</p> <p>If you want to convert it to JSON, install <code>jsonnet</code> command and run <code>flexeval_presets commonsense_qa | jsonnet -</code>.</p> <p>The skeleton of the configuration is as follows:</p> <pre><code>{\n  \"class_path\": \"Generation\",\n  \"init_args\": {\n    \"eval_dataset\": {\"class_path\": \"HFGenerationDataset\", \"init_args\": ...},\n    \"prompt_template\": {\"class_path\": \"Jinja2PromptTemplate\", \"init_args\": ...},\n    \"gen_kwargs\": {\"max_new_tokens\": 32, \"stop_sequences\": [\"\u300d\"]},\n    \"metrics\": [{\"class_path\": \"CharF1\"}, {\"class_path\": \"ExactMatch\"}],\n    \"batch_size\": 4\n  }\n}\n</code></pre> <p>The fields <code>class_path</code> and <code>init_args</code> directly mirror the initialization of the specified class.</p> <p>At the top level, <code>\"class_path\": \"Generation\"</code> specifies what kind of <code>EvalSetup</code> to use. Currently, there are four types of <code>EvalSetup</code>: <code>Generation</code>, <code>ChatResponse</code>, <code>MultipleChoice</code>, and <code>Perplexity</code>.</p> <p>Then, <code>Generation</code> is composed of the following components:</p> <ul> <li><code>eval_dataset</code>: The dataset to evaluate. You can choose from concrete classes inheriting <code>GenerationDataset</code>. Most presets use <code>HFGenerationDataset</code>, which load datasets from Hugging Face Hub.</li> <li><code>prompt_template</code>: The template to generate prompts fed to the language model. We have <code>Jinja2PromptTemplate</code>, which uses Jinja2 to embed the data from <code>GenerationDataset</code> into the prompt.</li> <li><code>gen_kwargs</code>: The keyword arguments passed to <code>LanguageModel.batch_complete_text</code>. For example, <code>max_new_tokens</code> and <code>stop_sequences</code> are used to control the generation process. Acceptable arguments depend on the underlying implementation of the generation function (e.g., <code>generate()</code> in <code>transformers</code>).</li> <li><code>metrics</code>: The metrics to compute. You can choose from concrete classes inheriting <code>Metric</code>. These modules take the outputs of the language model, the references, and dataset values, and compute the metrics.</li> </ul> <p>Please refer to the API reference for available classes and their arguments.</p>"},{"location":"configuration_guide/#customizing-the-configuration","title":"Customizing the Configuration","text":"<p>Writing a configuration file from scratch is a bit cumbersome, so we recommend starting from the preset configurations and modifying them as needed.</p> <pre><code>flexeval_presets commonsense_qa &gt; my_config.jsonnet\n</code></pre> <p>Then, pass your config file to <code>--eval_setup</code> argument.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"my_config.jsonnet\"\n</code></pre> <p>Info</p> <p>Under the hood, the preset name like <code>commonsense_qa</code> is resolved to the corresponding configuration file under <code>flexeval/preset_configs</code> in the library.</p>"},{"location":"configuration_guide/#argument-overrides","title":"Argument Overrides","text":"<p>jsonargparse allows you to flexibly combine configuration files and CLI arguments. You can override the argument values by specifying them in the CLI.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa\" \\\n  --eval_setup.batch_size 8\n</code></pre> <p>The value of <code>--eval_setup.batch_size</code> overrides the value defined in the config file of <code>commonsense_qa</code>.</p>"},{"location":"configuration_guide/#whats-next","title":"What's Next?","text":"<ul> <li>Proceed to How to to find examples that suit your needs.</li> <li>Look at the API reference to see the available classes and their arguments.</li> </ul>"},{"location":"design_principles/","title":"Design Principle","text":"<p><code>flexeval</code> is designed according to the following principles:</p> <ul> <li>Flexibility: <code>flexeval</code> should be flexible in terms of the evaluation setup and the language model to be evaluated.</li> <li>Modularity: The core components of <code>flexeval</code> should be easily extensible and replaceable.</li> <li>Clarity: The results of evaluation should be clear and easy to understand its configuration.</li> <li>Reproducibility: <code>flexeval</code> should be reproducible, with the ability to save and load configurations and results.</li> </ul> <p>To achieve flexibility and modularity, the core logic is implemented with abstract interfaces, and the concrete implementations are provided when running each CLI command.</p> <p>Thanks to jsonargparse, we can transparently specify the configuration of every component either via CLI arguments or jsonnet config files. Thus, when you want to use your own module, all you have to do is implement a concrete class inheriting the right interface and specify it in the configuration, without modifying the existing code.</p> <p>To achieve clarity and reproducibility, <code>flexeval</code> saves the configuration and the evaluation results in a directory specified by <code>--save_dir</code>. The resulting <code>config.json</code> file contains everything needed to replicate the evaluation, configuration of all modules, the version of <code>flexeval</code> and the installed packages.</p> <p>It is often a case that a small preprocessing in the data affects the evaluation results significantly. We would like to the config file tells us what preprocessing is done without we need to dig into the code. Thus we recommend loading datasets using a generic class such as <code>HFGenerationDataset</code> or <code>JsonlGenerationDataset</code> and specifying a preprocessing using their parameters or Jinja2 templates in the configuration file.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Most evaluations can be done with the <code>flexeval_lm</code> command. With <code>--eval_setup</code> option, you can specify the task to evaluate.</p>"},{"location":"getting_started/#generation-tasks","title":"Generation Tasks","text":"<p>The following minimal example evaluates the hugging face model <code>sbintuitions/tiny-lm</code> with the <code>commonsense_qa</code> task.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa\" \\\n  --save_dir \"results/commonsense_qa\"\n</code></pre> <p>The results are saved in the directory specified by <code>--save_dir</code>.</p> <p>You can find three files: <code>config.json</code>, <code>metrics.json</code> and <code>outputs.jsonl</code>.</p>"},{"location":"getting_started/#configjson","title":"<code>config.json</code>","text":"<p>The <code>config.json</code> file contains the configuration of the evaluation, as well as metadata useful for replicating the evaluation.</p> <pre><code>{\n    \"eval_setup\": {\n        \"class_path\": \"flexeval.scripts.flexeval_lm.Generation\",\n        \"init_args\": {\n          \"eval_dataset\": ...,\n          \"prompt_template\": ...,\n          \"gen_kwargs\": ...,\n          \"metrics\": ...,\n          \"batch_size\": ...,\n        },\n    },\n    \"language_model\": {\n      \"class_path\": \"flexeval.core.language_model.HuggingFaceLM\",\n      \"init_args\": {\n        \"model\": \"sbintuitions/tiny-lm\",\n        ...\n      }\n    },\n    \"save_dir\": \"results/commonsense_qa\",\n    \"metadata\": ...\n}\n</code></pre> <p>Tip</p> <p>You can replicate the evaluation by specifying the saved config in <code>flexeval_lm</code>:</p> <pre><code>flexeval_lm --config \"results/commonsense_qa/config.json\" --save_dir \"results/commonsense_qa_replicated\"\n</code></pre>"},{"location":"getting_started/#metricsjson","title":"<code>metrics.json</code>","text":"<p>The <code>metrics.json</code> file contains the evaluation metrics.</p> <pre><code>{\n    \"exact_match\": 0.004914004914004914,\n}\n</code></pre>"},{"location":"getting_started/#outputsjsonl","title":"<code>outputs.jsonl</code>","text":"<p>The <code>outputs.jsonl</code> file contains the outputs of the language model with the following fields:</p> <ul> <li><code>lm_prompt</code>: The prompt used to generate the output.</li> <li><code>lm_output</code>: The output generated by the language model.</li> <li><code>task_inputs</code>: The inputs of the task.</li> <li><code>references</code>: The references of the task.</li> <li>instance-level metrics (e.g., <code>exact_match</code>): The metrics computed for each instance.</li> </ul>"},{"location":"getting_started/#multiple-choice-tasks","title":"Multiple Choice Tasks","text":"<p>Some tasks are implemented as multiple choice tasks. The following example evaluates the model with the <code>commonsense_qa_mc</code> setup, which solves CommonsenseQA by choosing the answer with the highest probability.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa_mc\" \\\n  --save_dir \"results/commonsense_qa_mc\"\n</code></pre> <p>The results are basically the same as the generation tasks, but the <code>outputs.jsonl</code> file has a different format:</p> <ul> <li><code>prefix</code>: The prefix text before the choices.</li> <li><code>choices</code>: The choices of the task.</li> <li><code>answer_index</code>: The index of the correct choice.</li> <li><code>log_probs</code>: The log probabilities of each choice computed by the language model.</li> <li><code>prediction</code>: The index of the choice with the highest probability.</li> <li><code>byte_norm_log_probs</code>: The byte-normalized log probabilities of each choice.</li> <li><code>byte_norm_prediction</code>: The index of the choice with the highest byte-normalized probability.</li> </ul> <p>Whether to use <code>log_probs</code> or <code>byte_norm_log_probs</code> depends on the task, so both are provided.</p>"},{"location":"getting_started/#chat-models","title":"Chat Models","text":"<p>The examples so far are intended to evaluate pretrained language models in zero/few-shot settings. Evaluating chat models may require a different setup.</p> <pre><code>export OPENAI_API_KEY=\"YOUR_API_KEY\"\n\nflexeval_lm \\\n  --language_model OpenAIChatAPI \\\n  --language_model.model \"gpt-3.5-turbo\" \\\n  --eval_setup \"mt-en\" \\\n  --save_dir \"results/mt-en/gpt-3.5-turbo\"\n</code></pre> <p>Note</p> <p>You can also specify <code>HuggingFaceLM</code> for <code>--language_model</code> but the model should have a proper chat template.</p> <p><code>outputs.jsonl</code> contains the following fields:</p> <ul> <li><code>lm_output</code>: The response generated by the language model.</li> <li><code>task_inputs</code>: The inputs of the task.</li> <li><code>messages</code>: The chat history except for the last turn.</li> <li><code>references</code>: The references of the task, if any.</li> <li>instance-level metrics (e.g., <code>output_length</code>): The metrics computed for each instance.</li> </ul> <p>Usually, the model outputs are evaluated by human evaluation or another LLM. The preset config only defines simple metrics such as length statistics.</p> <p>To run automatic evaluation with LLMs, you can use <code>outputs.jsonl</code> from the previous command and run the following command:</p> <pre><code>flexeval_file \\\n  --eval_file \"results/mt-en/gpt-3.5-turbo/outputs.jsonl\" \\\n  --metrics \"assistant_eval_en_single_turn\" \\\n  --save_dir \"results/mt-en/gpt-3.5-turbo/eval_by_gpt\"\n</code></pre> <p>In the results, you can see the evaluation result like <code>{\"llm_score\": 7.795}</code>. You can also check the entire output of the judge LLM including the rationale of the evaluation in <code>llm_score_output</code> in <code>outputs.jsonl</code>.</p> <p>For further details and pairwise evaluation, see Evaluate with LLM Judges.</p>"},{"location":"getting_started/#perplexity","title":"Perplexity","text":"<p>You can also compute perplexity of text with the following command:</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"tiny_shakespeare\" \\\n  --save_dir \"results/tiny_shakespeare\"\n</code></pre> <p>When evaluating perplexity, there is no <code>outputs.jsonl</code> file. The <code>metrics.json</code> file contains the perplexity values normalized by the number of tokens.</p> <pre><code>{\n    \"perplexity_per_byte\": 9.080868808532346,\n    \"perplexity_per_character\": 9.080868808532346\n}\n</code></pre> <p>Tip</p> <p>You can get <code>perplexity_per_token</code> by specifying the <code>--tokenizer</code> option.  By default, the command only computes tokenizer-agnostic metrics.</p>"},{"location":"getting_started/#whats-next","title":"What's Next?","text":"<ul> <li>Run <code>flexeval_presets</code> to check the list of off-the-shelf presets. You can find the details in the Preset Configs section.</li> <li><code>flexeval</code> allows you to evaluate any language model with any task, any prompt, and any metric. To understand how to configure the evaluation, proceed to Configuration Guide.</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p><code>flexeval</code> is tested on Python 3.8+.</p>"},{"location":"installation/#install-from-pip","title":"Install from pip","text":"<pre><code>pip install flexeval\n</code></pre> <p>Extras dependencies can be installed via pip install -e \".[NAME]\".</p> Name Description vllm To load language models using vLLM."},{"location":"installation/#install-from-source","title":"Install from source","text":"<pre><code>git clone https://github.com/sbintuitions/flexeval\ncd flexeval\npip install -e .\n</code></pre>"},{"location":"installation/#install-with-docker","title":"Install with Docker","text":"<pre><code>git clone https://github.com/sbintuitions/flexeval\ncd flexeval\ndocker build -t flexeval .\n</code></pre>"},{"location":"api_reference/","title":"API Reference","text":"<ul> <li>ChatDataset</li> <li>EvalSetup</li> <li>FewShotGenerator</li> <li>GenerationDataset</li> <li>HFMultipleChoiceDataset</li> <li>HFRewardBenchDataset</li> <li>LanguageModel</li> <li>MatchMaker</li> <li>Metric</li> <li>PairwiseJudge</li> <li>PairwiseScorer</li> <li>PromptTemplate</li> <li>ResultRecorder</li> <li>RewardModel</li> <li>StringProcessor</li> <li>TextDataset</li> <li>Tokenizer</li> <li>utils</li> </ul>"},{"location":"api_reference/ChatDataset/","title":"ChatDataset","text":""},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatDataset","title":"ChatDataset","text":"<p>A dataset holding <code>ChatInstance</code>.</p> Source code in <code>flexeval/core/chat_dataset/base.py</code> <pre><code>class ChatDataset(Sequence[ChatInstance], ABC):\n    \"\"\"A dataset holding `ChatInstance`.\"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the number of chat instances in the dataset.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __getitem__(self, i: int) -&gt; ChatInstance:\n        \"\"\"\n        Returns the i-th chat instance.\n        \"\"\"\n        raise NotImplementedError\n\n    def require_incremental_response(self) -&gt; bool:\n        \"\"\"If true, the inputs consist of multiple user utterances and the\n        model should generate responses for each utterance incrementally.\n\n        Otherwise, the model just has to continue the conversation from the last user utterance.\n        \"\"\"\n        return False\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatDataset.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of chat instances in the dataset.</p> Source code in <code>flexeval/core/chat_dataset/base.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"\n    Returns the number of chat instances in the dataset.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatDataset.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(i: int) -&gt; ChatInstance\n</code></pre> <p>Returns the i-th chat instance.</p> Source code in <code>flexeval/core/chat_dataset/base.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, i: int) -&gt; ChatInstance:\n    \"\"\"\n    Returns the i-th chat instance.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatDataset.require_incremental_response","title":"require_incremental_response","text":"<pre><code>require_incremental_response() -&gt; bool\n</code></pre> <p>If true, the inputs consist of multiple user utterances and the model should generate responses for each utterance incrementally.</p> <p>Otherwise, the model just has to continue the conversation from the last user utterance.</p> Source code in <code>flexeval/core/chat_dataset/base.py</code> <pre><code>def require_incremental_response(self) -&gt; bool:\n    \"\"\"If true, the inputs consist of multiple user utterances and the\n    model should generate responses for each utterance incrementally.\n\n    Otherwise, the model just has to continue the conversation from the last user utterance.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/chat_dataset/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance","title":"ChatInstance  <code>dataclass</code>","text":"<p>A dataclass representing a single chat that will be fed to a chat language model.</p> Source code in <code>flexeval/core/chat_dataset/base.py</code> <pre><code>@dataclass\nclass ChatInstance:\n    \"\"\"\n    A dataclass representing a single chat that will be fed to a chat language model.\n    \"\"\"\n\n    messages: list[dict[str, str]]\n    \"\"\"\n    A list of messages in the chat.\n    The format of messages typically follows [OpenAI's Chat Completions API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api).\n    ```json\n    [\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Hello! How can I help you today?\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"I'd like to book a flight to Paris.\"\n        }\n    ]\n    ```\n    \"\"\"\n    references: list[str] = field(default_factory=list)\n    \"\"\"\n    A list of reference responses to the user's last message.\n    The model's response will be evaluated against these references.\n    \"\"\"\n    extra_info: dict[str, Any] = field(default_factory=dict)\n    \"\"\"\n    Extra information that can be used by passing to `Metric`.\n    \"\"\"\n\n    def __post_init__(self) -&gt; None:\n        if \"messages\" in self.extra_info:\n            msg = (\n                \"'extra_info' in ChatInstance cannot contain a key named 'messages', \"\n                \"as it will conflict with the 'messages' attribute. \"\n                \"The key 'messages' will be removed.\"\n            )\n            warnings.warn(msg, stacklevel=2)\n            self.extra_info.pop(\"messages\")\n\n    @property\n    def inputs(self) -&gt; list[dict[str, str]]:\n        \"\"\"\n        Alias for `messages`.\n        This is used in `FewShotGenerator` so that it can access the inputs with the same attribute name as\n        `GenerationInstance` and `MultipleChoiceInstance`.\n        \"\"\"\n        return self.messages\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance.messages","title":"messages  <code>instance-attribute</code>","text":"<pre><code>messages: list[dict[str, str]]\n</code></pre> <p>A list of messages in the chat. The format of messages typically follows OpenAI's Chat Completions API. <pre><code>[\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! How can I help you today?\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"I'd like to book a flight to Paris.\"\n    }\n]\n</code></pre></p>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance.references","title":"references  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>references: list[str] = field(default_factory=list)\n</code></pre> <p>A list of reference responses to the user's last message. The model's response will be evaluated against these references.</p>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance.extra_info","title":"extra_info  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra_info: dict[str, Any] = field(default_factory=dict)\n</code></pre> <p>Extra information that can be used by passing to <code>Metric</code>.</p>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance.inputs","title":"inputs  <code>property</code>","text":"<pre><code>inputs: list[dict[str, str]]\n</code></pre> <p>Alias for <code>messages</code>. This is used in <code>FewShotGenerator</code> so that it can access the inputs with the same attribute name as <code>GenerationInstance</code> and <code>MultipleChoiceInstance</code>.</p>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance.__init__","title":"__init__","text":"<pre><code>__init__(messages: list[dict[str, str]], references: list[str] = list(), extra_info: dict[str, Any] = dict()) -&gt; None\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/chat_dataset/base.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if \"messages\" in self.extra_info:\n        msg = (\n            \"'extra_info' in ChatInstance cannot contain a key named 'messages', \"\n            \"as it will conflict with the 'messages' attribute. \"\n            \"The key 'messages' will be removed.\"\n        )\n        warnings.warn(msg, stacklevel=2)\n        self.extra_info.pop(\"messages\")\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.HFChatDataset","title":"HFChatDataset","text":"<p>Load ChatInstances from a Hugging Face dataset.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the Hugging Face dataset.</p> </li> <li> <code>split</code>               (<code>str</code>)           \u2013            <p>The split of the dataset.</p> </li> <li> <code>subset</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The subset of the dataset.</p> </li> <li> <code>dataset_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>The keyword arguments to pass to the Hugging Face dataset.</p> </li> </ul> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>class HFChatDataset(TemplateChatDataset):\n    \"\"\"\n    Load ChatInstances from a Hugging Face dataset.\n\n    Args:\n        path: The path to the Hugging Face dataset.\n        split: The split of the dataset.\n        subset: The subset of the dataset.\n        dataset_kwargs: The keyword arguments to pass to the Hugging Face dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        split: str,\n        input_template: str,\n        subset: str | None = None,\n        dataset_kwargs: dict[str, Any] | None = None,\n        reference_template: str | None = None,\n        reference_list_template: str | None = None,\n        require_incremental_response: bool = False,\n        extra_info_templates: dict[str, str] | None = None,\n        system_message_template: str | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        dataset_kwargs = dataset_kwargs or {}\n        dataset = datasets.load_dataset(path, name=subset, split=split, **dataset_kwargs)\n        items = [dict(item) for item in dataset]\n\n        super().__init__(\n            items=items,\n            input_template=input_template,\n            reference_template=reference_template,\n            reference_list_template=reference_list_template,\n            require_incremental_response=require_incremental_response,\n            extra_info_templates=extra_info_templates,\n            system_message_template=system_message_template,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.HFChatDataset.__init__","title":"__init__","text":"<pre><code>__init__(path: str, split: str, input_template: str, subset: str | None = None, dataset_kwargs: dict[str, Any] | None = None, reference_template: str | None = None, reference_list_template: str | None = None, require_incremental_response: bool = False, extra_info_templates: dict[str, str] | None = None, system_message_template: str | None = None, data_range: tuple[int, int] | None = None, keep_conditions: dict[str, str] | None = None, remove_conditions: dict[str, str] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    split: str,\n    input_template: str,\n    subset: str | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    require_incremental_response: bool = False,\n    extra_info_templates: dict[str, str] | None = None,\n    system_message_template: str | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    dataset_kwargs = dataset_kwargs or {}\n    dataset = datasets.load_dataset(path, name=subset, split=split, **dataset_kwargs)\n    items = [dict(item) for item in dataset]\n\n    super().__init__(\n        items=items,\n        input_template=input_template,\n        reference_template=reference_template,\n        reference_list_template=reference_list_template,\n        require_incremental_response=require_incremental_response,\n        extra_info_templates=extra_info_templates,\n        system_message_template=system_message_template,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.JsonlChatDataset","title":"JsonlChatDataset","text":"<p>Load ChatInstances from a JSONL file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the JSONL file.</p> </li> </ul> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>class JsonlChatDataset(TemplateChatDataset):\n    \"\"\"\n    Load ChatInstances from a JSONL file.\n\n    Args:\n        path: The path to the JSONL file.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        input_template: str,\n        reference_template: str | None = None,\n        reference_list_template: str | None = None,\n        require_incremental_response: bool = False,\n        extra_info_templates: dict[str, str] | None = None,\n        system_message_template: str | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        with open(path) as f:\n            items = [json.loads(line) for line in f]\n\n        super().__init__(\n            items=items,\n            input_template=input_template,\n            reference_template=reference_template,\n            reference_list_template=reference_list_template,\n            require_incremental_response=require_incremental_response,\n            extra_info_templates=extra_info_templates,\n            system_message_template=system_message_template,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.JsonlChatDataset.__init__","title":"__init__","text":"<pre><code>__init__(path: str, input_template: str, reference_template: str | None = None, reference_list_template: str | None = None, require_incremental_response: bool = False, extra_info_templates: dict[str, str] | None = None, system_message_template: str | None = None, data_range: tuple[int, int] | None = None, keep_conditions: dict[str, str] | None = None, remove_conditions: dict[str, str] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    input_template: str,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    require_incremental_response: bool = False,\n    extra_info_templates: dict[str, str] | None = None,\n    system_message_template: str | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    with open(path) as f:\n        items = [json.loads(line) for line in f]\n\n    super().__init__(\n        items=items,\n        input_template=input_template,\n        reference_template=reference_template,\n        reference_list_template=reference_list_template,\n        require_incremental_response=require_incremental_response,\n        extra_info_templates=extra_info_templates,\n        system_message_template=system_message_template,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset","title":"TemplateChatDataset","text":"<p>A chat dataset using Hugging Face datasets. This class only supports single-turn chat.</p> <p>Parameters:</p> <ul> <li> <code>items</code>               (<code>list[dict[str, Any]]</code>)           \u2013            <p>A list of items in a dict format.</p> </li> <li> <code>input_template</code>               (<code>str</code>)           \u2013            <p>A Jinja2 template for the user input.</p> </li> <li> <code>reference_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Specify the Jinja2 template to render the reference string if the dataset has a single reference.</p> </li> <li> <code>reference_list_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Specify the Jinja2 template to render a list of reference strings if the dataset has multiple references.</p> </li> <li> <code>require_incremental_response</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether the dataset requires incremental response.</p> </li> <li> <code>extra_info_templates</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of Jinja2 templates for extra information.</p> </li> <li> <code>system_message_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A Jinja2 template for the system message.</p> </li> <li> <code>data_range</code>               (<code>tuple[int, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The range of data to use.</p> </li> <li> <code>keep_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to filter certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.</p> </li> <li> <code>remove_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to remove certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.</p> </li> </ul> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>class TemplateChatDataset(ChatDataset):\n    \"\"\"\n    A chat dataset using Hugging Face datasets.\n    This class only supports single-turn chat.\n\n    Args:\n        items: A list of items in a dict format.\n        input_template: A Jinja2 template for the user input.\n        reference_template: Specify the Jinja2 template to render the reference string\n            if the dataset has a single reference.\n        reference_list_template: Specify the Jinja2 template to render a list of reference strings\n            if the dataset has multiple references.\n        require_incremental_response: Whether the dataset requires incremental response.\n        extra_info_templates: A dictionary of Jinja2 templates for extra information.\n        system_message_template: A Jinja2 template for the system message.\n        data_range: The range of data to use.\n        keep_conditions: A dictionary to indicate the condition to filter certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.\n        remove_conditions: A dictionary to indicate the condition to remove certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.\n    \"\"\"\n\n    def __init__(\n        self,\n        items: list[dict[str, Any]],\n        input_template: str,\n        reference_template: str | None = None,\n        reference_list_template: str | None = None,\n        require_incremental_response: bool = False,\n        extra_info_templates: dict[str, str] | None = None,\n        system_message_template: str | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        if reference_template and reference_list_template:\n            msg = \"Only one of reference_template and reference_list_template can be set.\"\n            raise ValueError(msg)\n\n        if data_range:\n            start, end = data_range\n            items = items[start:end]\n\n        keep_conditions = keep_conditions or {}\n        for template_str, value_to_keep in keep_conditions.items():\n            key_template = JINJA2_ENV.from_string(template_str)\n            items = [item for item in items if key_template.render(**item) == value_to_keep]\n        remove_conditions = remove_conditions or {}\n        for template_str, value_to_remove in remove_conditions.items():\n            key_template = JINJA2_ENV.from_string(template_str)\n            items = [item for item in items if key_template.render(**item) != value_to_remove]\n\n        self.items = items\n\n        self.input_template = JINJA2_ENV.from_string(input_template)\n        self.reference_template = JINJA2_ENV.from_string(reference_template) if reference_template else None\n        self.reference_list_template = (\n            JINJA2_ENV.from_string(reference_list_template) if reference_list_template else None\n        )\n\n        extra_info_templates = extra_info_templates or {}\n        self._extra_info_templates: dict[str, Template] = {\n            key: JINJA2_ENV.from_string(template) for key, template in extra_info_templates.items()\n        }\n\n        self._system_message_template: Template | None = (\n            JINJA2_ENV.from_string(system_message_template) if system_message_template else None\n        )\n\n        self._require_incremental_response = require_incremental_response\n\n    def require_incremental_response(self) -&gt; bool:\n        return self._require_incremental_response\n\n    def __len__(self) -&gt; int:\n        return len(self.items)\n\n    def __getitem__(self, i: int) -&gt; ChatInstance:\n        item = self.items[i]\n        input_utterance = self.input_template.render(**item)\n        messages = [{\"role\": \"user\", \"content\": input_utterance}]\n\n        if self._system_message_template:\n            system_message = self._system_message_template.render(**item)\n            messages.insert(0, {\"role\": \"system\", \"content\": system_message})\n\n        reference_list: list[str] = []\n        if self.reference_template:\n            reference_string = self.reference_template.render(**item)\n            reference_list.append(reference_string)\n        if self.reference_list_template:\n            reference_list_string = self.reference_list_template.render(**item)\n            if not (reference_list_string.startswith(\"[\") and reference_list_string.endswith(\"]\")):\n                msg = (\n                    f\"The reference_list_template should render a list of strings \"\n                    f\"but we got `{reference_list_string}`.\"\n                )\n                raise ValueError(msg)\n            reference_list.extend([str(ref) for ref in literal_eval(reference_list_string)])\n\n        extra_info = dict(item.items())\n        extra_info_from_templates = {\n            key: template.render(**item) for key, template in self._extra_info_templates.items()\n        }\n        extra_info.update(extra_info_from_templates)\n\n        return ChatInstance(messages=messages, references=reference_list, extra_info=extra_info)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.items","title":"items  <code>instance-attribute</code>","text":"<pre><code>items = items\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.input_template","title":"input_template  <code>instance-attribute</code>","text":"<pre><code>input_template = from_string(input_template)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.reference_template","title":"reference_template  <code>instance-attribute</code>","text":"<pre><code>reference_template = from_string(reference_template) if reference_template else None\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.reference_list_template","title":"reference_list_template  <code>instance-attribute</code>","text":"<pre><code>reference_list_template = from_string(reference_list_template) if reference_list_template else None\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.__init__","title":"__init__","text":"<pre><code>__init__(items: list[dict[str, Any]], input_template: str, reference_template: str | None = None, reference_list_template: str | None = None, require_incremental_response: bool = False, extra_info_templates: dict[str, str] | None = None, system_message_template: str | None = None, data_range: tuple[int, int] | None = None, keep_conditions: dict[str, str] | None = None, remove_conditions: dict[str, str] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    items: list[dict[str, Any]],\n    input_template: str,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    require_incremental_response: bool = False,\n    extra_info_templates: dict[str, str] | None = None,\n    system_message_template: str | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    if reference_template and reference_list_template:\n        msg = \"Only one of reference_template and reference_list_template can be set.\"\n        raise ValueError(msg)\n\n    if data_range:\n        start, end = data_range\n        items = items[start:end]\n\n    keep_conditions = keep_conditions or {}\n    for template_str, value_to_keep in keep_conditions.items():\n        key_template = JINJA2_ENV.from_string(template_str)\n        items = [item for item in items if key_template.render(**item) == value_to_keep]\n    remove_conditions = remove_conditions or {}\n    for template_str, value_to_remove in remove_conditions.items():\n        key_template = JINJA2_ENV.from_string(template_str)\n        items = [item for item in items if key_template.render(**item) != value_to_remove]\n\n    self.items = items\n\n    self.input_template = JINJA2_ENV.from_string(input_template)\n    self.reference_template = JINJA2_ENV.from_string(reference_template) if reference_template else None\n    self.reference_list_template = (\n        JINJA2_ENV.from_string(reference_list_template) if reference_list_template else None\n    )\n\n    extra_info_templates = extra_info_templates or {}\n    self._extra_info_templates: dict[str, Template] = {\n        key: JINJA2_ENV.from_string(template) for key, template in extra_info_templates.items()\n    }\n\n    self._system_message_template: Template | None = (\n        JINJA2_ENV.from_string(system_message_template) if system_message_template else None\n    )\n\n    self._require_incremental_response = require_incremental_response\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.require_incremental_response","title":"require_incremental_response","text":"<pre><code>require_incremental_response() -&gt; bool\n</code></pre> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>def require_incremental_response(self) -&gt; bool:\n    return self._require_incremental_response\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.items)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; ChatInstance\n</code></pre> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>def __getitem__(self, i: int) -&gt; ChatInstance:\n    item = self.items[i]\n    input_utterance = self.input_template.render(**item)\n    messages = [{\"role\": \"user\", \"content\": input_utterance}]\n\n    if self._system_message_template:\n        system_message = self._system_message_template.render(**item)\n        messages.insert(0, {\"role\": \"system\", \"content\": system_message})\n\n    reference_list: list[str] = []\n    if self.reference_template:\n        reference_string = self.reference_template.render(**item)\n        reference_list.append(reference_string)\n    if self.reference_list_template:\n        reference_list_string = self.reference_list_template.render(**item)\n        if not (reference_list_string.startswith(\"[\") and reference_list_string.endswith(\"]\")):\n            msg = (\n                f\"The reference_list_template should render a list of strings \"\n                f\"but we got `{reference_list_string}`.\"\n            )\n            raise ValueError(msg)\n        reference_list.extend([str(ref) for ref in literal_eval(reference_list_string)])\n\n    extra_info = dict(item.items())\n    extra_info_from_templates = {\n        key: template.render(**item) for key, template in self._extra_info_templates.items()\n    }\n    extra_info.update(extra_info_from_templates)\n\n    return ChatInstance(messages=messages, references=reference_list, extra_info=extra_info)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.chatbot_bench.ChatbotBench","title":"ChatbotBench","text":"<p>This class loads data with the jsonl format used in chat evaluation benchmarks such as MT-Bench (Multi-turn Benchmark) or Vicuna QA Benchmark.</p> Example of a line from a jsonl file <p>{   \"question_id\": 00,   \"category\": \"writing\",   \"turns\": [     \"Compose an engaging travel blog post about a recent trip to Hawaii.\",     \"Rewrite your previous response. Start every sentence with the letter A.\"   ] }</p> Source code in <code>flexeval/core/chat_dataset/chatbot_bench.py</code> <pre><code>class ChatbotBench(ChatDataset):\n    \"\"\"This class loads data with the jsonl format used in chat evaluation benchmarks such as\n    MT-Bench (Multi-turn Benchmark) or Vicuna QA Benchmark.\n\n    Example of a line from a jsonl file:\n        {\n          \"question_id\": 00,\n          \"category\": \"writing\",\n          \"turns\": [\n            \"Compose an engaging travel blog post about a recent trip to Hawaii.\",\n            \"Rewrite your previous response. Start every sentence with the letter A.\"\n          ]\n        }\n    \"\"\"\n\n    def __init__(\n        self,\n        path_or_name: str,\n        ref_path_or_name: str | None = None,\n        need_ref_categories: list[str] | None = None,\n    ) -&gt; None:\n        file_path = resolve_path_or_name(path_or_name)\n\n        self._id_to_question_id: list[int | str] = []\n        self._id_to_category: list[str] = []\n        self._messages_dict: dict[int | str, list[dict[str, str]]] = {}\n        with open(file_path) as f:\n            for line in f:\n                item = json.loads(line)\n                self._id_to_question_id.append(item[\"question_id\"])\n                self._id_to_category.append(item[\"category\"])\n                self._messages_dict[item[\"question_id\"]] = [{\"role\": \"user\", \"content\": turn} for turn in item[\"turns\"]]\n\n        self._references_dict: dict[int | str, list[str]] = {}\n        if ref_path_or_name is not None:\n            ref_file_path = resolve_path_or_name(ref_path_or_name)\n            with open(ref_file_path) as f:\n                for line in f:\n                    item = json.loads(line)\n                    self._references_dict[item[\"question_id\"]] = item[\"choices\"][0][\"turns\"]\n\n        self.need_ref_categories = need_ref_categories or [\n            \"math\",\n            \"coding\",\n            \"reasoning\",\n        ]\n\n    def require_incremental_response(self) -&gt; bool:\n        return True\n\n    def __len__(self) -&gt; int:\n        return len(self._id_to_question_id)\n\n    def __getitem__(self, i: int) -&gt; ChatInstance:\n        question_id = self._id_to_question_id[i]\n        category = self._id_to_category[i]\n        references: list[str] = []\n        if category in self.need_ref_categories:\n            references = self._references_dict.get(question_id, [])\n        return ChatInstance(self._messages_dict[question_id], references=references, extra_info={\"category\": category})\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.chatbot_bench.ChatbotBench.need_ref_categories","title":"need_ref_categories  <code>instance-attribute</code>","text":"<pre><code>need_ref_categories = need_ref_categories or ['math', 'coding', 'reasoning']\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.chatbot_bench.ChatbotBench.__init__","title":"__init__","text":"<pre><code>__init__(path_or_name: str, ref_path_or_name: str | None = None, need_ref_categories: list[str] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/chat_dataset/chatbot_bench.py</code> <pre><code>def __init__(\n    self,\n    path_or_name: str,\n    ref_path_or_name: str | None = None,\n    need_ref_categories: list[str] | None = None,\n) -&gt; None:\n    file_path = resolve_path_or_name(path_or_name)\n\n    self._id_to_question_id: list[int | str] = []\n    self._id_to_category: list[str] = []\n    self._messages_dict: dict[int | str, list[dict[str, str]]] = {}\n    with open(file_path) as f:\n        for line in f:\n            item = json.loads(line)\n            self._id_to_question_id.append(item[\"question_id\"])\n            self._id_to_category.append(item[\"category\"])\n            self._messages_dict[item[\"question_id\"]] = [{\"role\": \"user\", \"content\": turn} for turn in item[\"turns\"]]\n\n    self._references_dict: dict[int | str, list[str]] = {}\n    if ref_path_or_name is not None:\n        ref_file_path = resolve_path_or_name(ref_path_or_name)\n        with open(ref_file_path) as f:\n            for line in f:\n                item = json.loads(line)\n                self._references_dict[item[\"question_id\"]] = item[\"choices\"][0][\"turns\"]\n\n    self.need_ref_categories = need_ref_categories or [\n        \"math\",\n        \"coding\",\n        \"reasoning\",\n    ]\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.chatbot_bench.ChatbotBench.require_incremental_response","title":"require_incremental_response","text":"<pre><code>require_incremental_response() -&gt; bool\n</code></pre> Source code in <code>flexeval/core/chat_dataset/chatbot_bench.py</code> <pre><code>def require_incremental_response(self) -&gt; bool:\n    return True\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.chatbot_bench.ChatbotBench.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/chat_dataset/chatbot_bench.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._id_to_question_id)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.chatbot_bench.ChatbotBench.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; ChatInstance\n</code></pre> Source code in <code>flexeval/core/chat_dataset/chatbot_bench.py</code> <pre><code>def __getitem__(self, i: int) -&gt; ChatInstance:\n    question_id = self._id_to_question_id[i]\n    category = self._id_to_category[i]\n    references: list[str] = []\n    if category in self.need_ref_categories:\n        references = self._references_dict.get(question_id, [])\n    return ChatInstance(self._messages_dict[question_id], references=references, extra_info={\"category\": category})\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.sacrebleu_dataset.SacreBleuChatDataset","title":"SacreBleuChatDataset","text":"<p>Load datasets from the sacrebleu library. The available datasets are defined in sacrebleu.DATASETS.</p> Source code in <code>flexeval/core/chat_dataset/sacrebleu_dataset.py</code> <pre><code>class SacreBleuChatDataset(ChatDataset):\n    \"\"\"Load datasets from the [sacrebleu](https://github.com/mjpost/sacrebleu) library.\n    The available datasets are defined in sacrebleu.DATASETS.\n    \"\"\"\n\n    def __init__(self, name: str, langpair: str) -&gt; None:\n        self._source_list: list[str] = list(sacrebleu.DATASETS[name].source(langpair))\n        self._references_list: list[list[str]] = [\n            [r.strip() for r in refs] for refs in sacrebleu.DATASETS[name].references(langpair)\n        ]\n\n        if len(self._source_list) != len(self._references_list):\n            msg = \"The number of source and reference pairs should be the same.\"\n            raise ValueError(msg)\n\n    def require_incremental_response(self) -&gt; bool:\n        return False\n\n    def __len__(self) -&gt; int:\n        return len(self._source_list)\n\n    def __getitem__(self, i: int) -&gt; ChatInstance:\n        return ChatInstance(\n            messages=[{\"role\": \"user\", \"content\": self._source_list[i]}],\n            references=self._references_list[i],\n            extra_info={},\n        )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.sacrebleu_dataset.SacreBleuChatDataset.__init__","title":"__init__","text":"<pre><code>__init__(name: str, langpair: str) -&gt; None\n</code></pre> Source code in <code>flexeval/core/chat_dataset/sacrebleu_dataset.py</code> <pre><code>def __init__(self, name: str, langpair: str) -&gt; None:\n    self._source_list: list[str] = list(sacrebleu.DATASETS[name].source(langpair))\n    self._references_list: list[list[str]] = [\n        [r.strip() for r in refs] for refs in sacrebleu.DATASETS[name].references(langpair)\n    ]\n\n    if len(self._source_list) != len(self._references_list):\n        msg = \"The number of source and reference pairs should be the same.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.sacrebleu_dataset.SacreBleuChatDataset.require_incremental_response","title":"require_incremental_response","text":"<pre><code>require_incremental_response() -&gt; bool\n</code></pre> Source code in <code>flexeval/core/chat_dataset/sacrebleu_dataset.py</code> <pre><code>def require_incremental_response(self) -&gt; bool:\n    return False\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.sacrebleu_dataset.SacreBleuChatDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/chat_dataset/sacrebleu_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._source_list)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.sacrebleu_dataset.SacreBleuChatDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; ChatInstance\n</code></pre> Source code in <code>flexeval/core/chat_dataset/sacrebleu_dataset.py</code> <pre><code>def __getitem__(self, i: int) -&gt; ChatInstance:\n    return ChatInstance(\n        messages=[{\"role\": \"user\", \"content\": self._source_list[i]}],\n        references=self._references_list[i],\n        extra_info={},\n    )\n</code></pre>"},{"location":"api_reference/EvalSetup/","title":"EvalSetup","text":""},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.EvalSetup","title":"EvalSetup","text":"<p>Abstract class to give evaluation functions a common interface.</p> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>class EvalSetup(ABC):\n    \"\"\"Abstract class to give evaluation functions a common interface.\"\"\"\n\n    @abstractmethod\n    def evaluate_lm(\n        self,\n        language_model: LanguageModel,\n    ) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n        pass\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.EvalSetup.evaluate_lm","title":"evaluate_lm  <code>abstractmethod</code>","text":"<pre><code>evaluate_lm(language_model: LanguageModel) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]\n</code></pre> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>@abstractmethod\ndef evaluate_lm(\n    self,\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n    pass\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse","title":"ChatResponse  <code>dataclass</code>","text":"<p>Evaluation setup for chat response generation. In this setup, the model receives context in a chat format like: <pre><code>[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"}\n]\n</code></pre></p> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>@dataclass\nclass ChatResponse(EvalSetup):\n    \"\"\"\n    Evaluation setup for chat response generation.\n    In this setup, the model receives context in a chat format like:\n    ```json\n    [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n        {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"}\n    ]\n    ```\n    \"\"\"\n\n    eval_dataset: ChatDataset\n    gen_kwargs: dict[str, Any]\n    few_shot_generator: FewShotGenerator | None = None\n    metrics: list[Metric] | Metric | None = None\n    batch_size: int = 4\n    max_instances: int | None = None\n\n    def evaluate_lm(\n        self,\n        language_model: LanguageModel,\n    ) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n        metrics = self.metrics or []\n        if isinstance(metrics, Metric):\n            metrics = [metrics]\n\n        return evaluate_chat_response(\n            language_model=language_model,\n            gen_kwargs=self.gen_kwargs,\n            eval_dataset=self.eval_dataset,\n            metrics=metrics,\n            batch_size=self.batch_size,\n            max_instances=self.max_instances,\n            few_shot_generator=self.few_shot_generator,\n        )\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.eval_dataset","title":"eval_dataset  <code>instance-attribute</code>","text":"<pre><code>eval_dataset: ChatDataset\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.gen_kwargs","title":"gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>gen_kwargs: dict[str, Any]\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.few_shot_generator","title":"few_shot_generator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>few_shot_generator: FewShotGenerator | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.metrics","title":"metrics  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metrics: list[Metric] | Metric | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 4\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.max_instances","title":"max_instances  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_instances: int | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.__init__","title":"__init__","text":"<pre><code>__init__(eval_dataset: ChatDataset, gen_kwargs: dict[str, Any], few_shot_generator: FewShotGenerator | None = None, metrics: list[Metric] | Metric | None = None, batch_size: int = 4, max_instances: int | None = None) -&gt; None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.evaluate_lm","title":"evaluate_lm","text":"<pre><code>evaluate_lm(language_model: LanguageModel) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]\n</code></pre> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>def evaluate_lm(\n    self,\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n    metrics = self.metrics or []\n    if isinstance(metrics, Metric):\n        metrics = [metrics]\n\n    return evaluate_chat_response(\n        language_model=language_model,\n        gen_kwargs=self.gen_kwargs,\n        eval_dataset=self.eval_dataset,\n        metrics=metrics,\n        batch_size=self.batch_size,\n        max_instances=self.max_instances,\n        few_shot_generator=self.few_shot_generator,\n    )\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation","title":"Generation  <code>dataclass</code>","text":"<p>Evaluation setup for text generation. The model receives a prompt in a plain text format and generates its continuation.</p> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>@dataclass\nclass Generation(EvalSetup):\n    \"\"\"\n    Evaluation setup for text generation.\n    The model receives a prompt in a plain text format and generates its continuation.\n    \"\"\"\n\n    eval_dataset: GenerationDataset\n    prompt_template: PromptTemplate\n    gen_kwargs: dict[str, Any]\n    few_shot_generator: FewShotGenerator | None = None\n    metrics: list[Metric] | Metric | None = None\n    batch_size: int = 4\n    max_instances: int | None = None\n\n    def evaluate_lm(\n        self,\n        language_model: LanguageModel,\n    ) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n        metrics = self.metrics or []\n        if isinstance(metrics, Metric):\n            metrics = [metrics]\n\n        return evaluate_generation(\n            language_model=language_model,\n            gen_kwargs=self.gen_kwargs,\n            eval_dataset=self.eval_dataset,\n            prompt_template=self.prompt_template,\n            few_shot_generator=self.few_shot_generator,\n            metrics=metrics,\n            batch_size=self.batch_size,\n            max_instances=self.max_instances,\n        )\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.eval_dataset","title":"eval_dataset  <code>instance-attribute</code>","text":"<pre><code>eval_dataset: GenerationDataset\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template: PromptTemplate\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.gen_kwargs","title":"gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>gen_kwargs: dict[str, Any]\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.few_shot_generator","title":"few_shot_generator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>few_shot_generator: FewShotGenerator | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.metrics","title":"metrics  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metrics: list[Metric] | Metric | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 4\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.max_instances","title":"max_instances  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_instances: int | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.__init__","title":"__init__","text":"<pre><code>__init__(eval_dataset: GenerationDataset, prompt_template: PromptTemplate, gen_kwargs: dict[str, Any], few_shot_generator: FewShotGenerator | None = None, metrics: list[Metric] | Metric | None = None, batch_size: int = 4, max_instances: int | None = None) -&gt; None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.evaluate_lm","title":"evaluate_lm","text":"<pre><code>evaluate_lm(language_model: LanguageModel) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]\n</code></pre> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>def evaluate_lm(\n    self,\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n    metrics = self.metrics or []\n    if isinstance(metrics, Metric):\n        metrics = [metrics]\n\n    return evaluate_generation(\n        language_model=language_model,\n        gen_kwargs=self.gen_kwargs,\n        eval_dataset=self.eval_dataset,\n        prompt_template=self.prompt_template,\n        few_shot_generator=self.few_shot_generator,\n        metrics=metrics,\n        batch_size=self.batch_size,\n        max_instances=self.max_instances,\n    )\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice","title":"MultipleChoice  <code>dataclass</code>","text":"<p>Evaluation setup for multiple choice questions. The model receives a prompt and a list of choices and selects the answer with the highest probability.</p> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>@dataclass\nclass MultipleChoice(EvalSetup):\n    \"\"\"\n    Evaluation setup for multiple choice questions.\n    The model receives a prompt and a list of choices and selects the answer with the highest probability.\n    \"\"\"\n\n    eval_dataset: MultipleChoiceDataset\n    prompt_template: PromptTemplate\n    few_shot_generator: FewShotGenerator | None = None\n    batch_size: int = 4\n    max_instances: int | None = None\n\n    def evaluate_lm(\n        self,\n        language_model: LanguageModel,\n    ) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n        return evaluate_multiple_choice(\n            language_model=language_model,\n            eval_dataset=self.eval_dataset,\n            prompt_template=self.prompt_template,\n            few_shot_generator=self.few_shot_generator,\n            batch_size=self.batch_size,\n            max_instances=self.max_instances,\n        )\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.eval_dataset","title":"eval_dataset  <code>instance-attribute</code>","text":"<pre><code>eval_dataset: MultipleChoiceDataset\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template: PromptTemplate\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.few_shot_generator","title":"few_shot_generator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>few_shot_generator: FewShotGenerator | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 4\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.max_instances","title":"max_instances  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_instances: int | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.__init__","title":"__init__","text":"<pre><code>__init__(eval_dataset: MultipleChoiceDataset, prompt_template: PromptTemplate, few_shot_generator: FewShotGenerator | None = None, batch_size: int = 4, max_instances: int | None = None) -&gt; None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.evaluate_lm","title":"evaluate_lm","text":"<pre><code>evaluate_lm(language_model: LanguageModel) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]\n</code></pre> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>def evaluate_lm(\n    self,\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n    return evaluate_multiple_choice(\n        language_model=language_model,\n        eval_dataset=self.eval_dataset,\n        prompt_template=self.prompt_template,\n        few_shot_generator=self.few_shot_generator,\n        batch_size=self.batch_size,\n        max_instances=self.max_instances,\n    )\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity","title":"Perplexity  <code>dataclass</code>","text":"<p>Evaluation setup for perplexity. The model receives plain text and computes the perplexity of the text.</p> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>@dataclass\nclass Perplexity(EvalSetup):\n    \"\"\"\n    Evaluation setup for perplexity.\n    The model receives plain text and computes the perplexity of the text.\n    \"\"\"\n\n    eval_dataset: TextDataset\n    batch_size: int = 4\n    tokenizer: Tokenizer | None = None\n    max_instances: int | None = None\n\n    def evaluate_lm(\n        self,\n        language_model: LanguageModel,\n    ) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n        metrics = evaluate_perplexity(\n            language_model=language_model,\n            eval_dataset=self.eval_dataset,\n            batch_size=self.batch_size,\n            tokenizer=self.tokenizer,\n            max_instances=self.max_instances,\n        )\n        return metrics, None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity.eval_dataset","title":"eval_dataset  <code>instance-attribute</code>","text":"<pre><code>eval_dataset: TextDataset\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 4\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity.tokenizer","title":"tokenizer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tokenizer: Tokenizer | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity.max_instances","title":"max_instances  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_instances: int | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity.__init__","title":"__init__","text":"<pre><code>__init__(eval_dataset: TextDataset, batch_size: int = 4, tokenizer: Tokenizer | None = None, max_instances: int | None = None) -&gt; None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity.evaluate_lm","title":"evaluate_lm","text":"<pre><code>evaluate_lm(language_model: LanguageModel) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]\n</code></pre> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>def evaluate_lm(\n    self,\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n    metrics = evaluate_perplexity(\n        language_model=language_model,\n        eval_dataset=self.eval_dataset,\n        batch_size=self.batch_size,\n        tokenizer=self.tokenizer,\n        max_instances=self.max_instances,\n    )\n    return metrics, None\n</code></pre>"},{"location":"api_reference/FewShotGenerator/","title":"FewShotGenerator","text":""},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.base.FewShotGenerator","title":"FewShotGenerator","text":"Source code in <code>flexeval/core/few_shot_generator/base.py</code> <pre><code>class FewShotGenerator(ABC):\n    def __init__(self, num_trials_to_avoid_leak: int) -&gt; None:\n        self._num_trials_to_avoid_leak = num_trials_to_avoid_leak\n\n    @abstractmethod\n    def _sample_instances(self, eval_inputs: list[dict[str, Any]] | dict[str, Any] | None = None) -&gt; list[Instance]:\n        \"\"\"\n        Sample instances for few-shot learning.\n        This method should be implemented in the derived class.\n        \"\"\"\n        raise NotImplementedError\n\n    def __call__(self, eval_inputs: list[dict[str, Any]] | dict[str, Any] | None = None) -&gt; list[Instance]:\n        \"\"\"\n        Sample instances for few-shot learning.\n        This method calls `_sample_instances` and\n        checks if the sampled instances have the same inputs as the evaluation instance.\n\n        Args:\n            eval_inputs: The inputs of the evaluation instance.\n                This is used to avoid data leakage\n                by checking if the sampled instances have the same inputs as the evaluation instance.\n\n        Returns:\n            A list of instances for few-shot learning.\n        \"\"\"\n        sampled_instances = self._sample_instances(eval_inputs=eval_inputs)\n\n        # check if the sampled instances are the same as the eval_instance\n        if self._num_trials_to_avoid_leak and eval_inputs is not None:\n            for _ in range(self._num_trials_to_avoid_leak):\n                if all(sampled.inputs != eval_inputs for sampled in sampled_instances):\n                    return sampled_instances\n                # retry sampling\n                sampled_instances = self._sample_instances(eval_inputs=eval_inputs)\n\n            msg = (\n                f\"Few-shot instance has the same inputs as the evaluation instance, \"\n                f\"which indicates a data leak. \"\n                f\"Failed to sample a different instance after {self._num_trials_to_avoid_leak} trials.\"\n            )\n            raise ValueError(msg)\n\n        return sampled_instances\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.base.FewShotGenerator.__init__","title":"__init__","text":"<pre><code>__init__(num_trials_to_avoid_leak: int) -&gt; None\n</code></pre> Source code in <code>flexeval/core/few_shot_generator/base.py</code> <pre><code>def __init__(self, num_trials_to_avoid_leak: int) -&gt; None:\n    self._num_trials_to_avoid_leak = num_trials_to_avoid_leak\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.base.FewShotGenerator.__call__","title":"__call__","text":"<pre><code>__call__(eval_inputs: list[dict[str, Any]] | dict[str, Any] | None = None) -&gt; list[Instance]\n</code></pre> <p>Sample instances for few-shot learning. This method calls <code>_sample_instances</code> and checks if the sampled instances have the same inputs as the evaluation instance.</p> <p>Parameters:</p> <ul> <li> <code>eval_inputs</code>               (<code>list[dict[str, Any]] | dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>The inputs of the evaluation instance. This is used to avoid data leakage by checking if the sampled instances have the same inputs as the evaluation instance.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Instance]</code>           \u2013            <p>A list of instances for few-shot learning.</p> </li> </ul> Source code in <code>flexeval/core/few_shot_generator/base.py</code> <pre><code>def __call__(self, eval_inputs: list[dict[str, Any]] | dict[str, Any] | None = None) -&gt; list[Instance]:\n    \"\"\"\n    Sample instances for few-shot learning.\n    This method calls `_sample_instances` and\n    checks if the sampled instances have the same inputs as the evaluation instance.\n\n    Args:\n        eval_inputs: The inputs of the evaluation instance.\n            This is used to avoid data leakage\n            by checking if the sampled instances have the same inputs as the evaluation instance.\n\n    Returns:\n        A list of instances for few-shot learning.\n    \"\"\"\n    sampled_instances = self._sample_instances(eval_inputs=eval_inputs)\n\n    # check if the sampled instances are the same as the eval_instance\n    if self._num_trials_to_avoid_leak and eval_inputs is not None:\n        for _ in range(self._num_trials_to_avoid_leak):\n            if all(sampled.inputs != eval_inputs for sampled in sampled_instances):\n                return sampled_instances\n            # retry sampling\n            sampled_instances = self._sample_instances(eval_inputs=eval_inputs)\n\n        msg = (\n            f\"Few-shot instance has the same inputs as the evaluation instance, \"\n            f\"which indicates a data leak. \"\n            f\"Failed to sample a different instance after {self._num_trials_to_avoid_leak} trials.\"\n        )\n        raise ValueError(msg)\n\n    return sampled_instances\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.balanced.BalancedFewShotGenerator","title":"BalancedFewShotGenerator","text":"Source code in <code>flexeval/core/few_shot_generator/balanced.py</code> <pre><code>class BalancedFewShotGenerator(FewShotGenerator):\n    def __init__(\n        self,\n        dataset: GenerationDataset,\n        num_shots: int,\n        seed: int = 42,\n        num_trials_to_avoid_leak: int = 3,\n    ) -&gt; None:\n        super().__init__(num_trials_to_avoid_leak=num_trials_to_avoid_leak)\n        if not isinstance(dataset, GenerationDataset):\n            msg = \"BalancedFewShotGenerator only supports GenerationDataset\"\n            raise TypeError(msg)\n\n        if num_shots &gt; len(dataset):\n            msg = (\n                f\"`num_shots` should be less than or equal to the number of instances in `dataset`. \"\n                f\"num_shots: {num_shots}, len(dataset): {len(dataset)}\"\n            )\n            raise ValueError(msg)\n\n        self.dataset = dataset\n        self.num_shots = num_shots\n        self._rnd = random.Random(seed)\n\n        # Separate instances by label\n        # Here we assume that the label is the first element of references of the instance.\n        label_to_ids: dict[str, list[int]] = defaultdict(list)\n        for i, instance in enumerate(dataset):\n            label_to_ids[instance.references[0]].append(i)\n        self._label_to_ids = label_to_ids\n\n    def _sample_instances(\n        self,\n        eval_inputs: list[dict[str, Any]] | dict[str, Any] | None = None,\n    ) -&gt; list[GenerationInstance]:\n        # Shuffle labels\n        labels = list(self._label_to_ids.keys())\n        self._rnd.shuffle(labels)\n\n        # Evenly distribute num_samples to each label\n        num_samples_list = [self.num_shots // len(labels)] * len(labels)\n        remaining_samples = self.num_shots % len(labels)\n        for i in range(remaining_samples):\n            num_samples_list[i] += 1\n\n        # Sample instances from each label\n        sampled_indices: list[int] = []\n        for label, num_samples_for_the_label in zip(labels, num_samples_list):\n            sampled_indices += self._rnd.sample(\n                self._label_to_ids[label],\n                num_samples_for_the_label,\n            )\n        self._rnd.shuffle(sampled_indices)\n\n        return [self.dataset[i] for i in sampled_indices]\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(dataset={self.dataset}, num_shots={self.num_shots})\"\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.balanced.BalancedFewShotGenerator.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset = dataset\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.balanced.BalancedFewShotGenerator.num_shots","title":"num_shots  <code>instance-attribute</code>","text":"<pre><code>num_shots = num_shots\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.balanced.BalancedFewShotGenerator.__init__","title":"__init__","text":"<pre><code>__init__(dataset: GenerationDataset, num_shots: int, seed: int = 42, num_trials_to_avoid_leak: int = 3) -&gt; None\n</code></pre> Source code in <code>flexeval/core/few_shot_generator/balanced.py</code> <pre><code>def __init__(\n    self,\n    dataset: GenerationDataset,\n    num_shots: int,\n    seed: int = 42,\n    num_trials_to_avoid_leak: int = 3,\n) -&gt; None:\n    super().__init__(num_trials_to_avoid_leak=num_trials_to_avoid_leak)\n    if not isinstance(dataset, GenerationDataset):\n        msg = \"BalancedFewShotGenerator only supports GenerationDataset\"\n        raise TypeError(msg)\n\n    if num_shots &gt; len(dataset):\n        msg = (\n            f\"`num_shots` should be less than or equal to the number of instances in `dataset`. \"\n            f\"num_shots: {num_shots}, len(dataset): {len(dataset)}\"\n        )\n        raise ValueError(msg)\n\n    self.dataset = dataset\n    self.num_shots = num_shots\n    self._rnd = random.Random(seed)\n\n    # Separate instances by label\n    # Here we assume that the label is the first element of references of the instance.\n    label_to_ids: dict[str, list[int]] = defaultdict(list)\n    for i, instance in enumerate(dataset):\n        label_to_ids[instance.references[0]].append(i)\n    self._label_to_ids = label_to_ids\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.balanced.BalancedFewShotGenerator.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/few_shot_generator/balanced.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(dataset={self.dataset}, num_shots={self.num_shots})\"\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.rand.RandomFewShotGenerator","title":"RandomFewShotGenerator","text":"Source code in <code>flexeval/core/few_shot_generator/rand.py</code> <pre><code>class RandomFewShotGenerator(FewShotGenerator):\n    def __init__(\n        self,\n        dataset: Dataset,\n        num_shots: int,\n        seed: int = 42,\n        num_trials_to_avoid_leak: int = 3,\n    ) -&gt; None:\n        super().__init__(num_trials_to_avoid_leak=num_trials_to_avoid_leak)\n\n        if num_shots &gt; len(dataset):\n            msg = (\n                f\"`num_shots` should be less than or equal to the number of instances in `dataset`. \"\n                f\"num_shots: {num_shots}, len(dataset): {len(dataset)}\"\n            )\n            raise ValueError(msg)\n\n        self.dataset = dataset\n        self.num_shots = num_shots\n        self._rnd = random.Random(seed)\n\n    def _sample_instances(self, eval_inputs: list[dict[str, Any]] | dict[str, Any] | None = None) -&gt; list[Instance]:\n        sampled_indices = self._rnd.sample(range(len(self.dataset)), self.num_shots)\n        return [self.dataset[i] for i in sampled_indices]\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(dataset={self.dataset}, num_shots={self.num_shots})\"\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.rand.RandomFewShotGenerator.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset = dataset\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.rand.RandomFewShotGenerator.num_shots","title":"num_shots  <code>instance-attribute</code>","text":"<pre><code>num_shots = num_shots\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.rand.RandomFewShotGenerator.__init__","title":"__init__","text":"<pre><code>__init__(dataset: Dataset, num_shots: int, seed: int = 42, num_trials_to_avoid_leak: int = 3) -&gt; None\n</code></pre> Source code in <code>flexeval/core/few_shot_generator/rand.py</code> <pre><code>def __init__(\n    self,\n    dataset: Dataset,\n    num_shots: int,\n    seed: int = 42,\n    num_trials_to_avoid_leak: int = 3,\n) -&gt; None:\n    super().__init__(num_trials_to_avoid_leak=num_trials_to_avoid_leak)\n\n    if num_shots &gt; len(dataset):\n        msg = (\n            f\"`num_shots` should be less than or equal to the number of instances in `dataset`. \"\n            f\"num_shots: {num_shots}, len(dataset): {len(dataset)}\"\n        )\n        raise ValueError(msg)\n\n    self.dataset = dataset\n    self.num_shots = num_shots\n    self._rnd = random.Random(seed)\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.rand.RandomFewShotGenerator.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/few_shot_generator/rand.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(dataset={self.dataset}, num_shots={self.num_shots})\"\n</code></pre>"},{"location":"api_reference/GenerationDataset/","title":"GenerationDataset","text":""},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationDataset","title":"GenerationDataset","text":"<p>A dataset holding <code>GenerationInstance</code>.</p> Source code in <code>flexeval/core/generation_dataset/base.py</code> <pre><code>class GenerationDataset(Sequence[GenerationInstance], ABC):\n    \"\"\"A dataset holding `GenerationInstance`.\"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the number of instances in the dataset.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __getitem__(self, i: int) -&gt; GenerationInstance:\n        \"\"\"\n        Returns the i-th instance.\n        \"\"\"\n        raise NotImplementedError\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationDataset.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of instances in the dataset.</p> Source code in <code>flexeval/core/generation_dataset/base.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"\n    Returns the number of instances in the dataset.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationDataset.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(i: int) -&gt; GenerationInstance\n</code></pre> <p>Returns the i-th instance.</p> Source code in <code>flexeval/core/generation_dataset/base.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, i: int) -&gt; GenerationInstance:\n    \"\"\"\n    Returns the i-th instance.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/generation_dataset/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationInstance","title":"GenerationInstance  <code>dataclass</code>","text":"<p>A dataclass representing a single input-output pair of a generation task.</p> Source code in <code>flexeval/core/generation_dataset/base.py</code> <pre><code>@dataclass\nclass GenerationInstance:\n    \"\"\"\n    A dataclass representing a single input-output pair of a generation task.\n    \"\"\"\n\n    inputs: dict[str, Any]\n    \"\"\"\n    Inputs of the generation task.\n    This will be embedded into the prompt for the language model in `PromptTemplate`.\n    \"\"\"\n    references: list[str] = field(default_factory=list)\n    \"\"\"\n    Reference outputs for the generation task.\n    The model's output will be evaluated against these references in `Metric`.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationInstance.inputs","title":"inputs  <code>instance-attribute</code>","text":"<pre><code>inputs: dict[str, Any]\n</code></pre> <p>Inputs of the generation task. This will be embedded into the prompt for the language model in <code>PromptTemplate</code>.</p>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationInstance.references","title":"references  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>references: list[str] = field(default_factory=list)\n</code></pre> <p>Reference outputs for the generation task. The model's output will be evaluated against these references in <code>Metric</code>.</p>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationInstance.__init__","title":"__init__","text":"<pre><code>__init__(inputs: dict[str, Any], references: list[str] = list()) -&gt; None\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.HFGenerationDataset","title":"HFGenerationDataset","text":"<p>Load GenerationInstances from a huggingface dataset.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the Hugging Face dataset.</p> </li> <li> <code>split</code>               (<code>str</code>)           \u2013            <p>The split of the dataset.</p> </li> <li> <code>subset</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The subset of the dataset.</p> </li> <li> <code>dataset_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>The additional keyword arguments for loading the dataset.</p> </li> </ul> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>class HFGenerationDataset(TemplateGenerationDataset):\n    \"\"\"\n    Load GenerationInstances from a huggingface dataset.\n\n    Args:\n        path: The path to the Hugging Face dataset.\n        split: The split of the dataset.\n        subset: The subset of the dataset.\n        dataset_kwargs: The additional keyword arguments for loading the dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        split: str,\n        subset: str | None = None,\n        dataset_kwargs: dict[str, Any] | None = None,\n        reference_template: str | None = None,\n        reference_list_template: str | None = None,\n        input_templates: dict[str, str] | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        dataset_kwargs = dataset_kwargs or {}\n        dataset = datasets.load_dataset(path, name=subset, split=split, **dataset_kwargs)\n        items = [dict(item) for item in dataset]\n\n        super().__init__(\n            items=items,\n            reference_template=reference_template,\n            reference_list_template=reference_list_template,\n            input_templates=input_templates,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.HFGenerationDataset.__init__","title":"__init__","text":"<pre><code>__init__(path: str, split: str, subset: str | None = None, dataset_kwargs: dict[str, Any] | None = None, reference_template: str | None = None, reference_list_template: str | None = None, input_templates: dict[str, str] | None = None, data_range: tuple[int, int] | None = None, keep_conditions: dict[str, str] | None = None, remove_conditions: dict[str, str] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    split: str,\n    subset: str | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    input_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    dataset_kwargs = dataset_kwargs or {}\n    dataset = datasets.load_dataset(path, name=subset, split=split, **dataset_kwargs)\n    items = [dict(item) for item in dataset]\n\n    super().__init__(\n        items=items,\n        reference_template=reference_template,\n        reference_list_template=reference_list_template,\n        input_templates=input_templates,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.JsonlGenerationDataset","title":"JsonlGenerationDataset","text":"<p>Load GenerationInstances from a JSONL file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the JSONL file.</p> </li> </ul> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>class JsonlGenerationDataset(TemplateGenerationDataset):\n    \"\"\"\n    Load GenerationInstances from a JSONL file.\n\n    Args:\n        path: The path to the JSONL file.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        reference_template: str | None = None,\n        reference_list_template: str | None = None,\n        input_templates: dict[str, str] | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        with open(path) as f:\n            items = [json.loads(line) for line in f]\n\n        super().__init__(\n            items=items,\n            reference_template=reference_template,\n            reference_list_template=reference_list_template,\n            input_templates=input_templates,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.JsonlGenerationDataset.__init__","title":"__init__","text":"<pre><code>__init__(path: str, reference_template: str | None = None, reference_list_template: str | None = None, input_templates: dict[str, str] | None = None, data_range: tuple[int, int] | None = None, keep_conditions: dict[str, str] | None = None, remove_conditions: dict[str, str] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    input_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    with open(path) as f:\n        items = [json.loads(line) for line in f]\n\n    super().__init__(\n        items=items,\n        reference_template=reference_template,\n        reference_list_template=reference_list_template,\n        input_templates=input_templates,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset","title":"TemplateGenerationDataset","text":"<p>Load GenerationInstances from a JSONL file.</p> <p>Parameters:</p> <ul> <li> <code>items</code>               (<code>list[dict[str, Any]]</code>)           \u2013            <p>A list of dict items.</p> </li> <li> <code>reference_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Specify the Jinja2 template to render the reference string if the dataset has a single reference.</p> </li> <li> <code>reference_list_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Specify the Jinja2 template to render a list of reference strings if the dataset has multiple references.</p> </li> <li> <code>input_templates</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of Jinja2 templates for the inputs.</p> </li> <li> <code>data_range</code>               (<code>tuple[int, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The range of data to use.</p> </li> <li> <code>keep_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to filter certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.</p> </li> <li> <code>remove_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to remove certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.</p> </li> </ul> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>class TemplateGenerationDataset(GenerationDataset):\n    \"\"\"\n    Load GenerationInstances from a JSONL file.\n\n    Args:\n        items: A list of dict items.\n        reference_template: Specify the Jinja2 template to render the reference string\n            if the dataset has a single reference.\n        reference_list_template: Specify the Jinja2 template to render a list of reference strings\n            if the dataset has multiple references.\n        input_templates: A dictionary of Jinja2 templates for the inputs.\n        data_range: The range of data to use.\n        keep_conditions: A dictionary to indicate the condition to filter certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.\n        remove_conditions: A dictionary to indicate the condition to remove certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.\n    \"\"\"\n\n    def __init__(\n        self,\n        items: list[dict[str, Any]],\n        reference_template: str | None = None,\n        reference_list_template: str | None = None,\n        input_templates: dict[str, str] | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        if reference_template and reference_list_template:\n            msg = \"Only one of reference_template and reference_list_template can be set.\"\n            raise ValueError(msg)\n\n        if data_range:\n            start, end = data_range\n            items = items[start:end]\n\n        keep_conditions = keep_conditions or {}\n        for template_str, value_to_keep in keep_conditions.items():\n            key_template = JINJA2_ENV.from_string(template_str)\n            items = [item for item in items if key_template.render(**item) == value_to_keep]\n        remove_conditions = remove_conditions or {}\n        for template_str, value_to_remove in remove_conditions.items():\n            key_template = JINJA2_ENV.from_string(template_str)\n            items = [item for item in items if key_template.render(**item) != value_to_remove]\n\n        self.items = items\n        input_templates = input_templates or {}\n        self.input_templates: dict[str, Template] = {k: JINJA2_ENV.from_string(v) for k, v in input_templates.items()}\n        self.reference_template = JINJA2_ENV.from_string(reference_template) if reference_template else None\n        self.reference_list_template = (\n            JINJA2_ENV.from_string(reference_list_template) if reference_list_template else None\n        )\n\n    def __len__(self) -&gt; int:\n        return len(self.items)\n\n    def __getitem__(self, i: int) -&gt; GenerationInstance:\n        item = self.items[i]\n        inputs = dict(item.items())\n        inputs.update({k: v.render(**item) for k, v in self.input_templates.items()})\n\n        reference_list: list[str] = []\n        if self.reference_template:\n            reference_string = self.reference_template.render(**item)\n            reference_list.append(reference_string)\n        if self.reference_list_template:\n            reference_list_string = self.reference_list_template.render(**item)\n            if not (reference_list_string.startswith(\"[\") and reference_list_string.endswith(\"]\")):\n                msg = (\n                    f\"The reference_list_template should render a list of strings \"\n                    f\"but we got `{reference_list_string}`.\"\n                )\n                raise ValueError(msg)\n            reference_list.extend([str(ref) for ref in literal_eval(reference_list_string)])\n        return GenerationInstance(inputs=inputs, references=reference_list)\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.items","title":"items  <code>instance-attribute</code>","text":"<pre><code>items = items\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.input_templates","title":"input_templates  <code>instance-attribute</code>","text":"<pre><code>input_templates: dict[str, Template] = {k: from_string(v)for (k, v) in items()}\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.reference_template","title":"reference_template  <code>instance-attribute</code>","text":"<pre><code>reference_template = from_string(reference_template) if reference_template else None\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.reference_list_template","title":"reference_list_template  <code>instance-attribute</code>","text":"<pre><code>reference_list_template = from_string(reference_list_template) if reference_list_template else None\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.__init__","title":"__init__","text":"<pre><code>__init__(items: list[dict[str, Any]], reference_template: str | None = None, reference_list_template: str | None = None, input_templates: dict[str, str] | None = None, data_range: tuple[int, int] | None = None, keep_conditions: dict[str, str] | None = None, remove_conditions: dict[str, str] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    items: list[dict[str, Any]],\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    input_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    if reference_template and reference_list_template:\n        msg = \"Only one of reference_template and reference_list_template can be set.\"\n        raise ValueError(msg)\n\n    if data_range:\n        start, end = data_range\n        items = items[start:end]\n\n    keep_conditions = keep_conditions or {}\n    for template_str, value_to_keep in keep_conditions.items():\n        key_template = JINJA2_ENV.from_string(template_str)\n        items = [item for item in items if key_template.render(**item) == value_to_keep]\n    remove_conditions = remove_conditions or {}\n    for template_str, value_to_remove in remove_conditions.items():\n        key_template = JINJA2_ENV.from_string(template_str)\n        items = [item for item in items if key_template.render(**item) != value_to_remove]\n\n    self.items = items\n    input_templates = input_templates or {}\n    self.input_templates: dict[str, Template] = {k: JINJA2_ENV.from_string(v) for k, v in input_templates.items()}\n    self.reference_template = JINJA2_ENV.from_string(reference_template) if reference_template else None\n    self.reference_list_template = (\n        JINJA2_ENV.from_string(reference_list_template) if reference_list_template else None\n    )\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.items)\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; GenerationInstance\n</code></pre> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>def __getitem__(self, i: int) -&gt; GenerationInstance:\n    item = self.items[i]\n    inputs = dict(item.items())\n    inputs.update({k: v.render(**item) for k, v in self.input_templates.items()})\n\n    reference_list: list[str] = []\n    if self.reference_template:\n        reference_string = self.reference_template.render(**item)\n        reference_list.append(reference_string)\n    if self.reference_list_template:\n        reference_list_string = self.reference_list_template.render(**item)\n        if not (reference_list_string.startswith(\"[\") and reference_list_string.endswith(\"]\")):\n            msg = (\n                f\"The reference_list_template should render a list of strings \"\n                f\"but we got `{reference_list_string}`.\"\n            )\n            raise ValueError(msg)\n        reference_list.extend([str(ref) for ref in literal_eval(reference_list_string)])\n    return GenerationInstance(inputs=inputs, references=reference_list)\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.sacrebleu_dataset.SacreBleuDataset","title":"SacreBleuDataset","text":"<p>Load datasets from the sacrebleu library. The available datasets are defined in sacrebleu.DATASETS.</p> Source code in <code>flexeval/core/generation_dataset/sacrebleu_dataset.py</code> <pre><code>class SacreBleuDataset(GenerationDataset):\n    \"\"\"Load datasets from the [sacrebleu](https://github.com/mjpost/sacrebleu) library.\n    The available datasets are defined in sacrebleu.DATASETS.\n    \"\"\"\n\n    def __init__(self, name: str, langpair: str) -&gt; None:\n        self._source_list: list[str] = list(sacrebleu.DATASETS[name].source(langpair))\n        self._references_list: list[list[str]] = [\n            [r.strip() for r in refs] for refs in sacrebleu.DATASETS[name].references(langpair)\n        ]\n\n        if len(self._source_list) != len(self._references_list):\n            msg = \"The number of source and reference pairs should be the same.\"\n            raise ValueError(msg)\n\n    def __len__(self) -&gt; int:\n        return len(self._source_list)\n\n    def __getitem__(self, i: int) -&gt; GenerationInstance:\n        return GenerationInstance(\n            inputs={\"source\": self._source_list[i]},\n            references=self._references_list[i],\n        )\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.sacrebleu_dataset.SacreBleuDataset.__init__","title":"__init__","text":"<pre><code>__init__(name: str, langpair: str) -&gt; None\n</code></pre> Source code in <code>flexeval/core/generation_dataset/sacrebleu_dataset.py</code> <pre><code>def __init__(self, name: str, langpair: str) -&gt; None:\n    self._source_list: list[str] = list(sacrebleu.DATASETS[name].source(langpair))\n    self._references_list: list[list[str]] = [\n        [r.strip() for r in refs] for refs in sacrebleu.DATASETS[name].references(langpair)\n    ]\n\n    if len(self._source_list) != len(self._references_list):\n        msg = \"The number of source and reference pairs should be the same.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.sacrebleu_dataset.SacreBleuDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/generation_dataset/sacrebleu_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._source_list)\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.sacrebleu_dataset.SacreBleuDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; GenerationInstance\n</code></pre> Source code in <code>flexeval/core/generation_dataset/sacrebleu_dataset.py</code> <pre><code>def __getitem__(self, i: int) -&gt; GenerationInstance:\n    return GenerationInstance(\n        inputs={\"source\": self._source_list[i]},\n        references=self._references_list[i],\n    )\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/","title":"HFMultipleChoiceDataset","text":""},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceDataset","title":"MultipleChoiceDataset","text":"Source code in <code>flexeval/core/multiple_choice_dataset/base.py</code> <pre><code>class MultipleChoiceDataset(Sequence[MultipleChoiceInstance], ABC):\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the number of instances in the dataset.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __getitem__(self, i: int) -&gt; MultipleChoiceInstance:\n        \"\"\"\n        Returns the i-th instance.\n        \"\"\"\n        raise NotImplementedError\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceDataset.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of instances in the dataset.</p> Source code in <code>flexeval/core/multiple_choice_dataset/base.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"\n    Returns the number of instances in the dataset.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceDataset.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(i: int) -&gt; MultipleChoiceInstance\n</code></pre> <p>Returns the i-th instance.</p> Source code in <code>flexeval/core/multiple_choice_dataset/base.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, i: int) -&gt; MultipleChoiceInstance:\n    \"\"\"\n    Returns the i-th instance.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/multiple_choice_dataset/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceInstance","title":"MultipleChoiceInstance  <code>dataclass</code>","text":"<p>A dataclass representing a single input-output pair of a multiple-choice task.</p> Source code in <code>flexeval/core/multiple_choice_dataset/base.py</code> <pre><code>@dataclass\nclass MultipleChoiceInstance:\n    \"\"\"\n    A dataclass representing a single input-output pair of a multiple-choice task.\n    \"\"\"\n\n    inputs: dict[str, str]\n    \"\"\"\n    Inputs of the multiple-choice task.\n    This will be embedded into the prompt for the language model in `PromptTemplate`.\n    \"\"\"\n    choices: list[str]\n    \"\"\"\n    Choices for the multiple-choice task.\n    `LanguageModel` will choose the answer based on the log-probabilities of these choices.\n    \"\"\"\n    answer_index: int\n    \"\"\"\n    Index of the correct answer in `choices`.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceInstance.inputs","title":"inputs  <code>instance-attribute</code>","text":"<pre><code>inputs: dict[str, str]\n</code></pre> <p>Inputs of the multiple-choice task. This will be embedded into the prompt for the language model in <code>PromptTemplate</code>.</p>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceInstance.choices","title":"choices  <code>instance-attribute</code>","text":"<pre><code>choices: list[str]\n</code></pre> <p>Choices for the multiple-choice task. <code>LanguageModel</code> will choose the answer based on the log-probabilities of these choices.</p>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceInstance.answer_index","title":"answer_index  <code>instance-attribute</code>","text":"<pre><code>answer_index: int\n</code></pre> <p>Index of the correct answer in <code>choices</code>.</p>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceInstance.__init__","title":"__init__","text":"<pre><code>__init__(inputs: dict[str, str], choices: list[str], answer_index: int) -&gt; None\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.HFMultipleChoiceDataset","title":"HFMultipleChoiceDataset","text":"<p>Load MultipleChoiceInstance from a huggingface dataset.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The name or path of the huggingface dataset.</p> </li> <li> <code>split</code>               (<code>str</code>)           \u2013            <p>The split of the dataset to use.</p> </li> <li> <code>subset</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The subset of the dataset to use.</p> </li> <li> <code>dataset_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>The keyword arguments for loading the dataset.</p> </li> </ul> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>class HFMultipleChoiceDataset(TemplateMultipleChoiceDataset):\n    \"\"\"\n    Load MultipleChoiceInstance from a huggingface dataset.\n\n    Args:\n        path: The name or path of the huggingface dataset.\n        split: The split of the dataset to use.\n        subset: The subset of the dataset to use.\n        dataset_kwargs: The keyword arguments for loading the dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        split: str,\n        choices_templates: list[str],\n        answer_index_template: str,\n        input_templates: dict[str, str] | None = None,\n        subset: str | None = None,\n        dataset_kwargs: dict[str, Any] | None = None,\n        whitespace_before_choices: bool = False,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        dataset_kwargs = dataset_kwargs or {}\n        items = datasets.load_dataset(path, split=split, name=subset, **dataset_kwargs)\n        items = [dict(item) for item in items]\n\n        super().__init__(\n            items=items,\n            choices_templates=choices_templates,\n            answer_index_template=answer_index_template,\n            input_templates=input_templates,\n            whitespace_before_choices=whitespace_before_choices,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.HFMultipleChoiceDataset.__init__","title":"__init__","text":"<pre><code>__init__(path: str, split: str, choices_templates: list[str], answer_index_template: str, input_templates: dict[str, str] | None = None, subset: str | None = None, dataset_kwargs: dict[str, Any] | None = None, whitespace_before_choices: bool = False, data_range: tuple[int, int] | None = None, keep_conditions: dict[str, str] | None = None, remove_conditions: dict[str, str] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    split: str,\n    choices_templates: list[str],\n    answer_index_template: str,\n    input_templates: dict[str, str] | None = None,\n    subset: str | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n    whitespace_before_choices: bool = False,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    dataset_kwargs = dataset_kwargs or {}\n    items = datasets.load_dataset(path, split=split, name=subset, **dataset_kwargs)\n    items = [dict(item) for item in items]\n\n    super().__init__(\n        items=items,\n        choices_templates=choices_templates,\n        answer_index_template=answer_index_template,\n        input_templates=input_templates,\n        whitespace_before_choices=whitespace_before_choices,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.JsonlMultipleChoiceDataset","title":"JsonlMultipleChoiceDataset","text":"<p>Load MultipleChoiceInstance from a JSONL file.</p> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>class JsonlMultipleChoiceDataset(TemplateMultipleChoiceDataset):\n    \"\"\"\n    Load MultipleChoiceInstance from a JSONL file.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        choices_templates: list[str],\n        answer_index_template: str,\n        input_templates: dict[str, str] | None = None,\n        whitespace_before_choices: bool = False,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        with open(path) as f:\n            items = [json.loads(line) for line in f]\n\n        super().__init__(\n            items=items,\n            choices_templates=choices_templates,\n            answer_index_template=answer_index_template,\n            input_templates=input_templates,\n            whitespace_before_choices=whitespace_before_choices,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.JsonlMultipleChoiceDataset.__init__","title":"__init__","text":"<pre><code>__init__(path: str, choices_templates: list[str], answer_index_template: str, input_templates: dict[str, str] | None = None, whitespace_before_choices: bool = False, data_range: tuple[int, int] | None = None, keep_conditions: dict[str, str] | None = None, remove_conditions: dict[str, str] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    choices_templates: list[str],\n    answer_index_template: str,\n    input_templates: dict[str, str] | None = None,\n    whitespace_before_choices: bool = False,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    with open(path) as f:\n        items = [json.loads(line) for line in f]\n\n    super().__init__(\n        items=items,\n        choices_templates=choices_templates,\n        answer_index_template=answer_index_template,\n        input_templates=input_templates,\n        whitespace_before_choices=whitespace_before_choices,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset","title":"TemplateMultipleChoiceDataset","text":"<p>An abstract dataset class for multiple-choice tasks. This class generates multiple-choice instances from a dict item and Jinja2 templates.</p> <p>Parameters:</p> <ul> <li> <code>items</code>               (<code>list[dict[str, Any]]</code>)           \u2013            <p>A list of dict items.</p> </li> <li> <code>choices_templates</code>               (<code>list[str]</code>)           \u2013            <p>A list of Jinja2 templates for the choices.</p> </li> <li> <code>answer_index_template</code>               (<code>str</code>)           \u2013            <p>A Jinja2 template for the index of the correct answer.</p> </li> <li> <code>input_templates</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of Jinja2 templates for the inputs.</p> </li> <li> <code>whitespace_before_choices</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add a whitespace before each choice. Maybe necessary for language with whitespaces.</p> </li> <li> <code>data_range</code>               (<code>tuple[int, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The range of data to use.</p> </li> <li> <code>keep_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to filter certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.</p> </li> <li> <code>remove_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to remove certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.</p> </li> </ul> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>class TemplateMultipleChoiceDataset(MultipleChoiceDataset):\n    \"\"\"\n    An abstract dataset class for multiple-choice tasks.\n    This class generates multiple-choice instances from a dict item and Jinja2 templates.\n\n    Args:\n        items: A list of dict items.\n        choices_templates: A list of Jinja2 templates for the choices.\n        answer_index_template: A Jinja2 template for the index of the correct answer.\n        input_templates: A dictionary of Jinja2 templates for the inputs.\n        whitespace_before_choices: Whether to add a whitespace before each choice.\n            Maybe necessary for language with whitespaces.\n        data_range: The range of data to use.\n        keep_conditions: A dictionary to indicate the condition to filter certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.\n        remove_conditions: A dictionary to indicate the condition to remove certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.\n    \"\"\"\n\n    def __init__(\n        self,\n        items: list[dict[str, Any]],\n        choices_templates: list[str],\n        answer_index_template: str,\n        input_templates: dict[str, str] | None = None,\n        whitespace_before_choices: bool = False,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        if data_range:\n            start, end = data_range\n            items = items[start:end]\n\n        keep_conditions = keep_conditions or {}\n        for template_str, value_to_keep in keep_conditions.items():\n            key_template = JINJA2_ENV.from_string(template_str)\n            items = [item for item in items if key_template.render(**item) == value_to_keep]\n        remove_conditions = remove_conditions or {}\n        for template_str, value_to_remove in remove_conditions.items():\n            key_template = JINJA2_ENV.from_string(template_str)\n            items = [item for item in items if key_template.render(**item) != value_to_remove]\n        self.items = items\n\n        input_templates = input_templates or {}\n        self.input_templates: dict[str, Template] = {k: JINJA2_ENV.from_string(v) for k, v in input_templates.items()}\n        self.choices_templates = [JINJA2_ENV.from_string(t) for t in choices_templates]\n        self.answer_index_template = JINJA2_ENV.from_string(\n            answer_index_template,\n        )\n        self.whitespace_before_choices = whitespace_before_choices\n\n    def __len__(self) -&gt; int:\n        return len(self.items)\n\n    def __getitem__(self, i: int) -&gt; MultipleChoiceInstance:\n        item = self.items[i]\n        inputs = dict(item.items())\n        inputs.update({k: v.render(**item) for k, v in self.input_templates.items()})\n\n        choices = [t.render(**item) for t in self.choices_templates]\n        choices = list(filter(lambda x: len(x) &gt; 0, choices))\n        if self.whitespace_before_choices:\n            choices = [\" \" + c for c in choices]\n\n        answer_index = int(self.answer_index_template.render(**item))\n        if not (answer_index &gt;= 0 and answer_index &lt; len(choices)):\n            msg = f\"at least {answer_index+1} choices required, but got {choices}\"\n            raise ValueError(msg)\n\n        return MultipleChoiceInstance(\n            inputs=inputs,\n            choices=choices,\n            answer_index=answer_index,\n        )\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.items","title":"items  <code>instance-attribute</code>","text":"<pre><code>items = items\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.input_templates","title":"input_templates  <code>instance-attribute</code>","text":"<pre><code>input_templates: dict[str, Template] = {k: from_string(v)for (k, v) in items()}\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.choices_templates","title":"choices_templates  <code>instance-attribute</code>","text":"<pre><code>choices_templates = [from_string(t) for t in choices_templates]\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.answer_index_template","title":"answer_index_template  <code>instance-attribute</code>","text":"<pre><code>answer_index_template = from_string(answer_index_template)\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.whitespace_before_choices","title":"whitespace_before_choices  <code>instance-attribute</code>","text":"<pre><code>whitespace_before_choices = whitespace_before_choices\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.__init__","title":"__init__","text":"<pre><code>__init__(items: list[dict[str, Any]], choices_templates: list[str], answer_index_template: str, input_templates: dict[str, str] | None = None, whitespace_before_choices: bool = False, data_range: tuple[int, int] | None = None, keep_conditions: dict[str, str] | None = None, remove_conditions: dict[str, str] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    items: list[dict[str, Any]],\n    choices_templates: list[str],\n    answer_index_template: str,\n    input_templates: dict[str, str] | None = None,\n    whitespace_before_choices: bool = False,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    if data_range:\n        start, end = data_range\n        items = items[start:end]\n\n    keep_conditions = keep_conditions or {}\n    for template_str, value_to_keep in keep_conditions.items():\n        key_template = JINJA2_ENV.from_string(template_str)\n        items = [item for item in items if key_template.render(**item) == value_to_keep]\n    remove_conditions = remove_conditions or {}\n    for template_str, value_to_remove in remove_conditions.items():\n        key_template = JINJA2_ENV.from_string(template_str)\n        items = [item for item in items if key_template.render(**item) != value_to_remove]\n    self.items = items\n\n    input_templates = input_templates or {}\n    self.input_templates: dict[str, Template] = {k: JINJA2_ENV.from_string(v) for k, v in input_templates.items()}\n    self.choices_templates = [JINJA2_ENV.from_string(t) for t in choices_templates]\n    self.answer_index_template = JINJA2_ENV.from_string(\n        answer_index_template,\n    )\n    self.whitespace_before_choices = whitespace_before_choices\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.items)\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; MultipleChoiceInstance\n</code></pre> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>def __getitem__(self, i: int) -&gt; MultipleChoiceInstance:\n    item = self.items[i]\n    inputs = dict(item.items())\n    inputs.update({k: v.render(**item) for k, v in self.input_templates.items()})\n\n    choices = [t.render(**item) for t in self.choices_templates]\n    choices = list(filter(lambda x: len(x) &gt; 0, choices))\n    if self.whitespace_before_choices:\n        choices = [\" \" + c for c in choices]\n\n    answer_index = int(self.answer_index_template.render(**item))\n    if not (answer_index &gt;= 0 and answer_index &lt; len(choices)):\n        msg = f\"at least {answer_index+1} choices required, but got {choices}\"\n        raise ValueError(msg)\n\n    return MultipleChoiceInstance(\n        inputs=inputs,\n        choices=choices,\n        answer_index=answer_index,\n    )\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/","title":"HFRewardBenchDataset","text":""},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchDataset","title":"RewardBenchDataset","text":"Source code in <code>flexeval/core/reward_bench_dataset/base.py</code> <pre><code>class RewardBenchDataset(Sequence[RewardBenchInstance], ABC):\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of instances in the dataset.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __getitem__(self, i: int) -&gt; RewardBenchInstance:\n        \"\"\"Returns the i-th instance.\"\"\"\n        raise NotImplementedError\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchDataset.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of instances in the dataset.</p> Source code in <code>flexeval/core/reward_bench_dataset/base.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Returns the number of instances in the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchDataset.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(i: int) -&gt; RewardBenchInstance\n</code></pre> <p>Returns the i-th instance.</p> Source code in <code>flexeval/core/reward_bench_dataset/base.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, i: int) -&gt; RewardBenchInstance:\n    \"\"\"Returns the i-th instance.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/reward_bench_dataset/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchInstance","title":"RewardBenchInstance  <code>dataclass</code>","text":"<p>A dataclass representing a triplet (prompt, chosen, rejected) of a reward bench task.</p> Source code in <code>flexeval/core/reward_bench_dataset/base.py</code> <pre><code>@dataclass\nclass RewardBenchInstance:\n    \"\"\"A dataclass representing a triplet (prompt, chosen, rejected) of a\n    reward bench task.\"\"\"\n\n    prompt: list[dict[str, str]]\n    \"\"\"\n    The prompt for chosen/rejected responses.\n    The format is a list of dictionaries, where each dictionary represents an OpenAI-format chat message,\n    such as `{\"role\": \"user\", \"content\": \"Hello!\"}`.\n    \"\"\"\n    chosen: list[dict[str, str]]\n    \"\"\"\n    The chosen response to the prompt.\n    The format is the same as `prompt`.\n    \"\"\"\n    rejected: list[dict[str, str]]\n    \"\"\"\n    The rejected response to the prompt.\n    The format is the same as `prompt`.\n    \"\"\"\n    extra_info: dict[str, Any]\n    \"\"\"\n    Extra information that can be used by passing to `Metric`.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchInstance.prompt","title":"prompt  <code>instance-attribute</code>","text":"<pre><code>prompt: list[dict[str, str]]\n</code></pre> <p>The prompt for chosen/rejected responses. The format is a list of dictionaries, where each dictionary represents an OpenAI-format chat message, such as <code>{\"role\": \"user\", \"content\": \"Hello!\"}</code>.</p>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchInstance.chosen","title":"chosen  <code>instance-attribute</code>","text":"<pre><code>chosen: list[dict[str, str]]\n</code></pre> <p>The chosen response to the prompt. The format is the same as <code>prompt</code>.</p>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchInstance.rejected","title":"rejected  <code>instance-attribute</code>","text":"<pre><code>rejected: list[dict[str, str]]\n</code></pre> <p>The rejected response to the prompt. The format is the same as <code>prompt</code>.</p>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchInstance.extra_info","title":"extra_info  <code>instance-attribute</code>","text":"<pre><code>extra_info: dict[str, Any]\n</code></pre> <p>Extra information that can be used by passing to <code>Metric</code>.</p>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchInstance.__init__","title":"__init__","text":"<pre><code>__init__(prompt: list[dict[str, str]], chosen: list[dict[str, str]], rejected: list[dict[str, str]], extra_info: dict[str, Any]) -&gt; None\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.template_based.HFRewardBenchDataset","title":"HFRewardBenchDataset","text":"<p>Load RewardBenchInstances from a Hugging Face dataset.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the Hugging Face dataset.</p> </li> <li> <code>split</code>               (<code>str</code>)           \u2013            <p>The split of the dataset.</p> </li> <li> <code>subset</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The subset of the dataset.</p> </li> <li> <code>dataset_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>The keyword arguments to pass to the Hugging Face dataset.</p> </li> </ul> Source code in <code>flexeval/core/reward_bench_dataset/template_based.py</code> <pre><code>class HFRewardBenchDataset(TemplateRewardBenchDataset):\n    \"\"\"\n    Load RewardBenchInstances from a Hugging Face dataset.\n\n    Args:\n        path: The path to the Hugging Face dataset.\n        split: The split of the dataset.\n        subset: The subset of the dataset.\n        dataset_kwargs: The keyword arguments to pass to the Hugging Face dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        split: str,\n        subset: str | None = None,\n        dataset_kwargs: dict[str, Any] | None = None,\n        prompt_template: str = \"{{ prompt }}\",\n        chosen_template: str = \"{{ chosen }}\",\n        rejected_template: str = \"{{ rejected }}\",\n        extra_info_templates: dict[str, str] | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        dataset_kwargs = dataset_kwargs or {}\n        dataset = datasets.load_dataset(path, name=subset, split=split, **dataset_kwargs)\n        items = [dict(item) for item in dataset]\n\n        super().__init__(\n            items=items,\n            prompt_template=prompt_template,\n            chosen_template=chosen_template,\n            rejected_template=rejected_template,\n            extra_info_templates=extra_info_templates,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.template_based.HFRewardBenchDataset.__init__","title":"__init__","text":"<pre><code>__init__(path: str, split: str, subset: str | None = None, dataset_kwargs: dict[str, Any] | None = None, prompt_template: str = '{{ prompt }}', chosen_template: str = '{{ chosen }}', rejected_template: str = '{{ rejected }}', extra_info_templates: dict[str, str] | None = None, data_range: tuple[int, int] | None = None, keep_conditions: dict[str, str] | None = None, remove_conditions: dict[str, str] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/reward_bench_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    split: str,\n    subset: str | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n    prompt_template: str = \"{{ prompt }}\",\n    chosen_template: str = \"{{ chosen }}\",\n    rejected_template: str = \"{{ rejected }}\",\n    extra_info_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    dataset_kwargs = dataset_kwargs or {}\n    dataset = datasets.load_dataset(path, name=subset, split=split, **dataset_kwargs)\n    items = [dict(item) for item in dataset]\n\n    super().__init__(\n        items=items,\n        prompt_template=prompt_template,\n        chosen_template=chosen_template,\n        rejected_template=rejected_template,\n        extra_info_templates=extra_info_templates,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.template_based.JsonlRewardBenchDataset","title":"JsonlRewardBenchDataset","text":"<p>Load RewardBenchInstances from a JSONL file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the JSONL file.</p> </li> </ul> Source code in <code>flexeval/core/reward_bench_dataset/template_based.py</code> <pre><code>class JsonlRewardBenchDataset(TemplateRewardBenchDataset):\n    \"\"\"\n    Load RewardBenchInstances from a JSONL file.\n\n    Args:\n        path: The path to the JSONL file.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        prompt_template: str = \"{{ prompt }}\",\n        chosen_template: str = \"{{ chosen }}\",\n        rejected_template: str = \"{{ rejected }}\",\n        extra_info_templates: dict[str, str] | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        with open(path) as f:\n            items = [json.loads(line) for line in f]\n\n        super().__init__(\n            items=items,\n            prompt_template=prompt_template,\n            chosen_template=chosen_template,\n            rejected_template=rejected_template,\n            extra_info_templates=extra_info_templates,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.template_based.JsonlRewardBenchDataset.__init__","title":"__init__","text":"<pre><code>__init__(path: str, prompt_template: str = '{{ prompt }}', chosen_template: str = '{{ chosen }}', rejected_template: str = '{{ rejected }}', extra_info_templates: dict[str, str] | None = None, data_range: tuple[int, int] | None = None, keep_conditions: dict[str, str] | None = None, remove_conditions: dict[str, str] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/reward_bench_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    prompt_template: str = \"{{ prompt }}\",\n    chosen_template: str = \"{{ chosen }}\",\n    rejected_template: str = \"{{ rejected }}\",\n    extra_info_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    with open(path) as f:\n        items = [json.loads(line) for line in f]\n\n    super().__init__(\n        items=items,\n        prompt_template=prompt_template,\n        chosen_template=chosen_template,\n        rejected_template=rejected_template,\n        extra_info_templates=extra_info_templates,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/LanguageModel/","title":"LanguageModel","text":""},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.base.LanguageModel","title":"LanguageModel","text":"<p>LanguageModel is what you want to evaluate with this library.</p> <p>It can generate text based on the input text, response to chat messages, and compute log probabilities.</p> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>class LanguageModel:\n    \"\"\"LanguageModel is what you want to evaluate with this library.\n\n    It can generate text based on the input text, response to chat messages, and compute log probabilities.\n    \"\"\"\n\n    def batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        \"\"\"\n        Generate text based on the input text list.\n\n        Args:\n            text_list: A list of input texts.\n            stop_sequences: A string or a list of strings that will stop the generation when they are generated.\n                This argument exists to give a common interface to various models that have different names for it.\n            max_new_tokens: The maximum number of tokens to generate for each text.\n                This argument exists to give a common interface to various models that have different names for it.\n            **kwargs: Additional keyword arguments for text generation.\n                The acceptable keys depend on the specific implementation of the model.\n                These arguments override corresponding values in the model's default_gen_kwargs.\n                Special cases:\n                - 'stop_sequences' or any similar model-specific kwargs:\n                    Merged with default_gen_kwargs instead of overriding.\n        \"\"\"\n        msg = f\"{self.__class__.__name__} cannot generate text.\"\n        raise NotImplementedError(msg)\n\n    def batch_generate_chat_response(\n        self,\n        chat_messages_list: list[list[dict[str, str]]],\n        **kwargs,\n    ) -&gt; list[str]:\n        \"\"\"Generate chat responses based on the chat messages in the list.\n        This method is used for chatbot models.\n\n        Args:\n            chat_messages_list: A list of chat messages.\n        \"\"\"\n        msg = f\"{self.__class__.__name__} cannot generate chat responses.\"\n        raise NotImplementedError(msg)\n\n    def batch_compute_log_probs(\n        self,\n        text_list: list[str],\n        prefix_list: list[str] | None = None,\n        stride: int | None = None,\n    ) -&gt; list[float]:\n        \"\"\"\n        Compute log probabilities of the text list.\n        Used for compute perplexity of text, or solving multiple choice questions.\n\n        Args:\n            text_list: A list of texts to compute log probabilities.\n            prefix_list: A list of prefixes for each text.\n            stride: The stride for computing log probabilities.\n        \"\"\"\n        msg = f\"{self.__class__.__name__} cannot compute perplexity.\"\n        raise NotImplementedError(msg)\n\n    def batch_compute_chat_log_probs(\n        self, prompt_list: list[list[dict[str, str]]], response_list: list[dict[str, str]]\n    ) -&gt; list[float]:\n        \"\"\"\n        Compute log probabilities of the chat responses given the chat history.\n\n        Args:\n            prompt_list: A list of chat histories.\n            response_list: A list of chat responses.\n        \"\"\"\n        msg = f\"{self.__class__.__name__} cannot compute chat log probabilities.\"\n        raise NotImplementedError(msg)\n\n    @final\n    def complete_text(\n        self,\n        text_list: str | list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; str | list[str]:\n        \"\"\"\n        A wrapper for `batch_complete_text` that accepts a single text or a list of texts.\n        This is a convenient method for end-users.\n        To implement generation logic, you should override `batch_complete_text` method.\n        \"\"\"\n\n        if isinstance(text_list, str):\n            return self.batch_complete_text([text_list], stop_sequences, max_new_tokens, **kwargs)[0]\n        return self.batch_complete_text(text_list, stop_sequences, max_new_tokens, **kwargs)\n\n    @final\n    def generate_chat_response(\n        self,\n        chat_messages: list[dict[str, str]] | list[list[dict[str, str]]],\n        **kwargs,\n    ) -&gt; str | list[str]:\n        \"\"\"\n        A wrapper for `batch_generate_chat_response` that accepts a single chat message or a list of chat messages.\n        This is a convenient method for end-users.\n        To implement generation logic, you should override `batch_generate_chat_response` method.\n        \"\"\"\n\n        if isinstance(chat_messages[0], dict):\n            return self.batch_generate_chat_response([chat_messages], **kwargs)[0]\n        return self.batch_generate_chat_response(chat_messages, **kwargs)\n\n    @final\n    def compute_log_probs(\n        self,\n        text_list: str | list[str],\n        prefix_list: list[str] | None = None,\n        stride: int | None = None,\n    ) -&gt; float | list[float]:\n        \"\"\"\n        A wrapper for `batch_compute_log_probs` that accepts a single text or a list of texts.\n        This is a convenient method for end-users.\n        To implement computation logic, you should override `batch_compute_log_probs` method.\n        \"\"\"\n\n        if isinstance(text_list, str):\n            return self.batch_compute_log_probs([text_list], prefix_list, stride)[0]\n        return self.batch_compute_log_probs(text_list, prefix_list, stride)\n\n    @final\n    def compute_chat_log_probs(\n        self, prompt: list[dict[str, str]] | list[list[dict[str, str]]], response: dict[str, str] | list[dict[str, str]]\n    ) -&gt; float | list[float]:\n        \"\"\"\n        A wrapper for `batch_compute_chat_log_probs` that accepts a single chat prompt or a list of chat prompts.\n        This is a convenient method for end-users.\n        To implement computation logic, you should override `batch_compute_chat_log_probs` method.\n        \"\"\"\n\n        if isinstance(prompt[0], dict):\n            return self.batch_compute_chat_log_probs([prompt], [response])[0]\n        return self.batch_compute_chat_log_probs(prompt, response)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.base.LanguageModel.batch_complete_text","title":"batch_complete_text","text":"<pre><code>batch_complete_text(text_list: list[str], stop_sequences: str | list[str] | None = None, max_new_tokens: int | None = None, **kwargs) -&gt; list[str]\n</code></pre> <p>Generate text based on the input text list.</p> <p>Parameters:</p> <ul> <li> <code>text_list</code>               (<code>list[str]</code>)           \u2013            <p>A list of input texts.</p> </li> <li> <code>stop_sequences</code>               (<code>str | list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A string or a list of strings that will stop the generation when they are generated. This argument exists to give a common interface to various models that have different names for it.</p> </li> <li> <code>max_new_tokens</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The maximum number of tokens to generate for each text. This argument exists to give a common interface to various models that have different names for it.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments for text generation. The acceptable keys depend on the specific implementation of the model. These arguments override corresponding values in the model's default_gen_kwargs. Special cases: - 'stop_sequences' or any similar model-specific kwargs:     Merged with default_gen_kwargs instead of overriding.</p> </li> </ul> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>def batch_complete_text(\n    self,\n    text_list: list[str],\n    stop_sequences: str | list[str] | None = None,\n    max_new_tokens: int | None = None,\n    **kwargs,\n) -&gt; list[str]:\n    \"\"\"\n    Generate text based on the input text list.\n\n    Args:\n        text_list: A list of input texts.\n        stop_sequences: A string or a list of strings that will stop the generation when they are generated.\n            This argument exists to give a common interface to various models that have different names for it.\n        max_new_tokens: The maximum number of tokens to generate for each text.\n            This argument exists to give a common interface to various models that have different names for it.\n        **kwargs: Additional keyword arguments for text generation.\n            The acceptable keys depend on the specific implementation of the model.\n            These arguments override corresponding values in the model's default_gen_kwargs.\n            Special cases:\n            - 'stop_sequences' or any similar model-specific kwargs:\n                Merged with default_gen_kwargs instead of overriding.\n    \"\"\"\n    msg = f\"{self.__class__.__name__} cannot generate text.\"\n    raise NotImplementedError(msg)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.base.LanguageModel.batch_generate_chat_response","title":"batch_generate_chat_response","text":"<pre><code>batch_generate_chat_response(chat_messages_list: list[list[dict[str, str]]], **kwargs) -&gt; list[str]\n</code></pre> <p>Generate chat responses based on the chat messages in the list. This method is used for chatbot models.</p> <p>Parameters:</p> <ul> <li> <code>chat_messages_list</code>               (<code>list[list[dict[str, str]]]</code>)           \u2013            <p>A list of chat messages.</p> </li> </ul> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>def batch_generate_chat_response(\n    self,\n    chat_messages_list: list[list[dict[str, str]]],\n    **kwargs,\n) -&gt; list[str]:\n    \"\"\"Generate chat responses based on the chat messages in the list.\n    This method is used for chatbot models.\n\n    Args:\n        chat_messages_list: A list of chat messages.\n    \"\"\"\n    msg = f\"{self.__class__.__name__} cannot generate chat responses.\"\n    raise NotImplementedError(msg)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.base.LanguageModel.batch_compute_log_probs","title":"batch_compute_log_probs","text":"<pre><code>batch_compute_log_probs(text_list: list[str], prefix_list: list[str] | None = None, stride: int | None = None) -&gt; list[float]\n</code></pre> <p>Compute log probabilities of the text list. Used for compute perplexity of text, or solving multiple choice questions.</p> <p>Parameters:</p> <ul> <li> <code>text_list</code>               (<code>list[str]</code>)           \u2013            <p>A list of texts to compute log probabilities.</p> </li> <li> <code>prefix_list</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of prefixes for each text.</p> </li> <li> <code>stride</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>The stride for computing log probabilities.</p> </li> </ul> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>def batch_compute_log_probs(\n    self,\n    text_list: list[str],\n    prefix_list: list[str] | None = None,\n    stride: int | None = None,\n) -&gt; list[float]:\n    \"\"\"\n    Compute log probabilities of the text list.\n    Used for compute perplexity of text, or solving multiple choice questions.\n\n    Args:\n        text_list: A list of texts to compute log probabilities.\n        prefix_list: A list of prefixes for each text.\n        stride: The stride for computing log probabilities.\n    \"\"\"\n    msg = f\"{self.__class__.__name__} cannot compute perplexity.\"\n    raise NotImplementedError(msg)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.base.LanguageModel.batch_compute_chat_log_probs","title":"batch_compute_chat_log_probs","text":"<pre><code>batch_compute_chat_log_probs(prompt_list: list[list[dict[str, str]]], response_list: list[dict[str, str]]) -&gt; list[float]\n</code></pre> <p>Compute log probabilities of the chat responses given the chat history.</p> <p>Parameters:</p> <ul> <li> <code>prompt_list</code>               (<code>list[list[dict[str, str]]]</code>)           \u2013            <p>A list of chat histories.</p> </li> <li> <code>response_list</code>               (<code>list[dict[str, str]]</code>)           \u2013            <p>A list of chat responses.</p> </li> </ul> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>def batch_compute_chat_log_probs(\n    self, prompt_list: list[list[dict[str, str]]], response_list: list[dict[str, str]]\n) -&gt; list[float]:\n    \"\"\"\n    Compute log probabilities of the chat responses given the chat history.\n\n    Args:\n        prompt_list: A list of chat histories.\n        response_list: A list of chat responses.\n    \"\"\"\n    msg = f\"{self.__class__.__name__} cannot compute chat log probabilities.\"\n    raise NotImplementedError(msg)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.base.LanguageModel.complete_text","title":"complete_text","text":"<pre><code>complete_text(text_list: str | list[str], stop_sequences: str | list[str] | None = None, max_new_tokens: int | None = None, **kwargs) -&gt; str | list[str]\n</code></pre> <p>A wrapper for <code>batch_complete_text</code> that accepts a single text or a list of texts. This is a convenient method for end-users. To implement generation logic, you should override <code>batch_complete_text</code> method.</p> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>@final\ndef complete_text(\n    self,\n    text_list: str | list[str],\n    stop_sequences: str | list[str] | None = None,\n    max_new_tokens: int | None = None,\n    **kwargs,\n) -&gt; str | list[str]:\n    \"\"\"\n    A wrapper for `batch_complete_text` that accepts a single text or a list of texts.\n    This is a convenient method for end-users.\n    To implement generation logic, you should override `batch_complete_text` method.\n    \"\"\"\n\n    if isinstance(text_list, str):\n        return self.batch_complete_text([text_list], stop_sequences, max_new_tokens, **kwargs)[0]\n    return self.batch_complete_text(text_list, stop_sequences, max_new_tokens, **kwargs)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.base.LanguageModel.generate_chat_response","title":"generate_chat_response","text":"<pre><code>generate_chat_response(chat_messages: list[dict[str, str]] | list[list[dict[str, str]]], **kwargs) -&gt; str | list[str]\n</code></pre> <p>A wrapper for <code>batch_generate_chat_response</code> that accepts a single chat message or a list of chat messages. This is a convenient method for end-users. To implement generation logic, you should override <code>batch_generate_chat_response</code> method.</p> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>@final\ndef generate_chat_response(\n    self,\n    chat_messages: list[dict[str, str]] | list[list[dict[str, str]]],\n    **kwargs,\n) -&gt; str | list[str]:\n    \"\"\"\n    A wrapper for `batch_generate_chat_response` that accepts a single chat message or a list of chat messages.\n    This is a convenient method for end-users.\n    To implement generation logic, you should override `batch_generate_chat_response` method.\n    \"\"\"\n\n    if isinstance(chat_messages[0], dict):\n        return self.batch_generate_chat_response([chat_messages], **kwargs)[0]\n    return self.batch_generate_chat_response(chat_messages, **kwargs)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.base.LanguageModel.compute_log_probs","title":"compute_log_probs","text":"<pre><code>compute_log_probs(text_list: str | list[str], prefix_list: list[str] | None = None, stride: int | None = None) -&gt; float | list[float]\n</code></pre> <p>A wrapper for <code>batch_compute_log_probs</code> that accepts a single text or a list of texts. This is a convenient method for end-users. To implement computation logic, you should override <code>batch_compute_log_probs</code> method.</p> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>@final\ndef compute_log_probs(\n    self,\n    text_list: str | list[str],\n    prefix_list: list[str] | None = None,\n    stride: int | None = None,\n) -&gt; float | list[float]:\n    \"\"\"\n    A wrapper for `batch_compute_log_probs` that accepts a single text or a list of texts.\n    This is a convenient method for end-users.\n    To implement computation logic, you should override `batch_compute_log_probs` method.\n    \"\"\"\n\n    if isinstance(text_list, str):\n        return self.batch_compute_log_probs([text_list], prefix_list, stride)[0]\n    return self.batch_compute_log_probs(text_list, prefix_list, stride)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.base.LanguageModel.compute_chat_log_probs","title":"compute_chat_log_probs","text":"<pre><code>compute_chat_log_probs(prompt: list[dict[str, str]] | list[list[dict[str, str]]], response: dict[str, str] | list[dict[str, str]]) -&gt; float | list[float]\n</code></pre> <p>A wrapper for <code>batch_compute_chat_log_probs</code> that accepts a single chat prompt or a list of chat prompts. This is a convenient method for end-users. To implement computation logic, you should override <code>batch_compute_chat_log_probs</code> method.</p> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>@final\ndef compute_chat_log_probs(\n    self, prompt: list[dict[str, str]] | list[list[dict[str, str]]], response: dict[str, str] | list[dict[str, str]]\n) -&gt; float | list[float]:\n    \"\"\"\n    A wrapper for `batch_compute_chat_log_probs` that accepts a single chat prompt or a list of chat prompts.\n    This is a convenient method for end-users.\n    To implement computation logic, you should override `batch_compute_chat_log_probs` method.\n    \"\"\"\n\n    if isinstance(prompt[0], dict):\n        return self.batch_compute_chat_log_probs([prompt], [response])[0]\n    return self.batch_compute_chat_log_probs(prompt, response)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.hf_lm.HuggingFaceLM","title":"HuggingFaceLM","text":"<p>LanguageModel implementation using Hugging Face Transformers.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The model name or path of the Hugging Face model.</p> </li> <li> <code>model_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments for the model instantiation by <code>from_pretrained()</code>.</p> </li> <li> <code>tokenizer</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The tokenizer name or path of the Hugging Face tokenizer.</p> </li> <li> <code>tokenizer_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments for the tokenizer instantiation by `from_pretrained().</p> </li> <li> <code>add_special_tokens</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add special tokens to the input. Note that whether BOS or EOS tokens are added depends on the tokenizer.</p> </li> <li> <code>amp_dtype</code>               (<code>Literal['float16', 'bfloat16'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The dtype for automatic mixed precision.</p> </li> <li> <code>random_seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for the model.</p> </li> <li> <code>load_peft</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Should be set to True when loading the model from PEFT weights.</p> </li> <li> <code>custom_chat_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A custom chat template for chatbot models. If specified, this overrides the default chat template of the tokenizer.</p> </li> </ul> Source code in <code>flexeval/core/language_model/hf_lm.py</code> <pre><code>class HuggingFaceLM(LanguageModel):\n    \"\"\"\n    LanguageModel implementation using Hugging Face Transformers.\n\n    Args:\n        model: The model name or path of the Hugging Face model.\n        model_kwargs: Keyword arguments for the model instantiation by `from_pretrained()`.\n        tokenizer: The tokenizer name or path of the Hugging Face tokenizer.\n        tokenizer_kwargs: Keyword arguments for the tokenizer instantiation by `from_pretrained().\n        add_special_tokens: Whether to add special tokens to the input.\n            Note that whether BOS or EOS tokens are added depends on the tokenizer.\n        amp_dtype: The dtype for automatic mixed precision.\n        random_seed: Random seed for the model.\n        load_peft: Should be set to True when loading the model from PEFT weights.\n        custom_chat_template: A custom chat template for chatbot models.\n            If specified, this overrides the default chat template of the tokenizer.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        model_kwargs: dict[str, Any] | None = None,\n        tokenizer: str | None = None,\n        tokenizer_kwargs: dict[str, Any] | None = None,\n        add_special_tokens: bool = False,\n        amp_dtype: Literal[\"float16\", \"bfloat16\"] | None = None,\n        random_seed: int = 42,\n        load_peft: bool = False,\n        custom_chat_template: str | None = None,\n        default_gen_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        self._model_name_or_path = model\n        tokenizer = tokenizer if tokenizer else model\n        tokenizer_kwargs = tokenizer_kwargs or {}\n        self.tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(tokenizer, **tokenizer_kwargs)\n        self.custom_chat_template = custom_chat_template\n        self.add_special_tokens = add_special_tokens\n        self.default_gen_kwargs = default_gen_kwargs or {}\n\n        model_kwargs = get_default_model_kwargs(model_kwargs)\n        if not load_peft:\n            self.model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(\n                model,\n                **model_kwargs,\n            )\n        else:\n            from peft import AutoPeftModelForCausalLM\n\n            self.model = AutoPeftModelForCausalLM.from_pretrained(\n                model,\n                **model_kwargs,\n            )\n\n        self.model.eval()\n\n        self.amp_dtype = amp_dtype\n\n        transformers.set_seed(random_seed)\n\n        logger.info(f\"model device: {self.model.device}\")\n        logger.info(f\"model dtype: {self.model.dtype}\")\n        logger.info(f\"amp_dtype: {amp_dtype}\")\n        logger.info(f\"random seed: {random_seed}\")\n\n    def _get_amp_context(self) -&gt; contextlib.AbstractContextManager:\n        if self.amp_dtype is None:\n            return contextlib.nullcontext()\n        if self.amp_dtype == \"float16\":\n            return torch.amp.autocast(\n                device_type=self.model.device.type,\n                dtype=torch.float16,\n            )\n        if self.amp_dtype == \"bfloat16\":\n            return torch.amp.autocast(\n                device_type=self.model.device.type,\n                dtype=torch.bfloat16,\n            )\n\n        msg = f\"Invalid amp_dtype: {self.amp_dtype}\"\n        raise ValueError(msg)\n\n    def _get_stop_token_ids(self, stop_sequences: list[str]) -&gt; list[int]:\n        stop_token_ids: list[int] = []\n        for stop_seq in stop_sequences:\n            # Try to convert string to id using `convert_tokens_to_ids`\n            # We do not use the `encode` method\n            # because in the case of sentencepiece-based tokenizers,\n            # calling the encode method adds a redundant space at the beginning of the string,\n            stop_token_id = self.tokenizer.convert_tokens_to_ids(stop_seq)\n\n            # NeoXTokenizer returns Unk when calling convert_tokens_ids\n            # because each token is stored in a peculiar way\n            # Ex. \"\u300d\" -&gt; \"\u00e3\u0122\u012f\"\n            if stop_token_id == self.tokenizer.unk_token_id:\n                # In such a case, we try to get the ID by calling the encode method.\n                stop_seq_tokens = self.tokenizer.encode(stop_seq, add_special_tokens=False)\n                if stop_seq_tokens:\n                    stop_token_id = stop_seq_tokens[-1]\n            # If the token does not match the specified string itself, we do not include it as a stop token id\n            if self.tokenizer.decode(stop_token_id) != stop_seq:\n                continue\n\n            stop_token_ids.append(stop_token_id)\n        return stop_token_ids\n\n    @torch.inference_mode()\n    def batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        ignore_eos: bool = False,\n        include_stop_str_in_output: bool = False,\n        **kwargs,\n    ) -&gt; list[str]:\n        gen_kwargs = self.default_gen_kwargs.copy()\n        gen_kwargs.update(kwargs)\n        if max_new_tokens is not None:\n            gen_kwargs[\"max_new_tokens\"] = max_new_tokens\n\n        model_inputs = tokenize_text_for_lm_prefix(\n            text_list,\n            self.tokenizer,\n            add_special_tokens=self.add_special_tokens,\n        ).to(self.model.device)\n        input_token_length = model_inputs[\"input_ids\"].shape[1]\n\n        # set the stop sequences\n        stop_sequences = normalize_stop_sequences(\n            stop_sequences_list=[\n                stop_sequences,\n                gen_kwargs.pop(\"stop_strings\", None),  # This is used in the transformers `generate` function\n                gen_kwargs.pop(\"stop_sequences\", None),  # This is a common variable name used in flexeval\n            ],\n            eos_token=self.tokenizer.eos_token,\n            ignore_eos=ignore_eos,\n        )\n        stop_token_ids = self._get_stop_token_ids(stop_sequences)\n        gen_kwargs.update(\n            {\n                \"eos_token_id\": stop_token_ids,\n                \"pad_token_id\": self.tokenizer.pad_token_id,\n            },\n        )\n\n        with self._get_amp_context():\n            lm_outputs = self.model.generate(**model_inputs, **gen_kwargs)\n\n        # `lm_outputs` contains full text including the input text.\n        # We strip the input text and stop sequences from the output text.\n        output_texts: list[str] = []\n        for output_tensor in lm_outputs[:, input_token_length:]:\n            output_tokens = [t for t in output_tensor.tolist() if t != self.tokenizer.pad_token_id]\n            decoded_text = self.tokenizer.decode(output_tokens, skip_special_tokens=False)\n\n            if include_stop_str_in_output:\n                output_texts.append(decoded_text)\n                continue\n\n            for stop_seq in stop_sequences:\n                idx = decoded_text.find(stop_seq)\n                if idx != -1:\n                    decoded_text = decoded_text[:idx]\n            output_texts.append(decoded_text)\n        return output_texts\n\n    def batch_generate_chat_response(\n        self,\n        chat_messages_list: list[list[dict[str, str]]],\n        **kwargs,\n    ) -&gt; list[str]:\n        chat_messages_as_string = [\n            self.tokenizer.apply_chat_template(\n                chat_messages,\n                tokenize=False,\n                add_generation_prompt=True,\n                chat_template=self.custom_chat_template,\n            )\n            for chat_messages in chat_messages_list\n        ]\n        return self.batch_complete_text(chat_messages_as_string, **kwargs)\n\n    @torch.inference_mode()\n    def batch_compute_log_probs(\n        self,\n        text_list: list[str],\n        prefix_list: list[str] | None = None,\n        stride: int | None = None,\n    ) -&gt; list[float]:\n        batch_size = len(text_list)\n\n        # prepare prefix encoding\n        prefix_list = prefix_list if prefix_list else [\"\" for _ in range(batch_size)]\n        # If the prefix is an empty string, replace it with the bos token regardless of the model being trained with it.\n        # This is needed to correctly calculate the log probabilities of the first token.\n        for i in range(batch_size):\n            if prefix_list[i] == \"\":\n                prefix_list[i] = self.tokenizer.bos_token\n\n        prefix_encoding = tokenize_text_for_lm_prefix(\n            prefix_list,\n            self.tokenizer,\n            add_special_tokens=self.add_special_tokens,\n        )\n\n        # prepare continuation encoding\n        # If the last token is a special token, it is treated as a beginning of a new sentence.\n        continuation_encoding = tokenize_text_for_lm_continuation(\n            text_list,\n            self.tokenizer,\n            as_continuation=[\n                prefix_ids[-1] not in self.tokenizer.all_special_ids for prefix_ids in prefix_encoding.input_ids\n            ],\n        )\n\n        input_data_dict: dict[str, torch.Tensor] = {}\n        for key in continuation_encoding:\n            input_data_dict[key] = torch.cat(\n                [prefix_encoding[key].long(), continuation_encoding[key].long()],\n                dim=1,\n            )\n        input_encoding = BatchEncoding(input_data_dict)\n\n        max_length = self.model.config.max_position_embeddings\n        stride = stride or max_length // 2\n        if not (0 &lt; stride &lt; max_length):\n            msg = f\"stride must be in (0, {max_length}), but got {stride}\"\n            raise ValueError(msg)\n        sequence_length = input_encoding.input_ids.size(1)\n\n        with self._get_amp_context():\n            # stores log probabilities of the next token for each input token\n            last_computed_index: int = 0\n            log_prob_of_next = torch.zeros_like(\n                input_encoding.input_ids,\n                dtype=torch.float32,\n            )\n            for chunk_start in range(0, sequence_length, stride):\n                chunk_end = min(chunk_start + max_length, sequence_length)\n\n                # Visualize the input / output processing\n                # input_encoding.input_ids: [ 0  1  2  3  4 ]\n                # chunk_input_ids:          [ 0  1  2  3    ]\n                # chunk_target_ids:         [    1  2  3  4 ]\n\n                input_start = chunk_start\n                input_end = chunk_end - 1\n\n                chunk_input_ids = input_encoding.input_ids[:, input_start:input_end].to(self.model.device)\n                chunk_input_mask = input_encoding.attention_mask[:, input_start:input_end].to(self.model.device)\n                chunk_target_ids = input_encoding.input_ids[:, chunk_start + 1 : chunk_end].to(self.model.device)\n\n                chunkmodel_inputs = self.model.prepare_inputs_for_generation(\n                    chunk_input_ids,\n                    attention_mask=chunk_input_mask,\n                )\n                lm_outputs = self.model.forward(**chunkmodel_inputs)\n\n                chunk_log_probs = F.log_softmax(lm_outputs.logits, dim=-1)\n                # shape of chunk_log_probs: (batch_size, sequence_length, vocab_size)\n                # shape of target_ids: (batch_size, sequence_length)\n                # get the log probs of the target ids\n                chunk_next_log_probs = chunk_log_probs.gather(\n                    dim=-1,\n                    index=chunk_target_ids.unsqueeze(-1),\n                ).squeeze(-1)\n\n                log_prob_of_next[:, last_computed_index:input_end] = chunk_next_log_probs[\n                    :,\n                    last_computed_index - input_start :,\n                ]\n\n                last_computed_index = input_end\n\n                if chunk_end == sequence_length:\n                    break\n\n            log_prob_mask = input_encoding.attention_mask.clone()\n            # replace the last token's log prob with 0\n            for i in range(log_prob_mask.shape[0]):\n                last_non_pad_index = log_prob_mask[i].nonzero(as_tuple=True)[0][-1].item()\n                log_prob_mask[i, last_non_pad_index] = 0\n            # mask out log probs of prefix tokens\n            prefix_length = prefix_encoding.input_ids.shape[1]\n            if prefix_length &gt; 0:\n                log_prob_mask[:, : prefix_length - 1] = 0\n            total_log_probs = (log_prob_of_next * log_prob_mask).sum(dim=-1)\n        return total_log_probs.tolist()\n\n    def batch_compute_chat_log_probs(\n        self, prompt_list: list[list[dict[str, str]]], response_list: list[dict[str, str]]\n    ) -&gt; list[float]:\n        prompt_as_string: list[str] = []\n        response_as_string: list[str] = []\n        for prompt, response in zip(prompt_list, response_list):\n            prompt_as_string_i, response_as_string_i = get_prefix_and_completion_from_chat(\n                prompt,\n                response,\n                self.tokenizer,\n                custom_chat_template=self.custom_chat_template,\n            )\n            prompt_as_string.append(prompt_as_string_i)\n            response_as_string.append(response_as_string_i)\n        return self.batch_compute_log_probs(prompt_as_string, prefix_list=response_as_string)\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(model={self._model_name_or_path!r})\"\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.hf_lm.HuggingFaceLM.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer: PreTrainedTokenizer = from_pretrained(tokenizer, **tokenizer_kwargs)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.hf_lm.HuggingFaceLM.custom_chat_template","title":"custom_chat_template  <code>instance-attribute</code>","text":"<pre><code>custom_chat_template = custom_chat_template\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.hf_lm.HuggingFaceLM.add_special_tokens","title":"add_special_tokens  <code>instance-attribute</code>","text":"<pre><code>add_special_tokens = add_special_tokens\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.hf_lm.HuggingFaceLM.default_gen_kwargs","title":"default_gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>default_gen_kwargs = default_gen_kwargs or {}\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.hf_lm.HuggingFaceLM.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: PreTrainedModel = from_pretrained(model, **model_kwargs)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.hf_lm.HuggingFaceLM.amp_dtype","title":"amp_dtype  <code>instance-attribute</code>","text":"<pre><code>amp_dtype = amp_dtype\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.hf_lm.HuggingFaceLM.__init__","title":"__init__","text":"<pre><code>__init__(model: str, model_kwargs: dict[str, Any] | None = None, tokenizer: str | None = None, tokenizer_kwargs: dict[str, Any] | None = None, add_special_tokens: bool = False, amp_dtype: Literal['float16', 'bfloat16'] | None = None, random_seed: int = 42, load_peft: bool = False, custom_chat_template: str | None = None, default_gen_kwargs: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/hf_lm.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    model_kwargs: dict[str, Any] | None = None,\n    tokenizer: str | None = None,\n    tokenizer_kwargs: dict[str, Any] | None = None,\n    add_special_tokens: bool = False,\n    amp_dtype: Literal[\"float16\", \"bfloat16\"] | None = None,\n    random_seed: int = 42,\n    load_peft: bool = False,\n    custom_chat_template: str | None = None,\n    default_gen_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    self._model_name_or_path = model\n    tokenizer = tokenizer if tokenizer else model\n    tokenizer_kwargs = tokenizer_kwargs or {}\n    self.tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(tokenizer, **tokenizer_kwargs)\n    self.custom_chat_template = custom_chat_template\n    self.add_special_tokens = add_special_tokens\n    self.default_gen_kwargs = default_gen_kwargs or {}\n\n    model_kwargs = get_default_model_kwargs(model_kwargs)\n    if not load_peft:\n        self.model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(\n            model,\n            **model_kwargs,\n        )\n    else:\n        from peft import AutoPeftModelForCausalLM\n\n        self.model = AutoPeftModelForCausalLM.from_pretrained(\n            model,\n            **model_kwargs,\n        )\n\n    self.model.eval()\n\n    self.amp_dtype = amp_dtype\n\n    transformers.set_seed(random_seed)\n\n    logger.info(f\"model device: {self.model.device}\")\n    logger.info(f\"model dtype: {self.model.dtype}\")\n    logger.info(f\"amp_dtype: {amp_dtype}\")\n    logger.info(f\"random seed: {random_seed}\")\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.hf_lm.HuggingFaceLM.batch_complete_text","title":"batch_complete_text","text":"<pre><code>batch_complete_text(text_list: list[str], stop_sequences: str | list[str] | None = None, max_new_tokens: int | None = None, ignore_eos: bool = False, include_stop_str_in_output: bool = False, **kwargs) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/language_model/hf_lm.py</code> <pre><code>@torch.inference_mode()\ndef batch_complete_text(\n    self,\n    text_list: list[str],\n    stop_sequences: str | list[str] | None = None,\n    max_new_tokens: int | None = None,\n    ignore_eos: bool = False,\n    include_stop_str_in_output: bool = False,\n    **kwargs,\n) -&gt; list[str]:\n    gen_kwargs = self.default_gen_kwargs.copy()\n    gen_kwargs.update(kwargs)\n    if max_new_tokens is not None:\n        gen_kwargs[\"max_new_tokens\"] = max_new_tokens\n\n    model_inputs = tokenize_text_for_lm_prefix(\n        text_list,\n        self.tokenizer,\n        add_special_tokens=self.add_special_tokens,\n    ).to(self.model.device)\n    input_token_length = model_inputs[\"input_ids\"].shape[1]\n\n    # set the stop sequences\n    stop_sequences = normalize_stop_sequences(\n        stop_sequences_list=[\n            stop_sequences,\n            gen_kwargs.pop(\"stop_strings\", None),  # This is used in the transformers `generate` function\n            gen_kwargs.pop(\"stop_sequences\", None),  # This is a common variable name used in flexeval\n        ],\n        eos_token=self.tokenizer.eos_token,\n        ignore_eos=ignore_eos,\n    )\n    stop_token_ids = self._get_stop_token_ids(stop_sequences)\n    gen_kwargs.update(\n        {\n            \"eos_token_id\": stop_token_ids,\n            \"pad_token_id\": self.tokenizer.pad_token_id,\n        },\n    )\n\n    with self._get_amp_context():\n        lm_outputs = self.model.generate(**model_inputs, **gen_kwargs)\n\n    # `lm_outputs` contains full text including the input text.\n    # We strip the input text and stop sequences from the output text.\n    output_texts: list[str] = []\n    for output_tensor in lm_outputs[:, input_token_length:]:\n        output_tokens = [t for t in output_tensor.tolist() if t != self.tokenizer.pad_token_id]\n        decoded_text = self.tokenizer.decode(output_tokens, skip_special_tokens=False)\n\n        if include_stop_str_in_output:\n            output_texts.append(decoded_text)\n            continue\n\n        for stop_seq in stop_sequences:\n            idx = decoded_text.find(stop_seq)\n            if idx != -1:\n                decoded_text = decoded_text[:idx]\n        output_texts.append(decoded_text)\n    return output_texts\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.hf_lm.HuggingFaceLM.batch_generate_chat_response","title":"batch_generate_chat_response","text":"<pre><code>batch_generate_chat_response(chat_messages_list: list[list[dict[str, str]]], **kwargs) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/language_model/hf_lm.py</code> <pre><code>def batch_generate_chat_response(\n    self,\n    chat_messages_list: list[list[dict[str, str]]],\n    **kwargs,\n) -&gt; list[str]:\n    chat_messages_as_string = [\n        self.tokenizer.apply_chat_template(\n            chat_messages,\n            tokenize=False,\n            add_generation_prompt=True,\n            chat_template=self.custom_chat_template,\n        )\n        for chat_messages in chat_messages_list\n    ]\n    return self.batch_complete_text(chat_messages_as_string, **kwargs)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.hf_lm.HuggingFaceLM.batch_compute_log_probs","title":"batch_compute_log_probs","text":"<pre><code>batch_compute_log_probs(text_list: list[str], prefix_list: list[str] | None = None, stride: int | None = None) -&gt; list[float]\n</code></pre> Source code in <code>flexeval/core/language_model/hf_lm.py</code> <pre><code>@torch.inference_mode()\ndef batch_compute_log_probs(\n    self,\n    text_list: list[str],\n    prefix_list: list[str] | None = None,\n    stride: int | None = None,\n) -&gt; list[float]:\n    batch_size = len(text_list)\n\n    # prepare prefix encoding\n    prefix_list = prefix_list if prefix_list else [\"\" for _ in range(batch_size)]\n    # If the prefix is an empty string, replace it with the bos token regardless of the model being trained with it.\n    # This is needed to correctly calculate the log probabilities of the first token.\n    for i in range(batch_size):\n        if prefix_list[i] == \"\":\n            prefix_list[i] = self.tokenizer.bos_token\n\n    prefix_encoding = tokenize_text_for_lm_prefix(\n        prefix_list,\n        self.tokenizer,\n        add_special_tokens=self.add_special_tokens,\n    )\n\n    # prepare continuation encoding\n    # If the last token is a special token, it is treated as a beginning of a new sentence.\n    continuation_encoding = tokenize_text_for_lm_continuation(\n        text_list,\n        self.tokenizer,\n        as_continuation=[\n            prefix_ids[-1] not in self.tokenizer.all_special_ids for prefix_ids in prefix_encoding.input_ids\n        ],\n    )\n\n    input_data_dict: dict[str, torch.Tensor] = {}\n    for key in continuation_encoding:\n        input_data_dict[key] = torch.cat(\n            [prefix_encoding[key].long(), continuation_encoding[key].long()],\n            dim=1,\n        )\n    input_encoding = BatchEncoding(input_data_dict)\n\n    max_length = self.model.config.max_position_embeddings\n    stride = stride or max_length // 2\n    if not (0 &lt; stride &lt; max_length):\n        msg = f\"stride must be in (0, {max_length}), but got {stride}\"\n        raise ValueError(msg)\n    sequence_length = input_encoding.input_ids.size(1)\n\n    with self._get_amp_context():\n        # stores log probabilities of the next token for each input token\n        last_computed_index: int = 0\n        log_prob_of_next = torch.zeros_like(\n            input_encoding.input_ids,\n            dtype=torch.float32,\n        )\n        for chunk_start in range(0, sequence_length, stride):\n            chunk_end = min(chunk_start + max_length, sequence_length)\n\n            # Visualize the input / output processing\n            # input_encoding.input_ids: [ 0  1  2  3  4 ]\n            # chunk_input_ids:          [ 0  1  2  3    ]\n            # chunk_target_ids:         [    1  2  3  4 ]\n\n            input_start = chunk_start\n            input_end = chunk_end - 1\n\n            chunk_input_ids = input_encoding.input_ids[:, input_start:input_end].to(self.model.device)\n            chunk_input_mask = input_encoding.attention_mask[:, input_start:input_end].to(self.model.device)\n            chunk_target_ids = input_encoding.input_ids[:, chunk_start + 1 : chunk_end].to(self.model.device)\n\n            chunkmodel_inputs = self.model.prepare_inputs_for_generation(\n                chunk_input_ids,\n                attention_mask=chunk_input_mask,\n            )\n            lm_outputs = self.model.forward(**chunkmodel_inputs)\n\n            chunk_log_probs = F.log_softmax(lm_outputs.logits, dim=-1)\n            # shape of chunk_log_probs: (batch_size, sequence_length, vocab_size)\n            # shape of target_ids: (batch_size, sequence_length)\n            # get the log probs of the target ids\n            chunk_next_log_probs = chunk_log_probs.gather(\n                dim=-1,\n                index=chunk_target_ids.unsqueeze(-1),\n            ).squeeze(-1)\n\n            log_prob_of_next[:, last_computed_index:input_end] = chunk_next_log_probs[\n                :,\n                last_computed_index - input_start :,\n            ]\n\n            last_computed_index = input_end\n\n            if chunk_end == sequence_length:\n                break\n\n        log_prob_mask = input_encoding.attention_mask.clone()\n        # replace the last token's log prob with 0\n        for i in range(log_prob_mask.shape[0]):\n            last_non_pad_index = log_prob_mask[i].nonzero(as_tuple=True)[0][-1].item()\n            log_prob_mask[i, last_non_pad_index] = 0\n        # mask out log probs of prefix tokens\n        prefix_length = prefix_encoding.input_ids.shape[1]\n        if prefix_length &gt; 0:\n            log_prob_mask[:, : prefix_length - 1] = 0\n        total_log_probs = (log_prob_of_next * log_prob_mask).sum(dim=-1)\n    return total_log_probs.tolist()\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.hf_lm.HuggingFaceLM.batch_compute_chat_log_probs","title":"batch_compute_chat_log_probs","text":"<pre><code>batch_compute_chat_log_probs(prompt_list: list[list[dict[str, str]]], response_list: list[dict[str, str]]) -&gt; list[float]\n</code></pre> Source code in <code>flexeval/core/language_model/hf_lm.py</code> <pre><code>def batch_compute_chat_log_probs(\n    self, prompt_list: list[list[dict[str, str]]], response_list: list[dict[str, str]]\n) -&gt; list[float]:\n    prompt_as_string: list[str] = []\n    response_as_string: list[str] = []\n    for prompt, response in zip(prompt_list, response_list):\n        prompt_as_string_i, response_as_string_i = get_prefix_and_completion_from_chat(\n            prompt,\n            response,\n            self.tokenizer,\n            custom_chat_template=self.custom_chat_template,\n        )\n        prompt_as_string.append(prompt_as_string_i)\n        response_as_string.append(response_as_string_i)\n    return self.batch_compute_log_probs(prompt_as_string, prefix_list=response_as_string)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.hf_lm.HuggingFaceLM.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/language_model/hf_lm.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(model={self._model_name_or_path!r})\"\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_api.OpenAIChatAPI","title":"OpenAIChatAPI","text":"<p>LanguageModel implementation using OpenAI's ChatGPT API.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>, default:                   <code>'gpt-3.5-turbo'</code> )           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_headers</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of headers to use when making requests to the OpenAI API.</p> </li> <li> <code>default_gen_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default generation kwargs to use when calling the API.</p> </li> </ul> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>class OpenAIChatAPI(LanguageModel):\n    \"\"\"\n    LanguageModel implementation using OpenAI's ChatGPT API.\n\n    Args:\n        model: The name of the model to use.\n        api_headers: A dictionary of headers to use when making requests to the OpenAI API.\n        default_gen_kwargs: Default generation kwargs to use when calling the API.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"gpt-3.5-turbo\",\n        api_headers: dict[str, str] | None = None,\n        default_gen_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        self.model = model\n        if api_headers is None:\n            api_headers = {}\n        self._client = AsyncOpenAI(**api_headers)\n        self.default_gen_kwargs = default_gen_kwargs or {}\n        # convert the flexeval-specific argument name to the OpenAI-specific name\n        if \"max_new_tokens\" in self.default_gen_kwargs:\n            self.default_gen_kwargs[\"max_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n\n    async def _async_batch_run_chatgpt(\n        self,\n        messages_list: list[list[dict[str, str]]],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        \"\"\"Send multiple chat requests to the OpenAI in parallel.\"\"\"\n\n        gen_kwargs = self.default_gen_kwargs.copy()\n        gen_kwargs.update(kwargs)\n        if max_new_tokens is not None:\n            gen_kwargs[\"max_tokens\"] = max_new_tokens\n\n        stop_sequences = normalize_stop_sequences(\n            stop_sequences_list=[\n                stop_sequences,\n                gen_kwargs.pop(\"stop\", None),  # This is used in the OpenAI API\n                gen_kwargs.pop(\"stop_sequences\", None),  # This is a common variable name used in flexeval\n            ],\n        )\n\n        tasks = [\n            _retry_on_error(\n                # Define an anonymous function with a lambda expression and pass it,\n                # and call it inside the _retry_on_error function\n                openai_call=lambda x=ms: self._client.chat.completions.create(\n                    model=self.model,\n                    messages=x,\n                    stop=stop_sequences,\n                    **gen_kwargs,\n                ),\n            )\n            for ms in messages_list\n        ]\n        return await asyncio.gather(*tasks)\n\n    def batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        messages_list = [[{\"role\": \"user\", \"content\": text}] for text in text_list]\n        api_responses = asyncio.run(\n            self._async_batch_run_chatgpt(\n                messages_list,\n                stop_sequences=stop_sequences,\n                max_new_tokens=max_new_tokens,\n                **kwargs,\n            ),\n        )\n        completions = [res.choices[0].message.content for res in api_responses]\n        if all(completion == \"\" for completion in completions):\n            logger.warning(\"All generated texts are empty strings. Something may be wrong.\")\n        return completions\n\n    def batch_generate_chat_response(\n        self,\n        chat_messages_list: list[list[dict[str, str]]],\n        **kwargs,\n    ) -&gt; list[str]:\n        api_responses = asyncio.run(\n            self._async_batch_run_chatgpt(chat_messages_list, **kwargs),\n        )\n        completions = [res.choices[0].message.content for res in api_responses]\n        if all(completion == \"\" for completion in completions):\n            logger.warning(\"All generated texts are empty string. Something may go wrong.\")\n        return completions\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_api.OpenAIChatAPI.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_api.OpenAIChatAPI.default_gen_kwargs","title":"default_gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>default_gen_kwargs = default_gen_kwargs or {}\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_api.OpenAIChatAPI.__init__","title":"__init__","text":"<pre><code>__init__(model: str = 'gpt-3.5-turbo', api_headers: dict[str, str] | None = None, default_gen_kwargs: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"gpt-3.5-turbo\",\n    api_headers: dict[str, str] | None = None,\n    default_gen_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    self.model = model\n    if api_headers is None:\n        api_headers = {}\n    self._client = AsyncOpenAI(**api_headers)\n    self.default_gen_kwargs = default_gen_kwargs or {}\n    # convert the flexeval-specific argument name to the OpenAI-specific name\n    if \"max_new_tokens\" in self.default_gen_kwargs:\n        self.default_gen_kwargs[\"max_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_api.OpenAIChatAPI.batch_complete_text","title":"batch_complete_text","text":"<pre><code>batch_complete_text(text_list: list[str], stop_sequences: str | list[str] | None = None, max_new_tokens: int | None = None, **kwargs) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>def batch_complete_text(\n    self,\n    text_list: list[str],\n    stop_sequences: str | list[str] | None = None,\n    max_new_tokens: int | None = None,\n    **kwargs,\n) -&gt; list[str]:\n    messages_list = [[{\"role\": \"user\", \"content\": text}] for text in text_list]\n    api_responses = asyncio.run(\n        self._async_batch_run_chatgpt(\n            messages_list,\n            stop_sequences=stop_sequences,\n            max_new_tokens=max_new_tokens,\n            **kwargs,\n        ),\n    )\n    completions = [res.choices[0].message.content for res in api_responses]\n    if all(completion == \"\" for completion in completions):\n        logger.warning(\"All generated texts are empty strings. Something may be wrong.\")\n    return completions\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_api.OpenAIChatAPI.batch_generate_chat_response","title":"batch_generate_chat_response","text":"<pre><code>batch_generate_chat_response(chat_messages_list: list[list[dict[str, str]]], **kwargs) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>def batch_generate_chat_response(\n    self,\n    chat_messages_list: list[list[dict[str, str]]],\n    **kwargs,\n) -&gt; list[str]:\n    api_responses = asyncio.run(\n        self._async_batch_run_chatgpt(chat_messages_list, **kwargs),\n    )\n    completions = [res.choices[0].message.content for res in api_responses]\n    if all(completion == \"\" for completion in completions):\n        logger.warning(\"All generated texts are empty string. Something may go wrong.\")\n    return completions\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_api.OpenAIChatAPI.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_api.OpenAICompletionAPI","title":"OpenAICompletionAPI","text":"<p>LanguageModel implementation using OpenAI's Completion API.</p> <p>Note that Completion API is a legacy API, with only a few models (such as gpt-3.5-turbo-instruct) supported by OpenAI. This LanguageModel implementation is primarily intended for use with on-premise VLLM servers, as described in the documentation: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>, default:                   <code>'gpt-3.5-turbo-instruct'</code> )           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_headers</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of headers to use when making requests to the OpenAI API.</p> </li> <li> <code>default_gen_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default generation kwargs to use when calling the API.</p> </li> </ul> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>class OpenAICompletionAPI(LanguageModel):\n    \"\"\"LanguageModel implementation using OpenAI's Completion API.\n\n    Note that Completion API is a legacy API, with only a few models (such as gpt-3.5-turbo-instruct)\n    supported by OpenAI. This LanguageModel implementation is primarily intended for use with on-premise\n    VLLM servers, as described in the documentation: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n\n    Args:\n        model: The name of the model to use.\n        api_headers: A dictionary of headers to use when making requests to the OpenAI API.\n        default_gen_kwargs: Default generation kwargs to use when calling the API.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"gpt-3.5-turbo-instruct\",\n        api_headers: dict[str, str] | None = None,\n        default_gen_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        self.model = model\n        if api_headers is None:\n            api_headers = {}\n        self._client = AsyncOpenAI(**api_headers)\n        self.default_gen_kwargs = default_gen_kwargs or {}\n        # convert the flexeval-specific argument name to the OpenAI-specific name\n        if \"max_new_tokens\" in self.default_gen_kwargs:\n            self.default_gen_kwargs[\"max_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n\n    async def _async_batch_run_completion(\n        self,\n        prompt_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        \"\"\"Send multiple completion requests to the OpenAI in parallel.\"\"\"\n\n        gen_kwargs = self.default_gen_kwargs.copy()\n        gen_kwargs.update(kwargs)\n        if max_new_tokens is not None:\n            gen_kwargs[\"max_tokens\"] = max_new_tokens\n\n        stop_sequences = normalize_stop_sequences(\n            stop_sequences_list=[\n                stop_sequences,\n                gen_kwargs.pop(\"stop\", None),  # This is used in the OpenAI API\n                gen_kwargs.pop(\"stop_sequences\", None),  # This is a common variable name used in flexeval\n            ],\n        )\n\n        tasks = [\n            _retry_on_error(\n                # Define an anonymous function with a lambda expression and pass it,\n                # and call it inside the _retry_on_error function\n                openai_call=lambda x=ms: self._client.completions.create(\n                    model=self.model,\n                    prompt=x,\n                    stop=stop_sequences,\n                    **gen_kwargs,\n                ),\n            )\n            for ms in prompt_list\n        ]\n        return await asyncio.gather(*tasks)\n\n    def batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        api_responses = asyncio.run(\n            self._async_batch_run_completion(\n                text_list,\n                stop_sequences=stop_sequences,\n                max_new_tokens=max_new_tokens,\n                **kwargs,\n            ),\n        )\n\n        return [res.choices[0].text for res in api_responses]\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_api.OpenAICompletionAPI.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_api.OpenAICompletionAPI.default_gen_kwargs","title":"default_gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>default_gen_kwargs = default_gen_kwargs or {}\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_api.OpenAICompletionAPI.__init__","title":"__init__","text":"<pre><code>__init__(model: str = 'gpt-3.5-turbo-instruct', api_headers: dict[str, str] | None = None, default_gen_kwargs: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"gpt-3.5-turbo-instruct\",\n    api_headers: dict[str, str] | None = None,\n    default_gen_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    self.model = model\n    if api_headers is None:\n        api_headers = {}\n    self._client = AsyncOpenAI(**api_headers)\n    self.default_gen_kwargs = default_gen_kwargs or {}\n    # convert the flexeval-specific argument name to the OpenAI-specific name\n    if \"max_new_tokens\" in self.default_gen_kwargs:\n        self.default_gen_kwargs[\"max_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_api.OpenAICompletionAPI.batch_complete_text","title":"batch_complete_text","text":"<pre><code>batch_complete_text(text_list: list[str], stop_sequences: str | list[str] | None = None, max_new_tokens: int | None = None, **kwargs) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>def batch_complete_text(\n    self,\n    text_list: list[str],\n    stop_sequences: str | list[str] | None = None,\n    max_new_tokens: int | None = None,\n    **kwargs,\n) -&gt; list[str]:\n    api_responses = asyncio.run(\n        self._async_batch_run_completion(\n            text_list,\n            stop_sequences=stop_sequences,\n            max_new_tokens=max_new_tokens,\n            **kwargs,\n        ),\n    )\n\n    return [res.choices[0].text for res in api_responses]\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_api.OpenAICompletionAPI.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI","title":"OpenAIChatBatchAPI","text":"<p>LanguageModel implementation using OpenAI's ChatGPT API for Batch API. NOTE: Batch size should be more than or equal to the size of the given dataset for efficient generation.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_headers</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of headers to use when making requests to the OpenAI API.</p> </li> <li> <code>polling_interval_seconds</code>               (<code>int</code>, default:                   <code>60</code> )           \u2013            <p>The interval in seconds to poll the batch status.</p> </li> </ul> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>class OpenAIChatBatchAPI(LanguageModel):\n    \"\"\"LanguageModel implementation using OpenAI's ChatGPT API for Batch API.\n    NOTE: Batch size should be more than or equal to the size of the given dataset for efficient generation.\n\n    Args:\n        model: The name of the model to use.\n        api_headers: A dictionary of headers to use when making requests to the OpenAI API.\n        polling_interval_seconds: The interval in seconds to poll the batch status.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        api_headers: dict[str, str] | None = None,\n        polling_interval_seconds: int = 60,\n    ) -&gt; None:\n        self.model = model\n        if api_headers is None:\n            api_headers = {}\n        self._client = AsyncOpenAI(**api_headers)\n        self.temp_jsonl_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".jsonl\")\n\n        self.polling_interval_seconds = polling_interval_seconds\n\n    def create_batch_file(self, custom_id_2_message: dict[str, list[dict[str, str]]], **kwargs) -&gt; None:\n        with open(self.temp_jsonl_file.name, mode=\"w\") as f:\n            for custom_id, message in custom_id_2_message.items():\n                f.write(\n                    json.dumps(create_request_details(self.model, custom_id, message, **kwargs), ensure_ascii=False)\n                    + \"\\n\",\n                )\n\n    async def _post_batch_requests(\n        self,\n        custom_id_2_message: dict[str, list[dict[str, str]]],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"Send batch chat requests to the OpenAI.\"\"\"\n        if stop_sequences is not None:\n            if \"stop\" in kwargs:\n                msg = (\n                    \"You specified both `stop_sequences` and `stop` in generation kwargs. \"\n                    \"However, `stop_sequences` will be normalized into `stop`. \"\n                    \"Please specify only one of them.\"\n                )\n                raise ValueError(msg)\n            kwargs[\"stop\"] = stop_sequences\n\n        if max_new_tokens is not None:\n            if \"max_tokens\" in kwargs:\n                msg = (\n                    \"You specified both `max_new_tokens` and `max_tokens` in generation kwargs. \"\n                    \"However, `max_new_tokens` will be normalized into `max_tokens`. \"\n                    \"Please specify only one of them.\"\n                )\n                raise ValueError(msg)\n            kwargs[\"max_tokens\"] = max_new_tokens\n\n        self.create_batch_file(custom_id_2_message, **kwargs)\n\n        # Update batch file\n        with open(self.temp_jsonl_file.name, \"rb\") as batch_file:  # noqa: ASYNC101\n            batch_input_file = await self._client.files.create(file=batch_file, purpose=\"batch\")\n\n        # Run Job\n        # Batch Object: https://platform.openai.com/docs/api-reference/batch/object\n        batch_object = await self._client.batches.create(\n            input_file_id=batch_input_file.id,\n            endpoint=\"/v1/chat/completions\",\n            completion_window=\"24h\",\n            metadata={\"description\": \"flexeval job\"},\n        )\n        logger.info(f\"Input File ID: {batch_input_file.id}, Batch ID: {batch_object.id}\")\n        return batch_object.id\n\n    async def poll_batch_status_until_completion(\n        self,\n        batch_id: str,\n        polling_interval_seconds: int,\n    ) -&gt; tuple[Status, Batch]:\n        status = Status.validating\n        while status not in (Status.completed, Status.failed, Status.canceled):\n            await asyncio.sleep(polling_interval_seconds)\n            batch_response = await self._client.batches.retrieve(batch_id)\n            status = Status(batch_response.status)\n            logger.info(f\"Current status: {status.value}\")\n        return status, batch_response\n\n    def _retrieve_file_content(self, file_id: str) -&gt; list[dict[any, any]]:\n        file_response = asyncio.run(self._client.files.content(file_id))\n        return [json.loads(line) for line in file_response.text.strip().split(\"\\n\")]\n\n    def _execute_batch_requests(\n        self,\n        messages_list: list[list[dict[str, str]]],\n        **kwargs,\n    ) -&gt; list[str]:\n        custom_id_2_message: dict[str, list[dict[str, str]]] = {\n            str(uuid.uuid4()): messages for messages in messages_list\n        }\n        # The response will be an empty string if the API produces an error.\n        custom_id_2_response: dict[str, str] = {custom_id: \"\" for custom_id in custom_id_2_message}\n        exec_cnt = 1\n\n        while len(custom_id_2_message) &gt; 0:\n            if exec_cnt &gt; MAX_NUM_TRIALS:\n                break\n            logger.info(f\"Trial {exec_cnt}\")\n            exec_cnt += 1\n            batch_id = asyncio.run(self._post_batch_requests(custom_id_2_message, **kwargs))\n\n            status, batch_response = asyncio.run(\n                self.poll_batch_status_until_completion(batch_id, self.polling_interval_seconds),\n            )\n            if status is not Status.completed:\n                error_message = f\"Failed: {batch_response}\"\n                raise ValueError(error_message)\n\n            # Check error_file_id exists and if exists, log error details.\n            error_file_id = batch_response.error_file_id\n            # If any request fails, error_file_id is set.\n            if error_file_id is not None:\n                logger.warning(\"Request on some messages failed following reason.\")\n                data: list[dict[str, Any]] = self._retrieve_file_content(error_file_id)\n                # [Error](https://github.com/openai/openai-openapi/blob/master/openapi.yaml#L8857])\n                # instance is embedded in response.\n                for data_i in data:\n                    error = data_i[\"response\"]\n                    logger.warning(f\"Failed: {error}\")\n\n            output_file_id = batch_response.output_file_id\n            # If completion on all input fails, output_file_id is None.\n            if output_file_id is None:\n                logger.warning(\"All request failed. Continue...\")\n                continue\n\n            data: list[dict[str, Any]] = self._retrieve_file_content(output_file_id)\n            for data_i in data:\n                if data_i[\"error\"] is not None:\n                    continue\n\n                custom_id = data_i[\"custom_id\"]\n                custom_id_2_message.pop(custom_id)\n                custom_id_2_response[custom_id] = data_i[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n\n        # The remaining elements are all those that failed to complete request.\n        if custom_id_2_message:\n            logger.warning(\"The following messages failed to complete request.\")\n            logger.warning(pformat(list(custom_id_2_message.values())))\n\n        return list(custom_id_2_response.values())\n\n    def batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        messages_list = [[{\"role\": \"user\", \"content\": text}] for text in text_list]\n        return self._execute_batch_requests(\n            messages_list,\n            stop_sequences=stop_sequences,\n            max_new_tokens=max_new_tokens,\n            **kwargs,\n        )\n\n    def batch_generate_chat_response(\n        self,\n        chat_messages_list: list[list[dict[str, str]]],\n        **kwargs,\n    ) -&gt; list[str]:\n        return self._execute_batch_requests(chat_messages_list, **kwargs)\n\n    def close(self) -&gt; None:\n        # in case that the program fails before the file is initialized in __init__\n        if not hasattr(self, \"temp_jsonl_file\"):\n            return\n\n        try:\n            self.temp_jsonl_file.close()\n            os.unlink(self.temp_jsonl_file.name)  # noqa: PTH108\n            logger.info(f\"Temporary file deleted: {self.temp_jsonl_file.name}\")\n        except OSError as e:\n            logger.error(f\"Error: {e.filename} - {e.strerror}.\")\n\n    def __del__(self) -&gt; None:\n        self.close()\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.temp_jsonl_file","title":"temp_jsonl_file  <code>instance-attribute</code>","text":"<pre><code>temp_jsonl_file = NamedTemporaryFile(delete=False, suffix='.jsonl')\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.polling_interval_seconds","title":"polling_interval_seconds  <code>instance-attribute</code>","text":"<pre><code>polling_interval_seconds = polling_interval_seconds\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.__init__","title":"__init__","text":"<pre><code>__init__(model: str, api_headers: dict[str, str] | None = None, polling_interval_seconds: int = 60) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    api_headers: dict[str, str] | None = None,\n    polling_interval_seconds: int = 60,\n) -&gt; None:\n    self.model = model\n    if api_headers is None:\n        api_headers = {}\n    self._client = AsyncOpenAI(**api_headers)\n    self.temp_jsonl_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".jsonl\")\n\n    self.polling_interval_seconds = polling_interval_seconds\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.create_batch_file","title":"create_batch_file","text":"<pre><code>create_batch_file(custom_id_2_message: dict[str, list[dict[str, str]]], **kwargs) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>def create_batch_file(self, custom_id_2_message: dict[str, list[dict[str, str]]], **kwargs) -&gt; None:\n    with open(self.temp_jsonl_file.name, mode=\"w\") as f:\n        for custom_id, message in custom_id_2_message.items():\n            f.write(\n                json.dumps(create_request_details(self.model, custom_id, message, **kwargs), ensure_ascii=False)\n                + \"\\n\",\n            )\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.poll_batch_status_until_completion","title":"poll_batch_status_until_completion  <code>async</code>","text":"<pre><code>poll_batch_status_until_completion(batch_id: str, polling_interval_seconds: int) -&gt; tuple[Status, Batch]\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>async def poll_batch_status_until_completion(\n    self,\n    batch_id: str,\n    polling_interval_seconds: int,\n) -&gt; tuple[Status, Batch]:\n    status = Status.validating\n    while status not in (Status.completed, Status.failed, Status.canceled):\n        await asyncio.sleep(polling_interval_seconds)\n        batch_response = await self._client.batches.retrieve(batch_id)\n        status = Status(batch_response.status)\n        logger.info(f\"Current status: {status.value}\")\n    return status, batch_response\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.batch_complete_text","title":"batch_complete_text","text":"<pre><code>batch_complete_text(text_list: list[str], stop_sequences: str | list[str] | None = None, max_new_tokens: int | None = None, **kwargs) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>def batch_complete_text(\n    self,\n    text_list: list[str],\n    stop_sequences: str | list[str] | None = None,\n    max_new_tokens: int | None = None,\n    **kwargs,\n) -&gt; list[str]:\n    messages_list = [[{\"role\": \"user\", \"content\": text}] for text in text_list]\n    return self._execute_batch_requests(\n        messages_list,\n        stop_sequences=stop_sequences,\n        max_new_tokens=max_new_tokens,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.batch_generate_chat_response","title":"batch_generate_chat_response","text":"<pre><code>batch_generate_chat_response(chat_messages_list: list[list[dict[str, str]]], **kwargs) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>def batch_generate_chat_response(\n    self,\n    chat_messages_list: list[list[dict[str, str]]],\n    **kwargs,\n) -&gt; list[str]:\n    return self._execute_batch_requests(chat_messages_list, **kwargs)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>def close(self) -&gt; None:\n    # in case that the program fails before the file is initialized in __init__\n    if not hasattr(self, \"temp_jsonl_file\"):\n        return\n\n    try:\n        self.temp_jsonl_file.close()\n        os.unlink(self.temp_jsonl_file.name)  # noqa: PTH108\n        logger.info(f\"Temporary file deleted: {self.temp_jsonl_file.name}\")\n    except OSError as e:\n        logger.error(f\"Error: {e.filename} - {e.strerror}.\")\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.__del__","title":"__del__","text":"<pre><code>__del__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>def __del__(self) -&gt; None:\n    self.close()\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.vllm_model.VLLM","title":"VLLM","text":"<p>LanguageModel implementation using VLLM.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The name of the model to use.</p> </li> <li> <code>model_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Additional keyword arguments to pass to the model.</p> </li> <li> <code>tokenizer</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the tokenizer to use. Defaults to the model_name.</p> </li> <li> <code>tokenizer_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments for the tokenizer instantiation by `from_pretrained().</p> </li> <li> <code>add_special_tokens</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add special tokens to the input. Note that whether BOS or EOS tokens are added depends on the tokenizer.</p> </li> <li> <code>custom_chat_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A custom chat template for chatbot models. If specified, this overrides the default chat template of the tokenizer.</p> </li> <li> <code>default_gen_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default generation kwargs to use when calling the model.</p> </li> </ul> Source code in <code>flexeval/core/language_model/vllm_model.py</code> <pre><code>class VLLM(LanguageModel):\n    \"\"\"LanguageModel implementation using VLLM.\n\n    Args:\n        model: The name of the model to use.\n        model_kwargs: Additional keyword arguments to pass to the model.\n        tokenizer: The name of the tokenizer to use. Defaults to the model_name.\n        tokenizer_kwargs: Keyword arguments for the tokenizer instantiation by `from_pretrained().\n        add_special_tokens: Whether to add special tokens to the input.\n            Note that whether BOS or EOS tokens are added depends on the tokenizer.\n        custom_chat_template: A custom chat template for chatbot models.\n            If specified, this overrides the default chat template of the tokenizer.\n        default_gen_kwargs: Default generation kwargs to use when calling the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        model_kwargs: dict[str, Any] | None = None,\n        tokenizer: str | None = None,\n        tokenizer_kwargs: dict[str, Any] | None = None,\n        add_special_tokens: bool = False,\n        custom_chat_template: str | None = None,\n        default_gen_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        self.model_name = model\n        tokenizer = tokenizer if tokenizer else model\n        tokenizer_kwargs = tokenizer_kwargs or {}\n        self.tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(tokenizer, **tokenizer_kwargs)\n        self.custom_chat_template = custom_chat_template\n        self.add_special_tokens = add_special_tokens\n        # use greedy decoding by default to make it consistent with `HuggingFaceLM`\n        self.default_gen_kwargs = default_gen_kwargs or {\"temperature\": 0.0}\n        # convert the flexeval-specific argument name to the vllm-specific name\n        if \"max_new_tokens\" in self.default_gen_kwargs:\n            self.default_gen_kwargs[\"max_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n\n        # import from vllm here because it is an extra dependency\n        from vllm import LLM\n\n        model_kwargs = model_kwargs or {}\n        # automatically set tensor_parallel_size to the number of GPUs\n        if \"tensor_parallel_size\" not in model_kwargs:\n            model_kwargs[\"tensor_parallel_size\"] = torch.cuda.device_count()\n        if \"enable_chunked_prefill\" not in model_kwargs:\n            model_kwargs[\"enable_chunked_prefill\"] = True\n            model_kwargs[\"disable_sliding_window\"] = True\n        self.llm = LLM(model, **model_kwargs)\n\n    def batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        gen_kwargs = self.default_gen_kwargs.copy()\n        gen_kwargs.update(kwargs)\n        if max_new_tokens is not None:\n            gen_kwargs[\"max_tokens\"] = max_new_tokens\n\n        stop_sequences = normalize_stop_sequences(\n            stop_sequences_list=[\n                stop_sequences,\n                gen_kwargs.pop(\"stop\", None),  # This is used in the vllm `SamplingParams`\n                gen_kwargs.pop(\"stop_sequences\", None),  # This is a common variable name used in flexeval\n            ],\n            eos_token=self.tokenizer.eos_token,\n            ignore_eos=gen_kwargs.get(\"ignore_eos\", False),\n        )\n\n        model_inputs = self.tokenizer(\n            text_list,\n            add_special_tokens=self.add_special_tokens,\n            return_token_type_ids=False,\n        )\n\n        from vllm import SamplingParams\n\n        vllm_outputs = self.llm.generate(\n            prompt_token_ids=model_inputs.input_ids,\n            sampling_params=SamplingParams(**gen_kwargs, stop=stop_sequences),\n            use_tqdm=False,\n        )\n        generated_texts = [self.tokenizer.decode(outputs.outputs[0].token_ids) for outputs in vllm_outputs]\n\n        # The `include_stop_str_in_output` option does not work, because we let llm generate tokens, not strings.\n        # We manually remove the stop sequences from the generated texts.\n        if not gen_kwargs.get(\"include_stop_str_in_output\", False):\n            for stop in stop_sequences:\n                for i, gen_text in enumerate(generated_texts):\n                    stop_index = gen_text.find(stop)\n                    if stop_index != -1:\n                        generated_texts[i] = gen_text[:stop_index]\n        return generated_texts\n\n    def batch_generate_chat_response(\n        self,\n        chat_messages_list: list[list[dict[str, str]]],\n        **kwargs,\n    ) -&gt; list[str]:\n        chat_messages_as_string = [\n            self.tokenizer.apply_chat_template(\n                chat_messages,\n                tokenize=False,\n                add_generation_prompt=True,\n                chat_template=self.custom_chat_template,\n            )\n            for chat_messages in chat_messages_list\n        ]\n        return self.batch_complete_text(chat_messages_as_string, **kwargs)\n\n    def batch_compute_log_probs(\n        self, text_list: list[str], prefix_list: list[str] | None = None, stride: int | None = None\n    ) -&gt; list[float]:\n        batch_size = len(text_list)\n\n        # prepare prefix encoding\n        prefix_list = prefix_list if prefix_list else [\"\" for _ in range(batch_size)]\n        # If the prefix is an empty string, replace it with the bos token regardless of the model being trained with it.\n        # This is needed to correctly calculate the log probabilities of the first token.\n        for i in range(batch_size):\n            if prefix_list[i] == \"\":\n                prefix_list[i] = self.tokenizer.bos_token\n\n        batch_prefix_ids = tokenize_text_for_lm_prefix(\n            prefix_list,\n            self.tokenizer,\n            add_special_tokens=self.add_special_tokens,\n        )\n\n        # prepare continuation encoding\n        # If the last token is a special token, it is treated as a beginning of a new sentence.\n        batch_continuation_ids = tokenize_text_for_lm_continuation(\n            text_list,\n            self.tokenizer,\n            as_continuation=[prefix_ids[-1] not in self.tokenizer.all_special_ids for prefix_ids in batch_prefix_ids],\n        )\n\n        batch_input_ids = [\n            prefix + continuation for prefix, continuation in zip(batch_prefix_ids, batch_continuation_ids)\n        ]\n\n        max_length = self.llm.llm_engine.get_model_config().max_seq_len_to_capture\n        stride = stride or max_length // 2\n        if not (0 &lt; stride &lt; max_length):\n            msg = f\"stride must be in (0, {max_length}), but got {stride}\"\n            raise ValueError(msg)\n        sequence_length = max([len(input_ids) for input_ids in batch_input_ids])\n\n        from vllm import RequestOutput, SamplingParams\n        from vllm.sequence import Logprob\n\n        sampling_params = SamplingParams(temperature=0.0, max_tokens=1, prompt_logprobs=1)\n\n        batch_logprobs = [0.0] * batch_size\n        last_computed_index = 0\n        for chunk_start in range(0, sequence_length, stride):\n            chunk_end = min(chunk_start + max_length, sequence_length)\n            chunk_batch_input_ids = [input_ids[chunk_start:chunk_end] for input_ids in batch_input_ids]\n            chunk_batch_input_ids = [\n                [self.tokenizer.bos_token_id] if len(chunk_input_ids) == 0 else chunk_input_ids\n                for chunk_input_ids in chunk_batch_input_ids\n            ]\n            chunk_batch_outputs: list[RequestOutput] = self.llm.generate(\n                prompt_token_ids=chunk_batch_input_ids,\n                sampling_params=sampling_params,\n                use_tqdm=False,\n            )\n\n            i = 0\n            for ids, output, prefix_ids in zip(chunk_batch_input_ids, chunk_batch_outputs, batch_prefix_ids):\n                chunk_rest_prefix_length = max(len(prefix_ids) - last_computed_index, 0)\n                chunk_continuation_start = last_computed_index - chunk_start + chunk_rest_prefix_length\n\n                # `prompt_logprobs` has the same length as the input `ids`.\n                # The i-th element contains the log probabilities of the i-th token in `ids`\n                # and the highest-likelihood token at that position.\n                # The 0-th element is always `None` because the log probability cannot be computed for it.\n                prompt_logprobs: list[dict[int, Logprob] | None] = output.prompt_logprobs\n                all_token_logprobs = [\n                    cands[token_id].logprob if cands else 0.0 for cands, token_id in zip(prompt_logprobs, ids)\n                ]\n                continuation_logprob = float(sum(all_token_logprobs[chunk_continuation_start:]))\n                batch_logprobs[i] += continuation_logprob\n                i += 1\n\n            last_computed_index = chunk_end\n\n        return batch_logprobs\n\n    def batch_compute_chat_log_probs(\n        self, prompt_list: list[list[dict[str, str]]], response_list: list[dict[str, str]]\n    ) -&gt; list[float]:\n        prompt_as_string: list[str] = []\n        response_as_string: list[str] = []\n        for prompt, response in zip(prompt_list, response_list):\n            prompt_as_string_i, response_as_string_i = get_prefix_and_completion_from_chat(\n                prompt,\n                response,\n                self.tokenizer,\n                custom_chat_template=self.custom_chat_template,\n            )\n            prompt_as_string.append(prompt_as_string_i)\n            response_as_string.append(response_as_string_i)\n        return self.batch_compute_log_probs(prompt_as_string, prefix_list=response_as_string)\n\n    def __repr__(self) -&gt; str:\n        return f\"VLLM(model_name={self.model_name})\"\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.vllm_model.VLLM.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name = model\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.vllm_model.VLLM.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer: PreTrainedTokenizer = from_pretrained(tokenizer, **tokenizer_kwargs)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.vllm_model.VLLM.custom_chat_template","title":"custom_chat_template  <code>instance-attribute</code>","text":"<pre><code>custom_chat_template = custom_chat_template\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.vllm_model.VLLM.add_special_tokens","title":"add_special_tokens  <code>instance-attribute</code>","text":"<pre><code>add_special_tokens = add_special_tokens\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.vllm_model.VLLM.default_gen_kwargs","title":"default_gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>default_gen_kwargs = default_gen_kwargs or {'temperature': 0.0}\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.vllm_model.VLLM.llm","title":"llm  <code>instance-attribute</code>","text":"<pre><code>llm = LLM(model, **model_kwargs)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.vllm_model.VLLM.__init__","title":"__init__","text":"<pre><code>__init__(model: str, model_kwargs: dict[str, Any] | None = None, tokenizer: str | None = None, tokenizer_kwargs: dict[str, Any] | None = None, add_special_tokens: bool = False, custom_chat_template: str | None = None, default_gen_kwargs: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/vllm_model.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    model_kwargs: dict[str, Any] | None = None,\n    tokenizer: str | None = None,\n    tokenizer_kwargs: dict[str, Any] | None = None,\n    add_special_tokens: bool = False,\n    custom_chat_template: str | None = None,\n    default_gen_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    self.model_name = model\n    tokenizer = tokenizer if tokenizer else model\n    tokenizer_kwargs = tokenizer_kwargs or {}\n    self.tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(tokenizer, **tokenizer_kwargs)\n    self.custom_chat_template = custom_chat_template\n    self.add_special_tokens = add_special_tokens\n    # use greedy decoding by default to make it consistent with `HuggingFaceLM`\n    self.default_gen_kwargs = default_gen_kwargs or {\"temperature\": 0.0}\n    # convert the flexeval-specific argument name to the vllm-specific name\n    if \"max_new_tokens\" in self.default_gen_kwargs:\n        self.default_gen_kwargs[\"max_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n\n    # import from vllm here because it is an extra dependency\n    from vllm import LLM\n\n    model_kwargs = model_kwargs or {}\n    # automatically set tensor_parallel_size to the number of GPUs\n    if \"tensor_parallel_size\" not in model_kwargs:\n        model_kwargs[\"tensor_parallel_size\"] = torch.cuda.device_count()\n    if \"enable_chunked_prefill\" not in model_kwargs:\n        model_kwargs[\"enable_chunked_prefill\"] = True\n        model_kwargs[\"disable_sliding_window\"] = True\n    self.llm = LLM(model, **model_kwargs)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.vllm_model.VLLM.batch_complete_text","title":"batch_complete_text","text":"<pre><code>batch_complete_text(text_list: list[str], stop_sequences: str | list[str] | None = None, max_new_tokens: int | None = None, **kwargs) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/language_model/vllm_model.py</code> <pre><code>def batch_complete_text(\n    self,\n    text_list: list[str],\n    stop_sequences: str | list[str] | None = None,\n    max_new_tokens: int | None = None,\n    **kwargs,\n) -&gt; list[str]:\n    gen_kwargs = self.default_gen_kwargs.copy()\n    gen_kwargs.update(kwargs)\n    if max_new_tokens is not None:\n        gen_kwargs[\"max_tokens\"] = max_new_tokens\n\n    stop_sequences = normalize_stop_sequences(\n        stop_sequences_list=[\n            stop_sequences,\n            gen_kwargs.pop(\"stop\", None),  # This is used in the vllm `SamplingParams`\n            gen_kwargs.pop(\"stop_sequences\", None),  # This is a common variable name used in flexeval\n        ],\n        eos_token=self.tokenizer.eos_token,\n        ignore_eos=gen_kwargs.get(\"ignore_eos\", False),\n    )\n\n    model_inputs = self.tokenizer(\n        text_list,\n        add_special_tokens=self.add_special_tokens,\n        return_token_type_ids=False,\n    )\n\n    from vllm import SamplingParams\n\n    vllm_outputs = self.llm.generate(\n        prompt_token_ids=model_inputs.input_ids,\n        sampling_params=SamplingParams(**gen_kwargs, stop=stop_sequences),\n        use_tqdm=False,\n    )\n    generated_texts = [self.tokenizer.decode(outputs.outputs[0].token_ids) for outputs in vllm_outputs]\n\n    # The `include_stop_str_in_output` option does not work, because we let llm generate tokens, not strings.\n    # We manually remove the stop sequences from the generated texts.\n    if not gen_kwargs.get(\"include_stop_str_in_output\", False):\n        for stop in stop_sequences:\n            for i, gen_text in enumerate(generated_texts):\n                stop_index = gen_text.find(stop)\n                if stop_index != -1:\n                    generated_texts[i] = gen_text[:stop_index]\n    return generated_texts\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.vllm_model.VLLM.batch_generate_chat_response","title":"batch_generate_chat_response","text":"<pre><code>batch_generate_chat_response(chat_messages_list: list[list[dict[str, str]]], **kwargs) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/language_model/vllm_model.py</code> <pre><code>def batch_generate_chat_response(\n    self,\n    chat_messages_list: list[list[dict[str, str]]],\n    **kwargs,\n) -&gt; list[str]:\n    chat_messages_as_string = [\n        self.tokenizer.apply_chat_template(\n            chat_messages,\n            tokenize=False,\n            add_generation_prompt=True,\n            chat_template=self.custom_chat_template,\n        )\n        for chat_messages in chat_messages_list\n    ]\n    return self.batch_complete_text(chat_messages_as_string, **kwargs)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.vllm_model.VLLM.batch_compute_log_probs","title":"batch_compute_log_probs","text":"<pre><code>batch_compute_log_probs(text_list: list[str], prefix_list: list[str] | None = None, stride: int | None = None) -&gt; list[float]\n</code></pre> Source code in <code>flexeval/core/language_model/vllm_model.py</code> <pre><code>def batch_compute_log_probs(\n    self, text_list: list[str], prefix_list: list[str] | None = None, stride: int | None = None\n) -&gt; list[float]:\n    batch_size = len(text_list)\n\n    # prepare prefix encoding\n    prefix_list = prefix_list if prefix_list else [\"\" for _ in range(batch_size)]\n    # If the prefix is an empty string, replace it with the bos token regardless of the model being trained with it.\n    # This is needed to correctly calculate the log probabilities of the first token.\n    for i in range(batch_size):\n        if prefix_list[i] == \"\":\n            prefix_list[i] = self.tokenizer.bos_token\n\n    batch_prefix_ids = tokenize_text_for_lm_prefix(\n        prefix_list,\n        self.tokenizer,\n        add_special_tokens=self.add_special_tokens,\n    )\n\n    # prepare continuation encoding\n    # If the last token is a special token, it is treated as a beginning of a new sentence.\n    batch_continuation_ids = tokenize_text_for_lm_continuation(\n        text_list,\n        self.tokenizer,\n        as_continuation=[prefix_ids[-1] not in self.tokenizer.all_special_ids for prefix_ids in batch_prefix_ids],\n    )\n\n    batch_input_ids = [\n        prefix + continuation for prefix, continuation in zip(batch_prefix_ids, batch_continuation_ids)\n    ]\n\n    max_length = self.llm.llm_engine.get_model_config().max_seq_len_to_capture\n    stride = stride or max_length // 2\n    if not (0 &lt; stride &lt; max_length):\n        msg = f\"stride must be in (0, {max_length}), but got {stride}\"\n        raise ValueError(msg)\n    sequence_length = max([len(input_ids) for input_ids in batch_input_ids])\n\n    from vllm import RequestOutput, SamplingParams\n    from vllm.sequence import Logprob\n\n    sampling_params = SamplingParams(temperature=0.0, max_tokens=1, prompt_logprobs=1)\n\n    batch_logprobs = [0.0] * batch_size\n    last_computed_index = 0\n    for chunk_start in range(0, sequence_length, stride):\n        chunk_end = min(chunk_start + max_length, sequence_length)\n        chunk_batch_input_ids = [input_ids[chunk_start:chunk_end] for input_ids in batch_input_ids]\n        chunk_batch_input_ids = [\n            [self.tokenizer.bos_token_id] if len(chunk_input_ids) == 0 else chunk_input_ids\n            for chunk_input_ids in chunk_batch_input_ids\n        ]\n        chunk_batch_outputs: list[RequestOutput] = self.llm.generate(\n            prompt_token_ids=chunk_batch_input_ids,\n            sampling_params=sampling_params,\n            use_tqdm=False,\n        )\n\n        i = 0\n        for ids, output, prefix_ids in zip(chunk_batch_input_ids, chunk_batch_outputs, batch_prefix_ids):\n            chunk_rest_prefix_length = max(len(prefix_ids) - last_computed_index, 0)\n            chunk_continuation_start = last_computed_index - chunk_start + chunk_rest_prefix_length\n\n            # `prompt_logprobs` has the same length as the input `ids`.\n            # The i-th element contains the log probabilities of the i-th token in `ids`\n            # and the highest-likelihood token at that position.\n            # The 0-th element is always `None` because the log probability cannot be computed for it.\n            prompt_logprobs: list[dict[int, Logprob] | None] = output.prompt_logprobs\n            all_token_logprobs = [\n                cands[token_id].logprob if cands else 0.0 for cands, token_id in zip(prompt_logprobs, ids)\n            ]\n            continuation_logprob = float(sum(all_token_logprobs[chunk_continuation_start:]))\n            batch_logprobs[i] += continuation_logprob\n            i += 1\n\n        last_computed_index = chunk_end\n\n    return batch_logprobs\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.vllm_model.VLLM.batch_compute_chat_log_probs","title":"batch_compute_chat_log_probs","text":"<pre><code>batch_compute_chat_log_probs(prompt_list: list[list[dict[str, str]]], response_list: list[dict[str, str]]) -&gt; list[float]\n</code></pre> Source code in <code>flexeval/core/language_model/vllm_model.py</code> <pre><code>def batch_compute_chat_log_probs(\n    self, prompt_list: list[list[dict[str, str]]], response_list: list[dict[str, str]]\n) -&gt; list[float]:\n    prompt_as_string: list[str] = []\n    response_as_string: list[str] = []\n    for prompt, response in zip(prompt_list, response_list):\n        prompt_as_string_i, response_as_string_i = get_prefix_and_completion_from_chat(\n            prompt,\n            response,\n            self.tokenizer,\n            custom_chat_template=self.custom_chat_template,\n        )\n        prompt_as_string.append(prompt_as_string_i)\n        response_as_string.append(response_as_string_i)\n    return self.batch_compute_log_probs(prompt_as_string, prefix_list=response_as_string)\n</code></pre>"},{"location":"api_reference/LanguageModel/#flexeval.core.language_model.vllm_model.VLLM.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/language_model/vllm_model.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"VLLM(model_name={self.model_name})\"\n</code></pre>"},{"location":"api_reference/MatchMaker/","title":"MatchMaker","text":""},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.base.MatchMaker","title":"MatchMaker","text":"<p>Generate matches between items from different models.</p> <p>The output is instances of the <code>Match</code> class.</p> Source code in <code>flexeval/core/pairwise_comparison/match_maker/base.py</code> <pre><code>class MatchMaker(ABC):\n    \"\"\"Generate matches between items from different models.\n\n    The output is instances of the `Match` class.\n    \"\"\"\n\n    @abstractmethod\n    def generate_matches(\n        self,\n        model_items: dict[str, list[T]],\n        cached_matches: list[Match] | None = None,\n    ) -&gt; Iterable[Match]:\n        pass\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.base.MatchMaker.generate_matches","title":"generate_matches  <code>abstractmethod</code>","text":"<pre><code>generate_matches(model_items: dict[str, list[T]], cached_matches: list[Match] | None = None) -&gt; Iterable[Match]\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/match_maker/base.py</code> <pre><code>@abstractmethod\ndef generate_matches(\n    self,\n    model_items: dict[str, list[T]],\n    cached_matches: list[Match] | None = None,\n) -&gt; Iterable[Match]:\n    pass\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.all_combinations.AllCombinations","title":"AllCombinations","text":"Source code in <code>flexeval/core/pairwise_comparison/match_maker/all_combinations.py</code> <pre><code>class AllCombinations(MatchMaker):\n    def __init__(self, include_reversed: bool = True) -&gt; None:\n        self.include_reversed = include_reversed\n\n    def generate_matches(self, model_items: dict[str, list[T]], cached_matches: list[Match] | None) -&gt; Iterable[Match]:\n        model_names = sorted(model_items.keys())\n        all_combinations = list(itertools.combinations(model_names, 2))\n\n        cached_matches = cached_matches or []\n        cache_dict = {match.get_key_for_cache(): match for match in cached_matches}\n\n        if self.include_reversed:\n            all_combinations += [(m2, m1) for m1, m2 in all_combinations]\n\n        for m1, m2 in all_combinations:\n            for item1, item2 in zip(model_items[m1], model_items[m2]):\n                match = Match(m1, item1, m2, item2)\n                if cached_match := cache_dict.get(match.get_key_for_cache()):\n                    yield cached_match\n                else:\n                    yield match\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.all_combinations.AllCombinations.include_reversed","title":"include_reversed  <code>instance-attribute</code>","text":"<pre><code>include_reversed = include_reversed\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.all_combinations.AllCombinations.__init__","title":"__init__","text":"<pre><code>__init__(include_reversed: bool = True) -&gt; None\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/match_maker/all_combinations.py</code> <pre><code>def __init__(self, include_reversed: bool = True) -&gt; None:\n    self.include_reversed = include_reversed\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.all_combinations.AllCombinations.generate_matches","title":"generate_matches","text":"<pre><code>generate_matches(model_items: dict[str, list[T]], cached_matches: list[Match] | None) -&gt; Iterable[Match]\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/match_maker/all_combinations.py</code> <pre><code>def generate_matches(self, model_items: dict[str, list[T]], cached_matches: list[Match] | None) -&gt; Iterable[Match]:\n    model_names = sorted(model_items.keys())\n    all_combinations = list(itertools.combinations(model_names, 2))\n\n    cached_matches = cached_matches or []\n    cache_dict = {match.get_key_for_cache(): match for match in cached_matches}\n\n    if self.include_reversed:\n        all_combinations += [(m2, m1) for m1, m2 in all_combinations]\n\n    for m1, m2 in all_combinations:\n        for item1, item2 in zip(model_items[m1], model_items[m2]):\n            match = Match(m1, item1, m2, item2)\n            if cached_match := cache_dict.get(match.get_key_for_cache()):\n                yield cached_match\n            else:\n                yield match\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.random_combinations.RandomCombinations","title":"RandomCombinations","text":"Source code in <code>flexeval/core/pairwise_comparison/match_maker/random_combinations.py</code> <pre><code>class RandomCombinations(MatchMaker):\n    def __init__(self, n: int = 100, incremental: bool = False, seed: int = 42) -&gt; None:\n        self.n = n\n        self.incremental = incremental\n        self.seed = seed\n\n    def generate_matches(\n        self,\n        model_items: dict[str, list[T]],\n        cached_matches: list[Match] | None = None,\n    ) -&gt; Iterable[Match]:\n        model_names = sorted(model_items.keys())\n        all_permutations = list(itertools.permutations(model_names, 2))\n\n        cached_matches = cached_matches or []\n        cache_dict = {match.get_key_for_cache(): match for match in cached_matches}\n        model_match_counter: dict[str, int] = {name: 0 for name in model_names}\n        possible_new_matches: list[Match] = []\n        matches: list[Match] = []\n        for m1, m2 in all_permutations:\n            for item1, item2 in zip(model_items[m1], model_items[m2]):\n                match = Match(m1, item1, m2, item2)\n                if cached_match := cache_dict.get(match.get_key_for_cache()):\n                    matches.append(cached_match)\n                    model_match_counter[m1] += 1\n                    model_match_counter[m2] += 1\n                else:\n                    possible_new_matches.append(Match(m1, item1, m2, item2))\n\n        # If `self.incremental` is `True`, add n more matches in addition to the cached data.\n        max_matches = self.n + len(matches) if self.incremental else self.n\n\n        random.seed(self.seed)\n\n        # For each iteration, assign the model with the fewest matches to a new match.\n        while (len(matches) &lt; max_matches) and (len(possible_new_matches) &gt; 0):\n            target_model = min(model_match_counter, key=model_match_counter.get)\n            candidate_matches = [\n                (i, match)\n                for i, match in enumerate(possible_new_matches)\n                if target_model in (match.model1, match.model2)\n            ]\n            index, selected_match = random.choice(candidate_matches)\n            matches.append(selected_match)\n            del possible_new_matches[index]\n            model_match_counter[selected_match.model1] += 1\n            model_match_counter[selected_match.model2] += 1\n\n        for match in matches:\n            yield match\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.random_combinations.RandomCombinations.n","title":"n  <code>instance-attribute</code>","text":"<pre><code>n = n\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.random_combinations.RandomCombinations.incremental","title":"incremental  <code>instance-attribute</code>","text":"<pre><code>incremental = incremental\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.random_combinations.RandomCombinations.seed","title":"seed  <code>instance-attribute</code>","text":"<pre><code>seed = seed\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.random_combinations.RandomCombinations.__init__","title":"__init__","text":"<pre><code>__init__(n: int = 100, incremental: bool = False, seed: int = 42) -&gt; None\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/match_maker/random_combinations.py</code> <pre><code>def __init__(self, n: int = 100, incremental: bool = False, seed: int = 42) -&gt; None:\n    self.n = n\n    self.incremental = incremental\n    self.seed = seed\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.random_combinations.RandomCombinations.generate_matches","title":"generate_matches","text":"<pre><code>generate_matches(model_items: dict[str, list[T]], cached_matches: list[Match] | None = None) -&gt; Iterable[Match]\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/match_maker/random_combinations.py</code> <pre><code>def generate_matches(\n    self,\n    model_items: dict[str, list[T]],\n    cached_matches: list[Match] | None = None,\n) -&gt; Iterable[Match]:\n    model_names = sorted(model_items.keys())\n    all_permutations = list(itertools.permutations(model_names, 2))\n\n    cached_matches = cached_matches or []\n    cache_dict = {match.get_key_for_cache(): match for match in cached_matches}\n    model_match_counter: dict[str, int] = {name: 0 for name in model_names}\n    possible_new_matches: list[Match] = []\n    matches: list[Match] = []\n    for m1, m2 in all_permutations:\n        for item1, item2 in zip(model_items[m1], model_items[m2]):\n            match = Match(m1, item1, m2, item2)\n            if cached_match := cache_dict.get(match.get_key_for_cache()):\n                matches.append(cached_match)\n                model_match_counter[m1] += 1\n                model_match_counter[m2] += 1\n            else:\n                possible_new_matches.append(Match(m1, item1, m2, item2))\n\n    # If `self.incremental` is `True`, add n more matches in addition to the cached data.\n    max_matches = self.n + len(matches) if self.incremental else self.n\n\n    random.seed(self.seed)\n\n    # For each iteration, assign the model with the fewest matches to a new match.\n    while (len(matches) &lt; max_matches) and (len(possible_new_matches) &gt; 0):\n        target_model = min(model_match_counter, key=model_match_counter.get)\n        candidate_matches = [\n            (i, match)\n            for i, match in enumerate(possible_new_matches)\n            if target_model in (match.model1, match.model2)\n        ]\n        index, selected_match = random.choice(candidate_matches)\n        matches.append(selected_match)\n        del possible_new_matches[index]\n        model_match_counter[selected_match.model1] += 1\n        model_match_counter[selected_match.model2] += 1\n\n    for match in matches:\n        yield match\n</code></pre>"},{"location":"api_reference/Metric/","title":"Metric","text":""},{"location":"api_reference/Metric/#flexeval.core.metric.base.Metric","title":"Metric","text":"<p>Base class for metrics.</p> Source code in <code>flexeval/core/metric/base.py</code> <pre><code>class Metric(ABC):\n    \"\"\"\n    Base class for metrics.\n    \"\"\"\n\n    @abstractmethod\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]],\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        \"\"\"\n        Evaluate the outputs of `LanguageModel` against the references.\n\n        Args:\n            lm_outputs: List of model outputs.\n            references_list: List of reference outputs.\n            task_inputs_list: List of task inputs.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.base.Metric.evaluate","title":"evaluate  <code>abstractmethod</code>","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]], task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> <p>Evaluate the outputs of <code>LanguageModel</code> against the references.</p> <p>Parameters:</p> <ul> <li> <code>lm_outputs</code>               (<code>list[str]</code>)           \u2013            <p>List of model outputs.</p> </li> <li> <code>references_list</code>               (<code>list[list[str]]</code>)           \u2013            <p>List of reference outputs.</p> </li> <li> <code>task_inputs_list</code>               (<code>list[dict[str, str]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of task inputs.</p> </li> </ul> Source code in <code>flexeval/core/metric/base.py</code> <pre><code>@abstractmethod\ndef evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]],\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    \"\"\"\n    Evaluate the outputs of `LanguageModel` against the references.\n\n    Args:\n        lm_outputs: List of model outputs.\n        references_list: List of reference outputs.\n        task_inputs_list: List of task inputs.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.base.MetricResult","title":"MetricResult  <code>dataclass</code>","text":"<p>A dataclass representing the result of a metric evaluation.</p> Source code in <code>flexeval/core/metric/base.py</code> <pre><code>@dataclass\nclass MetricResult:\n    \"\"\"\n    A dataclass representing the result of a metric evaluation.\n    \"\"\"\n\n    summary: dict[str, Any]\n    \"\"\"\n    Summary containing aggregated metric values.\n    \"\"\"\n    instance_details: list[dict[str, Any]] | None = None\n    \"\"\"\n    A list of evaluate details for each instance.\n    Useful for error analysis.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.base.MetricResult.summary","title":"summary  <code>instance-attribute</code>","text":"<pre><code>summary: dict[str, Any]\n</code></pre> <p>Summary containing aggregated metric values.</p>"},{"location":"api_reference/Metric/#flexeval.core.metric.base.MetricResult.instance_details","title":"instance_details  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>instance_details: list[dict[str, Any]] | None = None\n</code></pre> <p>A list of evaluate details for each instance. Useful for error analysis.</p>"},{"location":"api_reference/Metric/#flexeval.core.metric.base.MetricResult.__init__","title":"__init__","text":"<pre><code>__init__(summary: dict[str, Any], instance_details: list[dict[str, Any]] | None = None) -&gt; None\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.bleu.BLEU","title":"BLEU","text":"<p>An implementation of BLEU. The calculation is based on the sacrebleu library.</p> <p>Parameters:</p> <ul> <li> <code>tokenize_option</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Tokenization option for sacrebleu. If <code>None</code>, sacrebleu will use the default tokenization.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import BLEU\n&gt;&gt;&gt; bleu = BLEU()\n&gt;&gt;&gt; lm_outputs = [\"I am a student .\", \"I am a teacher .\"]\n&gt;&gt;&gt; references_list = [[\"I am a student .\", \"I am a learner .\"], [\"I am a teacher .\"]]\n&gt;&gt;&gt; result = bleu.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={\n        'bleu_score': 1.0,\n        'bleu_bp': 1.0,\n        'bleu_signature': nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.1},\n        instance_details=[\n            {'bleu_score': 1.0, 'bleu_bp': 1.0},\n            {'bleu_score': 1.0, 'bleu_bp': 1.0}\n        ]\n    )\n</code></pre> Source code in <code>flexeval/core/metric/bleu.py</code> <pre><code>class BLEU(Metric):\n    \"\"\"An implementation of [BLEU](https://aclanthology.org/P02-1040/).\n    The calculation is based on the [sacrebleu](https://github.com/mjpost/sacrebleu) library.\n\n    Args:\n        tokenize_option: Tokenization option for sacrebleu.\n            If `None`, sacrebleu will use the default tokenization.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import BLEU\n        &gt;&gt;&gt; bleu = BLEU()\n        &gt;&gt;&gt; lm_outputs = [\"I am a student .\", \"I am a teacher .\"]\n        &gt;&gt;&gt; references_list = [[\"I am a student .\", \"I am a learner .\"], [\"I am a teacher .\"]]\n        &gt;&gt;&gt; result = bleu.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={\n                'bleu_score': 1.0,\n                'bleu_bp': 1.0,\n                'bleu_signature': nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.1},\n                instance_details=[\n                    {'bleu_score': 1.0, 'bleu_bp': 1.0},\n                    {'bleu_score': 1.0, 'bleu_bp': 1.0}\n                ]\n            )\n    \"\"\"\n\n    def __init__(self, tokenize_option: str | None = None) -&gt; None:\n        self._bleu = sacrebleu.metrics.BLEU(tokenize=tokenize_option)\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]],\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if len(lm_outputs) != len(references_list):\n            msg = (\n                f\"lm_outputs and references_list must have the same length, \"\n                f\"but got {len(lm_outputs)} and {len(references_list)}.\"\n            )\n            raise ValueError(msg)\n\n        # we need restructure the references to match the format expected by sacrebleu\n        max_num_refs = max(len(refs) for refs in references_list)\n        references_for_sacrebleu: list[list[str]] = []\n        for i in range(max_num_refs):\n            set_of_references: list[str] = []\n            for refs_for_source in references_list:\n                if i &lt; len(refs_for_source):\n                    set_of_references.append(refs_for_source[i])\n                else:\n                    set_of_references.append(\"\")\n            references_for_sacrebleu.append(set_of_references)\n\n        bleu = self._bleu.corpus_score([o.strip() for o in lm_outputs], references_for_sacrebleu)\n        sentence_bleu_list = [\n            self._bleu.sentence_score(o.strip(), refs) for o, refs in zip(lm_outputs, references_list)\n        ]\n\n        return MetricResult(\n            {\n                \"bleu_score\": bleu.score / 100,\n                \"bleu_bp\": bleu.bp,\n                \"bleu_signature\": self._bleu.get_signature(),\n            },\n            instance_details=[{\"bleu_score\": b.score / 100, \"bleu_bp\": b.bp} for b in sentence_bleu_list],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.bleu.BLEU.__init__","title":"__init__","text":"<pre><code>__init__(tokenize_option: str | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/bleu.py</code> <pre><code>def __init__(self, tokenize_option: str | None = None) -&gt; None:\n    self._bleu = sacrebleu.metrics.BLEU(tokenize=tokenize_option)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.bleu.BLEU.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]], task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/bleu.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]],\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if len(lm_outputs) != len(references_list):\n        msg = (\n            f\"lm_outputs and references_list must have the same length, \"\n            f\"but got {len(lm_outputs)} and {len(references_list)}.\"\n        )\n        raise ValueError(msg)\n\n    # we need restructure the references to match the format expected by sacrebleu\n    max_num_refs = max(len(refs) for refs in references_list)\n    references_for_sacrebleu: list[list[str]] = []\n    for i in range(max_num_refs):\n        set_of_references: list[str] = []\n        for refs_for_source in references_list:\n            if i &lt; len(refs_for_source):\n                set_of_references.append(refs_for_source[i])\n            else:\n                set_of_references.append(\"\")\n        references_for_sacrebleu.append(set_of_references)\n\n    bleu = self._bleu.corpus_score([o.strip() for o in lm_outputs], references_for_sacrebleu)\n    sentence_bleu_list = [\n        self._bleu.sentence_score(o.strip(), refs) for o, refs in zip(lm_outputs, references_list)\n    ]\n\n    return MetricResult(\n        {\n            \"bleu_score\": bleu.score / 100,\n            \"bleu_bp\": bleu.bp,\n            \"bleu_signature\": self._bleu.get_signature(),\n        },\n        instance_details=[{\"bleu_score\": b.score / 100, \"bleu_bp\": b.bp} for b in sentence_bleu_list],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.char_f1.CharF1","title":"CharF1","text":"<p>A metric that calculates how many characters in the output string are included in the characters of the expected output. If there are multiple expected outputs, the highest score is adopted.</p> <p>Parameters:</p> <ul> <li> <code>processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>StringProcessor or list of Normalizers to apply to the model outputs before comparison. Unless reference_processor is specified, this processor will be applied to the references as well.</p> </li> <li> <code>reference_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>StringProcessor or list of Normalizers to apply to the references before comparison.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import CharF1\n&gt;&gt;&gt; char_f1 = CharF1()\n&gt;&gt;&gt; lm_outputs = [\"abcd\", \"efgh\"]\n&gt;&gt;&gt; references_list = [[\"abcd\", \"ABCD\"], [\"efGH\"]]\n&gt;&gt;&gt; result = char_f1.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(summary={'char_f1': 0.75}, instance_details=[{'char_f1': 1.0}, {'char_f1': 0.5}])\n</code></pre> Source code in <code>flexeval/core/metric/char_f1.py</code> <pre><code>class CharF1(Metric):\n    \"\"\"\n    A metric that calculates how many characters in the output string are included\n    in the characters of the expected output.\n    If there are multiple expected outputs, the highest score is adopted.\n\n    Args:\n        processor: StringProcessor or list of Normalizers to apply to the model outputs before comparison.\n            Unless reference_processor is specified, this processor will be applied to the references as well.\n        reference_processor: StringProcessor or list of Normalizers to apply to the references before comparison.\n\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import CharF1\n        &gt;&gt;&gt; char_f1 = CharF1()\n        &gt;&gt;&gt; lm_outputs = [\"abcd\", \"efgh\"]\n        &gt;&gt;&gt; references_list = [[\"abcd\", \"ABCD\"], [\"efGH\"]]\n        &gt;&gt;&gt; result = char_f1.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(summary={'char_f1': 0.75}, instance_details=[{'char_f1': 1.0}, {'char_f1': 0.5}])\n    \"\"\"\n\n    def __init__(\n        self,\n        processor: StringProcessor | list[StringProcessor] | None = None,\n        reference_processor: StringProcessor | list[StringProcessor] | None = None,\n    ) -&gt; None:\n        if isinstance(processor, StringProcessor):\n            processor = [processor]\n        if isinstance(reference_processor, StringProcessor):\n            reference_processor = [reference_processor]\n\n        self.processors = processor\n        self.reference_processors = reference_processor or processor\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]],\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if self.processors:\n            lm_outputs = [functools.reduce(lambda x, norm: norm(x), self.processors, output) for output in lm_outputs]\n\n        if self.reference_processors:\n            references_list = [\n                [functools.reduce(lambda x, norm: norm(x), self.reference_processors, ref) for ref in references]\n                for references in references_list\n            ]\n\n        char_f1_scores: list[float] = []\n        for lm_output, expected_output in zip(lm_outputs, references_list):\n            score = max(fuzz.ratio(lm_output, o) for o in expected_output) / 100\n            char_f1_scores.append(score)\n        return MetricResult(\n            {\"char_f1\": sum(char_f1_scores) / len(char_f1_scores)},\n            instance_details=[{\"char_f1\": s} for s in char_f1_scores],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.char_f1.CharF1.processors","title":"processors  <code>instance-attribute</code>","text":"<pre><code>processors = processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.char_f1.CharF1.reference_processors","title":"reference_processors  <code>instance-attribute</code>","text":"<pre><code>reference_processors = reference_processor or processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.char_f1.CharF1.__init__","title":"__init__","text":"<pre><code>__init__(processor: StringProcessor | list[StringProcessor] | None = None, reference_processor: StringProcessor | list[StringProcessor] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/char_f1.py</code> <pre><code>def __init__(\n    self,\n    processor: StringProcessor | list[StringProcessor] | None = None,\n    reference_processor: StringProcessor | list[StringProcessor] | None = None,\n) -&gt; None:\n    if isinstance(processor, StringProcessor):\n        processor = [processor]\n    if isinstance(reference_processor, StringProcessor):\n        reference_processor = [reference_processor]\n\n    self.processors = processor\n    self.reference_processors = reference_processor or processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.char_f1.CharF1.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]], task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/char_f1.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]],\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if self.processors:\n        lm_outputs = [functools.reduce(lambda x, norm: norm(x), self.processors, output) for output in lm_outputs]\n\n    if self.reference_processors:\n        references_list = [\n            [functools.reduce(lambda x, norm: norm(x), self.reference_processors, ref) for ref in references]\n            for references in references_list\n        ]\n\n    char_f1_scores: list[float] = []\n    for lm_output, expected_output in zip(lm_outputs, references_list):\n        score = max(fuzz.ratio(lm_output, o) for o in expected_output) / 100\n        char_f1_scores.append(score)\n    return MetricResult(\n        {\"char_f1\": sum(char_f1_scores) / len(char_f1_scores)},\n        instance_details=[{\"char_f1\": s} for s in char_f1_scores],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.code_eval.CodeEval","title":"CodeEval","text":"<p>A metric that evaluates generated code with test cases.</p> <p>Parameters:</p> <ul> <li> <code>code_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A Jinja2 template string to make the generated code. The template can contain variables from task inputs. If <code>None</code>, the code prompt will be the generated text itself.</p> </li> <li> <code>processor</code>               (<code>StringProcessor | None</code>, default:                   <code>None</code> )           \u2013            <p>A processor applied to model outputs before evaluation.</p> </li> <li> <code>evaluate_module</code>               (<code>str</code>, default:                   <code>'code_eval'</code> )           \u2013            <p>An evaluate module to use.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import CodeEval\n&gt;&gt;&gt; code_eval = CodeEval()\n&gt;&gt;&gt; lm_outputs = [\"def add(a, b):\\n    return a + b\", \"def is_equal(a, b):\\n    return a = b\"]\n&gt;&gt;&gt; references_list = [[\"assert add(1, 2) == 3\"], [\"assert is_equal(1, 2) == False\"]]\n&gt;&gt;&gt; result = code_eval.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'pass@1': 0.5},\n    instance_details=[\n        {'passed': True, 'result': 'passed'},\n        {'passed': False, 'result': 'failed: invalid syntax (&lt;string&gt;, line 2)'}\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/code_eval.py</code> <pre><code>class CodeEval(Metric):\n    \"\"\"\n    A metric that evaluates generated code with test cases.\n\n    Args:\n        code_template: A Jinja2 template string to make the generated code.\n            The template can contain variables from task inputs.\n            If `None`, the code prompt will be the generated text itself.\n        processor: A processor applied to model outputs before evaluation.\n        evaluate_module: An evaluate module to use.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import CodeEval\n        &gt;&gt;&gt; code_eval = CodeEval()\n        &gt;&gt;&gt; lm_outputs = [\"def add(a, b):\\\\n    return a + b\", \"def is_equal(a, b):\\\\n    return a = b\"]\n        &gt;&gt;&gt; references_list = [[\"assert add(1, 2) == 3\"], [\"assert is_equal(1, 2) == False\"]]\n        &gt;&gt;&gt; result = code_eval.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'pass@1': 0.5},\n            instance_details=[\n                {'passed': True, 'result': 'passed'},\n                {'passed': False, 'result': 'failed: invalid syntax (&lt;string&gt;, line 2)'}\n            ]\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        code_template: str | None = None,\n        processor: StringProcessor | None = None,\n        evaluate_module: str = \"code_eval\",\n    ) -&gt; None:\n        if code_template is None:\n            code_template = \"{{ lm_output }}\"\n\n        self.code_template = JINJA2_ENV.from_string(code_template)\n        self.code_eval = evaluate.load(evaluate_module)\n        self.processor = processor\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]],\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if task_inputs_list is None:\n            task_inputs_list = [{} for _ in lm_outputs]\n\n        generated_code_list: list[str] = []\n        test_case_list: list[str] = []\n        # in code generation tasks, references_list contains the test cases\n        for lm_output, task_inputs, test_cases in zip(\n            lm_outputs,\n            task_inputs_list,\n            references_list,\n        ):\n            if self.processor is not None:\n                lm_output = self.processor(lm_output)  # noqa: PLW2901\n\n            generated_code = self.code_template.render(lm_output=lm_output, **task_inputs)\n\n            generated_code_list.append(generated_code)\n            test_case_list.append(\"\\n\".join(test_cases))\n        pass_at_k, results = self.code_eval.compute(\n            references=test_case_list,\n            predictions=[[c] for c in generated_code_list],\n            k=[1],\n        )\n\n        # `results` contain the detailed results for each test case\n        # e.g., {0: [(0, {'task_id': 0, 'passed': False, 'result': \"failed\", 'completion_id': 0})]}\n        results: dict[int, list[tuple[int, dict[str, Any]]]]\n\n        instance_details: list[dict[str, Any]] = []\n        for i in range(len(lm_outputs)):\n            first_result = results[i][0]  # we only assume one candidate code per instance, so we take the first result\n            _, detail_result = first_result  # the first element is just the index so we ignore it\n            # remove unnecessary fields to save space\n            detail_result.pop(\"completion_id\")\n            detail_result.pop(\"task_id\")\n            instance_details.append(detail_result)\n\n        return MetricResult(pass_at_k, instance_details=instance_details)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.code_eval.CodeEval.code_template","title":"code_template  <code>instance-attribute</code>","text":"<pre><code>code_template = from_string(code_template)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.code_eval.CodeEval.code_eval","title":"code_eval  <code>instance-attribute</code>","text":"<pre><code>code_eval = load(evaluate_module)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.code_eval.CodeEval.processor","title":"processor  <code>instance-attribute</code>","text":"<pre><code>processor = processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.code_eval.CodeEval.__init__","title":"__init__","text":"<pre><code>__init__(code_template: str | None = None, processor: StringProcessor | None = None, evaluate_module: str = 'code_eval') -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/code_eval.py</code> <pre><code>def __init__(\n    self,\n    code_template: str | None = None,\n    processor: StringProcessor | None = None,\n    evaluate_module: str = \"code_eval\",\n) -&gt; None:\n    if code_template is None:\n        code_template = \"{{ lm_output }}\"\n\n    self.code_template = JINJA2_ENV.from_string(code_template)\n    self.code_eval = evaluate.load(evaluate_module)\n    self.processor = processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.code_eval.CodeEval.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]], task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/code_eval.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]],\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if task_inputs_list is None:\n        task_inputs_list = [{} for _ in lm_outputs]\n\n    generated_code_list: list[str] = []\n    test_case_list: list[str] = []\n    # in code generation tasks, references_list contains the test cases\n    for lm_output, task_inputs, test_cases in zip(\n        lm_outputs,\n        task_inputs_list,\n        references_list,\n    ):\n        if self.processor is not None:\n            lm_output = self.processor(lm_output)  # noqa: PLW2901\n\n        generated_code = self.code_template.render(lm_output=lm_output, **task_inputs)\n\n        generated_code_list.append(generated_code)\n        test_case_list.append(\"\\n\".join(test_cases))\n    pass_at_k, results = self.code_eval.compute(\n        references=test_case_list,\n        predictions=[[c] for c in generated_code_list],\n        k=[1],\n    )\n\n    # `results` contain the detailed results for each test case\n    # e.g., {0: [(0, {'task_id': 0, 'passed': False, 'result': \"failed\", 'completion_id': 0})]}\n    results: dict[int, list[tuple[int, dict[str, Any]]]]\n\n    instance_details: list[dict[str, Any]] = []\n    for i in range(len(lm_outputs)):\n        first_result = results[i][0]  # we only assume one candidate code per instance, so we take the first result\n        _, detail_result = first_result  # the first element is just the index so we ignore it\n        # remove unnecessary fields to save space\n        detail_result.pop(\"completion_id\")\n        detail_result.pop(\"task_id\")\n        instance_details.append(detail_result)\n\n    return MetricResult(pass_at_k, instance_details=instance_details)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.common_prefix_length.CommonPrefixLength","title":"CommonPrefixLength","text":"<p>A metric that calculates the length of the longest common prefix between the model output and the reference.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import CommonPrefixLength\n&gt;&gt;&gt; common_prefix_length = CommonPrefixLength()\n&gt;&gt;&gt; lm_outputs = [\"ABCDEFG\"]\n&gt;&gt;&gt; references_list = [[\"ABCdefg\"]]\n&gt;&gt;&gt; result = common_prefix_length.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={\"average_common_prefix_length\": 3.0, \"longest_common_prefix_length\": 3},\n    instance_details=[{\"common_prefix_length\": 3}],\n)\n</code></pre> Source code in <code>flexeval/core/metric/common_prefix_length.py</code> <pre><code>class CommonPrefixLength(Metric):\n    \"\"\"\n    A metric that calculates the length of the longest common prefix between the model output and the reference.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import CommonPrefixLength\n        &gt;&gt;&gt; common_prefix_length = CommonPrefixLength()\n        &gt;&gt;&gt; lm_outputs = [\"ABCDEFG\"]\n        &gt;&gt;&gt; references_list = [[\"ABCdefg\"]]\n        &gt;&gt;&gt; result = common_prefix_length.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={\"average_common_prefix_length\": 3.0, \"longest_common_prefix_length\": 3},\n            instance_details=[{\"common_prefix_length\": 3}],\n        )\n    \"\"\"\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]],\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        common_prefix_length_list: list[int] = []\n        for lm_output, references in zip(lm_outputs, references_list):\n            common_prefix_length = max(len(get_longest_common_prefix(lm_output, gt)) for gt in references)\n            common_prefix_length_list.append(common_prefix_length)\n\n        return MetricResult(\n            {\n                \"average_common_prefix_length\": sum(common_prefix_length_list) / len(common_prefix_length_list),\n                \"longest_common_prefix_length\": max(common_prefix_length_list),\n            },\n            instance_details=[{\"common_prefix_length\": s} for s in common_prefix_length_list],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.common_prefix_length.CommonPrefixLength.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]], task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/common_prefix_length.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]],\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    common_prefix_length_list: list[int] = []\n    for lm_output, references in zip(lm_outputs, references_list):\n        common_prefix_length = max(len(get_longest_common_prefix(lm_output, gt)) for gt in references)\n        common_prefix_length_list.append(common_prefix_length)\n\n    return MetricResult(\n        {\n            \"average_common_prefix_length\": sum(common_prefix_length_list) / len(common_prefix_length_list),\n            \"longest_common_prefix_length\": max(common_prefix_length_list),\n        },\n        instance_details=[{\"common_prefix_length\": s} for s in common_prefix_length_list],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.common_string_length.CommonStringLength","title":"CommonStringLength","text":"<p>A metric that calculates the length of the longest common substring between the model output and the reference.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import CommonStringLength\n&gt;&gt;&gt; common_string_length = CommonStringLength()\n&gt;&gt;&gt; lm_outputs = [\"aBCDEFG\"]\n&gt;&gt;&gt; references_list = [[\"ABCDefg\"]]\n&gt;&gt;&gt; result = common_string_length.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={\"average_common_string_length\": 3.0, \"longest_common_string_length\": 3},\n    instance_details=[{\"common_string_length\": 3}],\n)\n</code></pre> Source code in <code>flexeval/core/metric/common_string_length.py</code> <pre><code>class CommonStringLength(Metric):\n    \"\"\"\n    A metric that calculates the length of the longest common substring between the model output and the reference.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import CommonStringLength\n        &gt;&gt;&gt; common_string_length = CommonStringLength()\n        &gt;&gt;&gt; lm_outputs = [\"aBCDEFG\"]\n        &gt;&gt;&gt; references_list = [[\"ABCDefg\"]]\n        &gt;&gt;&gt; result = common_string_length.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={\"average_common_string_length\": 3.0, \"longest_common_string_length\": 3},\n            instance_details=[{\"common_string_length\": 3}],\n        )\n    \"\"\"\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]],\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        common_string_length_list: list[int] = []\n        for lm_output, references in zip(lm_outputs, references_list):\n            common_string_length = max(len(get_longest_common_substring(lm_output, gt)) for gt in references)\n            common_string_length_list.append(common_string_length)\n\n        return MetricResult(\n            {\n                \"average_common_string_length\": sum(common_string_length_list) / len(common_string_length_list),\n                \"longest_common_string_length\": max(common_string_length_list),\n            },\n            instance_details=[{\"common_string_length\": s} for s in common_string_length_list],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.common_string_length.CommonStringLength.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]], task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/common_string_length.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]],\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    common_string_length_list: list[int] = []\n    for lm_output, references in zip(lm_outputs, references_list):\n        common_string_length = max(len(get_longest_common_substring(lm_output, gt)) for gt in references)\n        common_string_length_list.append(common_string_length)\n\n    return MetricResult(\n        {\n            \"average_common_string_length\": sum(common_string_length_list) / len(common_string_length_list),\n            \"longest_common_string_length\": max(common_string_length_list),\n        },\n        instance_details=[{\"common_string_length\": s} for s in common_string_length_list],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.exact_match.ExactMatch","title":"ExactMatch","text":"<p>Exact match metric. If there are multiple references, the output is considered correct if it matches any of the references.</p> <p>Parameters:</p> <ul> <li> <code>processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>StringProcessor or a list of StringProcessor to be applied to the model outputs before comparison. Unless reference_processor is specified, this processor will be applied to the references as well.</p> </li> <li> <code>reference_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>StringProcessor or list of Normalizers to apply to the references before comparison.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import ExactMatch\n&gt;&gt;&gt; exact_match = ExactMatch()\n&gt;&gt;&gt; lm_outputs = [\"ABC\", \"DEF\"]\n&gt;&gt;&gt; references_list = [[\"ABC\"], [\"DEFG\"]]\n&gt;&gt;&gt; result = exact_match.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={\"exact_match\": 0.5},\n    instance_details=[{\"exact_match\": True}, {\"exact_match\": False}],\n)\n</code></pre> Source code in <code>flexeval/core/metric/exact_match.py</code> <pre><code>class ExactMatch(Metric):\n    \"\"\"\n    Exact match metric.\n    If there are multiple references, the output is considered correct if it matches any of the references.\n\n    Args:\n        processor: StringProcessor or a list of StringProcessor to be applied to the model outputs before comparison.\n            Unless reference_processor is specified, this processor will be applied to the references as well.\n        reference_processor: StringProcessor or list of Normalizers to apply to the references before comparison.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import ExactMatch\n        &gt;&gt;&gt; exact_match = ExactMatch()\n        &gt;&gt;&gt; lm_outputs = [\"ABC\", \"DEF\"]\n        &gt;&gt;&gt; references_list = [[\"ABC\"], [\"DEFG\"]]\n        &gt;&gt;&gt; result = exact_match.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={\"exact_match\": 0.5},\n            instance_details=[{\"exact_match\": True}, {\"exact_match\": False}],\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        processor: StringProcessor | list[StringProcessor] | None = None,\n        reference_processor: StringProcessor | list[StringProcessor] | None = None,\n    ) -&gt; None:\n        if isinstance(processor, StringProcessor):\n            processor = [processor]\n        if isinstance(reference_processor, StringProcessor):\n            reference_processor = [reference_processor]\n\n        self.processors = processor\n        self.reference_processors = reference_processor or processor\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]],\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if len(lm_outputs) != len(references_list):\n            msg = (\n                f\"Number of model outputs ({len(lm_outputs)}) and number of references ({len(references_list)}) \"\n                \"should be the same.\"\n            )\n            raise ValueError(msg)\n\n        if self.processors:\n            lm_outputs = [functools.reduce(lambda x, norm: norm(x), self.processors, output) for output in lm_outputs]\n\n        if self.reference_processors:\n            references_list = [\n                [functools.reduce(lambda x, norm: norm(x), self.reference_processors, ref) for ref in references]\n                for references in references_list\n            ]\n\n        exact_match_list = [\n            lm_output in expected_output for lm_output, expected_output in zip(lm_outputs, references_list)\n        ]\n\n        return MetricResult(\n            {\"exact_match\": sum(exact_match_list) / len(exact_match_list)},\n            instance_details=[{\"exact_match\": s} for s in exact_match_list],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.exact_match.ExactMatch.processors","title":"processors  <code>instance-attribute</code>","text":"<pre><code>processors = processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.exact_match.ExactMatch.reference_processors","title":"reference_processors  <code>instance-attribute</code>","text":"<pre><code>reference_processors = reference_processor or processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.exact_match.ExactMatch.__init__","title":"__init__","text":"<pre><code>__init__(processor: StringProcessor | list[StringProcessor] | None = None, reference_processor: StringProcessor | list[StringProcessor] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/exact_match.py</code> <pre><code>def __init__(\n    self,\n    processor: StringProcessor | list[StringProcessor] | None = None,\n    reference_processor: StringProcessor | list[StringProcessor] | None = None,\n) -&gt; None:\n    if isinstance(processor, StringProcessor):\n        processor = [processor]\n    if isinstance(reference_processor, StringProcessor):\n        reference_processor = [reference_processor]\n\n    self.processors = processor\n    self.reference_processors = reference_processor or processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.exact_match.ExactMatch.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]], task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/exact_match.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]],\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if len(lm_outputs) != len(references_list):\n        msg = (\n            f\"Number of model outputs ({len(lm_outputs)}) and number of references ({len(references_list)}) \"\n            \"should be the same.\"\n        )\n        raise ValueError(msg)\n\n    if self.processors:\n        lm_outputs = [functools.reduce(lambda x, norm: norm(x), self.processors, output) for output in lm_outputs]\n\n    if self.reference_processors:\n        references_list = [\n            [functools.reduce(lambda x, norm: norm(x), self.reference_processors, ref) for ref in references]\n            for references in references_list\n        ]\n\n    exact_match_list = [\n        lm_output in expected_output for lm_output, expected_output in zip(lm_outputs, references_list)\n    ]\n\n    return MetricResult(\n        {\"exact_match\": sum(exact_match_list) / len(exact_match_list)},\n        instance_details=[{\"exact_match\": s} for s in exact_match_list],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel","title":"ChatLLMLabel","text":"<p>A metric that evaluates the output of <code>LanguageModel.batch_generate_chat_response</code>.</p> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>LanguageModel</code>)           \u2013            <p>An instance of <code>LanguageModel</code> to evaluate the output of the model.</p> </li> <li> <code>prompt_template</code>               (<code>PromptTemplate</code>)           \u2013            <p>An instance of <code>PromptTemplate</code> to embed the input for the evaluator.</p> </li> <li> <code>label_names</code>               (<code>list[str]</code>)           \u2013            <p>A list of valid label names.</p> </li> <li> <code>label_points</code>               (<code>list[float | int] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of points for each label specified in label_names.</p> </li> <li> <code>system_message</code>               (<code>str | PromptTemplate | None</code>, default:                   <code>None</code> )           \u2013            <p>A system message to be prepended to the input for the evaluator.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The batch size for the evaluator.</p> </li> <li> <code>disable_tqdm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to disable the progress bar.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in task inputs.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import ChatLLMScore, OpenAIChatAPI, Jinja2PromptTemplate\n&gt;&gt;&gt; language_model = OpenAIChatAPI(model_name=\"gpt-3.5-turbo\")\n&gt;&gt;&gt; template = \"Evaluate the quality of this text on a scale of Good/Bad.\\n`{{ lm_output }}`\\nPut the label at the end like [[Good]].\"\n&gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n&gt;&gt;&gt; system_message = \"This is the system message.\"\n&gt;&gt;&gt; label_names = [\"Good\", \"Bad\"]\n&gt;&gt;&gt; label_points = [1.0, 0.0]\n&gt;&gt;&gt; llm_label = ChatLLMLabel(language_model, prompt_template, label_names, label_points)\n&gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n&gt;&gt;&gt; result = llm_label.evaluate(lm_outputs)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'llm_score': 0.5, 'llm_label_distribution': {'Good': 0.5, 'Bad': 0.5}, 'num_failed_score_parses': 0},\n    instance_details=[\n        {\n            'llm_label': 'Good',\n            'llm_score': 1.0,\n            'llm_label_input': 'Evaluate the quality of this text...',\n            'llm_label_output': 'This text is natural, ... [[Good]]'\n        },\n        {\n            'llm_label': 'Bad',\n            'llm_score': 0.0,\n            'llm_label_input': 'Evaluate the quality of this text on a scale of Good/Bad.\\n`Good mrrrning!`\\nPut the label at the end like [[Good]].',\n            'llm_label_output': 'This text contains a spelling error, ... [[Bad]]'\n        }\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>class ChatLLMLabel(Metric):\n    \"\"\"\n    A metric that evaluates the output of `LanguageModel.batch_generate_chat_response`.\n\n    Args:\n        language_model: An instance of `LanguageModel` to evaluate the output of the model.\n        prompt_template: An instance of `PromptTemplate` to embed the input for the evaluator.\n        label_names: A list of valid label names.\n        label_points: A list of points for each label specified in label_names.\n        system_message: A system message to be prepended to the input for the evaluator.\n        batch_size: The batch size for the evaluator.\n        disable_tqdm: Whether to disable the progress bar.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in task inputs.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import ChatLLMScore, OpenAIChatAPI, Jinja2PromptTemplate\n        &gt;&gt;&gt; language_model = OpenAIChatAPI(model_name=\"gpt-3.5-turbo\")\n        &gt;&gt;&gt; template = \"Evaluate the quality of this text on a scale of Good/Bad.\\\\n`{{ lm_output }}`\\\\nPut the label at the end like [[Good]].\"\n        &gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n        &gt;&gt;&gt; system_message = \"This is the system message.\"\n        &gt;&gt;&gt; label_names = [\"Good\", \"Bad\"]\n        &gt;&gt;&gt; label_points = [1.0, 0.0]\n        &gt;&gt;&gt; llm_label = ChatLLMLabel(language_model, prompt_template, label_names, label_points)\n        &gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n        &gt;&gt;&gt; result = llm_label.evaluate(lm_outputs)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'llm_score': 0.5, 'llm_label_distribution': {'Good': 0.5, 'Bad': 0.5}, 'num_failed_score_parses': 0},\n            instance_details=[\n                {\n                    'llm_label': 'Good',\n                    'llm_score': 1.0,\n                    'llm_label_input': 'Evaluate the quality of this text...',\n                    'llm_label_output': 'This text is natural, ... [[Good]]'\n                },\n                {\n                    'llm_label': 'Bad',\n                    'llm_score': 0.0,\n                    'llm_label_input': 'Evaluate the quality of this text on a scale of Good/Bad.\\\\n`Good mrrrning!`\\\\nPut the label at the end like [[Good]].',\n                    'llm_label_output': 'This text contains a spelling error, ... [[Bad]]'\n                }\n            ]\n        )\n    \"\"\"  # noqa: E501\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        label_names: list[str],\n        label_points: list[float | int] | None = None,\n        system_message: str | PromptTemplate | None = None,\n        batch_size: int = 4,\n        disable_tqdm: bool = False,\n        category_key: str | None = None,\n    ) -&gt; None:\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.label_names = [re.escape(label) for label in label_names]\n\n        if label_points:\n            if len(self.label_names) != len(label_points):\n                msg = \"The lengths of label_names and weights do not match.\"\n                raise ValueError(msg)\n            label_points: list[float] = list(map(float, label_points))\n        else:\n            label_points = [0.0] * len(label_names)\n            label_points[0] = 1.0\n\n        self.weights = label_points\n        self.system_message = system_message\n        self.batch_size = batch_size\n        self.disable_tqdm = disable_tqdm\n        self.category_key = category_key\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]] | None = None,\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if task_inputs_list is None:\n            task_inputs_list = [{} for _ in lm_outputs]\n        if references_list is None:\n            references_list = [[] for _ in lm_outputs]\n\n        evaluator_input_list = prepare_chat_input_for_evaluator(\n            lm_outputs, references_list, task_inputs_list, self.prompt_template, self.system_message\n        )\n\n        evaluator_output_list: list[str] = generate_evaluations(\n            evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating ChatLLM score\"\n        )\n\n        evaluator_label_list: list[str] = []\n        for evaluator_output in evaluator_output_list:\n            evaluator_label = parse_label_from_evaluator_output(\n                evaluator_output,\n                label_names=self.label_names,\n            )\n            if evaluator_label is None:\n                logger.warning(f\"Failed to parse label from evaluator output: {evaluator_output}\")\n            evaluator_label_list.append(evaluator_label)\n\n        label2point = dict(zip(self.label_names, self.weights))\n        evaluator_score_list: list[float | None] = [label2point.get(label) for label in evaluator_label_list]\n\n        summary = summarize_evaluator_labels(\n            evaluator_label_list,\n            task_inputs_list,\n            self.label_names,\n            self.weights,\n            self.category_key,\n        )\n\n        return MetricResult(\n            summary,\n            instance_details=[\n                {\n                    \"llm_label\": eval_label,\n                    \"llm_score\": eval_score,\n                    \"llm_label_input\": eval_in,\n                    \"llm_label_output\": eval_out,\n                }\n                for eval_label, eval_score, eval_in, eval_out in zip(\n                    evaluator_label_list,\n                    evaluator_score_list,\n                    evaluator_input_list,\n                    evaluator_output_list,\n                )\n            ],\n        )\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.label_names","title":"label_names  <code>instance-attribute</code>","text":"<pre><code>label_names = [escape(label) for label in label_names]\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.weights","title":"weights  <code>instance-attribute</code>","text":"<pre><code>weights = label_points\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.system_message","title":"system_message  <code>instance-attribute</code>","text":"<pre><code>system_message = system_message\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.disable_tqdm","title":"disable_tqdm  <code>instance-attribute</code>","text":"<pre><code>disable_tqdm = disable_tqdm\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.__init__","title":"__init__","text":"<pre><code>__init__(language_model: LanguageModel, prompt_template: PromptTemplate, label_names: list[str], label_points: list[float | int] | None = None, system_message: str | PromptTemplate | None = None, batch_size: int = 4, disable_tqdm: bool = False, category_key: str | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    label_names: list[str],\n    label_points: list[float | int] | None = None,\n    system_message: str | PromptTemplate | None = None,\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    category_key: str | None = None,\n) -&gt; None:\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.label_names = [re.escape(label) for label in label_names]\n\n    if label_points:\n        if len(self.label_names) != len(label_points):\n            msg = \"The lengths of label_names and weights do not match.\"\n            raise ValueError(msg)\n        label_points: list[float] = list(map(float, label_points))\n    else:\n        label_points = [0.0] * len(label_names)\n        label_points[0] = 1.0\n\n    self.weights = label_points\n    self.system_message = system_message\n    self.batch_size = batch_size\n    self.disable_tqdm = disable_tqdm\n    self.category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]] | None = None, task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]] | None = None,\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if task_inputs_list is None:\n        task_inputs_list = [{} for _ in lm_outputs]\n    if references_list is None:\n        references_list = [[] for _ in lm_outputs]\n\n    evaluator_input_list = prepare_chat_input_for_evaluator(\n        lm_outputs, references_list, task_inputs_list, self.prompt_template, self.system_message\n    )\n\n    evaluator_output_list: list[str] = generate_evaluations(\n        evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating ChatLLM score\"\n    )\n\n    evaluator_label_list: list[str] = []\n    for evaluator_output in evaluator_output_list:\n        evaluator_label = parse_label_from_evaluator_output(\n            evaluator_output,\n            label_names=self.label_names,\n        )\n        if evaluator_label is None:\n            logger.warning(f\"Failed to parse label from evaluator output: {evaluator_output}\")\n        evaluator_label_list.append(evaluator_label)\n\n    label2point = dict(zip(self.label_names, self.weights))\n    evaluator_score_list: list[float | None] = [label2point.get(label) for label in evaluator_label_list]\n\n    summary = summarize_evaluator_labels(\n        evaluator_label_list,\n        task_inputs_list,\n        self.label_names,\n        self.weights,\n        self.category_key,\n    )\n\n    return MetricResult(\n        summary,\n        instance_details=[\n            {\n                \"llm_label\": eval_label,\n                \"llm_score\": eval_score,\n                \"llm_label_input\": eval_in,\n                \"llm_label_output\": eval_out,\n            }\n            for eval_label, eval_score, eval_in, eval_out in zip(\n                evaluator_label_list,\n                evaluator_score_list,\n                evaluator_input_list,\n                evaluator_output_list,\n            )\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return (\n        f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel","title":"LLMLabel","text":"<p>Let LanguageModel to evaluate the output of another LanguageModel.</p> <p>You can specify the evaluation criteria in <code>PromptTemplate</code>. The last label value found in the output of the evaluator is used to compute the evaluation score. You can assign a score to each label. The final output is the average score and the distribution of the labels.</p> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>LanguageModel</code>)           \u2013            <p>An instance of <code>LanguageModel</code> to evaluate the output of the model.</p> </li> <li> <code>prompt_template</code>               (<code>PromptTemplate</code>)           \u2013            <p>An instance of <code>PromptTemplate</code> to embed the input for the evaluator.</p> </li> <li> <code>label_names</code>               (<code>list[str]</code>)           \u2013            <p>A list of valid label names.</p> </li> <li> <code>label_points</code>               (<code>list[float | int] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of points for each label specified in label_names.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The batch size for the evaluator.</p> </li> <li> <code>disable_tqdm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to disable the progress bar.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in task inputs.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import OpenAIChatAPI, Jinja2PromptTemplate, LLMLabel\n&gt;&gt;&gt; language_model = OpenAIChatAPI(model=\"gpt-3.5-turbo\")\n&gt;&gt;&gt; template = \"Evaluate the quality of this text on a scale of Good/Bad.\\n`{{ lm_output }}`\\nPut the label at the end like [[Good]].\"\n&gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n&gt;&gt;&gt; label_names = [\"Good\", \"Bad\"]\n&gt;&gt;&gt; label_points = [1.0, 0.0]\n&gt;&gt;&gt; llm_label = LLMLabel(language_model, prompt_template, label_names, label_points)\n&gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good mrrrning!\"]\n&gt;&gt;&gt; result = llm_label.evaluate(lm_outputs)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'llm_score': 0.5, 'llm_label_distribution': {'Good': 0.5, 'Bad': 0.5}, 'num_failed_score_parses': 0},\n    instance_details=[\n        {\n            'llm_label': 'Good',\n            'llm_score': 1.0,\n            'llm_label_input': 'Evaluate the quality of this text...',\n            'llm_label_output': 'This text is natural, ... [[Good]]'\n        },\n        {\n            'llm_label': 'Bad',\n            'llm_score': 0.0,\n            'llm_label_input': 'Evaluate the quality of this text on a scale of Good/Bad.\\n`Good mrrrning!`\\nPut the label at the end like [[Good]].',\n            'llm_label_output': 'This text contains a spelling error, ... [[Bad]]'\n        }\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>class LLMLabel(Metric):\n    \"\"\"Let LanguageModel to evaluate the output of another LanguageModel.\n\n    You can specify the evaluation criteria in `PromptTemplate`.\n    The last label value found in the output of the evaluator is used to compute the evaluation score.\n    You can assign a score to each label.\n    The final output is the average score and the distribution of the labels.\n\n    Args:\n        language_model: An instance of `LanguageModel` to evaluate the output of the model.\n        prompt_template: An instance of `PromptTemplate` to embed the input for the evaluator.\n        label_names: A list of valid label names.\n        label_points: A list of points for each label specified in label_names.\n        batch_size: The batch size for the evaluator.\n        disable_tqdm: Whether to disable the progress bar.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in task inputs.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import OpenAIChatAPI, Jinja2PromptTemplate, LLMLabel\n        &gt;&gt;&gt; language_model = OpenAIChatAPI(model=\"gpt-3.5-turbo\")\n        &gt;&gt;&gt; template = \"Evaluate the quality of this text on a scale of Good/Bad.\\\\n`{{ lm_output }}`\\\\nPut the label at the end like [[Good]].\"\n        &gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n        &gt;&gt;&gt; label_names = [\"Good\", \"Bad\"]\n        &gt;&gt;&gt; label_points = [1.0, 0.0]\n        &gt;&gt;&gt; llm_label = LLMLabel(language_model, prompt_template, label_names, label_points)\n        &gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good mrrrning!\"]\n        &gt;&gt;&gt; result = llm_label.evaluate(lm_outputs)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'llm_score': 0.5, 'llm_label_distribution': {'Good': 0.5, 'Bad': 0.5}, 'num_failed_score_parses': 0},\n            instance_details=[\n                {\n                    'llm_label': 'Good',\n                    'llm_score': 1.0,\n                    'llm_label_input': 'Evaluate the quality of this text...',\n                    'llm_label_output': 'This text is natural, ... [[Good]]'\n                },\n                {\n                    'llm_label': 'Bad',\n                    'llm_score': 0.0,\n                    'llm_label_input': 'Evaluate the quality of this text on a scale of Good/Bad.\\\\n`Good mrrrning!`\\\\nPut the label at the end like [[Good]].',\n                    'llm_label_output': 'This text contains a spelling error, ... [[Bad]]'\n                }\n            ]\n        )\n    \"\"\"  # noqa: E501\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        label_names: list[str],\n        label_points: list[float | int] | None = None,\n        batch_size: int = 4,\n        disable_tqdm: bool = False,\n        valid_score_range: tuple[int, int] | None = None,\n        category_key: str | None = None,\n    ) -&gt; None:\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.label_names = [re.escape(label) for label in label_names]\n\n        if label_points:\n            if len(self.label_names) != len(label_points):\n                msg = \"The lengths of label_names and weights do not match.\"\n                raise ValueError(msg)\n            label_points: list[float] = list(map(float, label_points))\n        else:\n            label_points = [0.0] * len(label_names)\n            label_points[0] = 1.0\n\n        self.weights = label_points\n        self.batch_size = batch_size\n        self.disable_tqdm = disable_tqdm\n        self.valid_score_range = valid_score_range\n        self.category_key = category_key\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]] | None = None,\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if task_inputs_list is None:\n            task_inputs_list = [{} for _ in lm_outputs]\n        if references_list is None:\n            references_list = [[] for _ in lm_outputs]\n\n        evaluator_input_list: list[str] = prepare_text_input_for_evaluator(\n            lm_outputs, references_list, task_inputs_list, self.prompt_template\n        )\n        evaluator_output_list: list[str] = generate_evaluations(\n            evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating LLM score\"\n        )\n\n        evaluator_label_list: list[int | None] = []\n        for evaluator_output in evaluator_output_list:\n            evaluator_label = parse_label_from_evaluator_output(\n                evaluator_output,\n                label_names=self.label_names,\n            )\n            if evaluator_label is None:\n                logger.warning(f\"Failed to parse label from evaluator output: {evaluator_output}\")\n            evaluator_label_list.append(evaluator_label)\n\n        label2point = dict(zip(self.label_names, self.weights))\n        evaluator_score_list: list[float | None] = [label2point.get(label) for label in evaluator_label_list]\n\n        summary = summarize_evaluator_labels(\n            evaluator_label_list,\n            task_inputs_list,\n            self.label_names,\n            self.weights,\n            self.category_key,\n        )\n\n        return MetricResult(\n            summary,\n            instance_details=[\n                {\n                    \"llm_label\": eval_label,\n                    \"llm_score\": eval_score,\n                    \"llm_label_input\": eval_in,\n                    \"llm_label_output\": eval_out,\n                }\n                for eval_label, eval_score, eval_in, eval_out in zip(\n                    evaluator_label_list,\n                    evaluator_score_list,\n                    evaluator_input_list,\n                    evaluator_output_list,\n                )\n            ],\n        )\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.label_names","title":"label_names  <code>instance-attribute</code>","text":"<pre><code>label_names = [escape(label) for label in label_names]\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.weights","title":"weights  <code>instance-attribute</code>","text":"<pre><code>weights = label_points\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.disable_tqdm","title":"disable_tqdm  <code>instance-attribute</code>","text":"<pre><code>disable_tqdm = disable_tqdm\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.valid_score_range","title":"valid_score_range  <code>instance-attribute</code>","text":"<pre><code>valid_score_range = valid_score_range\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.__init__","title":"__init__","text":"<pre><code>__init__(language_model: LanguageModel, prompt_template: PromptTemplate, label_names: list[str], label_points: list[float | int] | None = None, batch_size: int = 4, disable_tqdm: bool = False, valid_score_range: tuple[int, int] | None = None, category_key: str | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    label_names: list[str],\n    label_points: list[float | int] | None = None,\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    valid_score_range: tuple[int, int] | None = None,\n    category_key: str | None = None,\n) -&gt; None:\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.label_names = [re.escape(label) for label in label_names]\n\n    if label_points:\n        if len(self.label_names) != len(label_points):\n            msg = \"The lengths of label_names and weights do not match.\"\n            raise ValueError(msg)\n        label_points: list[float] = list(map(float, label_points))\n    else:\n        label_points = [0.0] * len(label_names)\n        label_points[0] = 1.0\n\n    self.weights = label_points\n    self.batch_size = batch_size\n    self.disable_tqdm = disable_tqdm\n    self.valid_score_range = valid_score_range\n    self.category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]] | None = None, task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]] | None = None,\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if task_inputs_list is None:\n        task_inputs_list = [{} for _ in lm_outputs]\n    if references_list is None:\n        references_list = [[] for _ in lm_outputs]\n\n    evaluator_input_list: list[str] = prepare_text_input_for_evaluator(\n        lm_outputs, references_list, task_inputs_list, self.prompt_template\n    )\n    evaluator_output_list: list[str] = generate_evaluations(\n        evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating LLM score\"\n    )\n\n    evaluator_label_list: list[int | None] = []\n    for evaluator_output in evaluator_output_list:\n        evaluator_label = parse_label_from_evaluator_output(\n            evaluator_output,\n            label_names=self.label_names,\n        )\n        if evaluator_label is None:\n            logger.warning(f\"Failed to parse label from evaluator output: {evaluator_output}\")\n        evaluator_label_list.append(evaluator_label)\n\n    label2point = dict(zip(self.label_names, self.weights))\n    evaluator_score_list: list[float | None] = [label2point.get(label) for label in evaluator_label_list]\n\n    summary = summarize_evaluator_labels(\n        evaluator_label_list,\n        task_inputs_list,\n        self.label_names,\n        self.weights,\n        self.category_key,\n    )\n\n    return MetricResult(\n        summary,\n        instance_details=[\n            {\n                \"llm_label\": eval_label,\n                \"llm_score\": eval_score,\n                \"llm_label_input\": eval_in,\n                \"llm_label_output\": eval_out,\n            }\n            for eval_label, eval_score, eval_in, eval_out in zip(\n                evaluator_label_list,\n                evaluator_score_list,\n                evaluator_input_list,\n                evaluator_output_list,\n            )\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return (\n        f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore","title":"ChatLLMScore","text":"<p>A metric that evaluates the output of <code>LanguageModel.batch_generate_chat_response</code>.</p> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>LanguageModel</code>)           \u2013            <p>An instance of <code>LanguageModel</code> to evaluate the output of the model.</p> </li> <li> <code>prompt_template</code>               (<code>PromptTemplate</code>)           \u2013            <p>An instance of <code>PromptTemplate</code> to embed the input for the evaluator.</p> </li> <li> <code>system_message</code>               (<code>str | PromptTemplate | None</code>, default:                   <code>None</code> )           \u2013            <p>A system message to be prepended to the input for the evaluator.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The batch size for the evaluator.</p> </li> <li> <code>disable_tqdm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to disable the progress bar.</p> </li> <li> <code>valid_score_range</code>               (<code>tuple[int, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>A tuple of two integers representing the valid score range. If the parsed score is out of the range, it will be ignored.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in task inputs.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import ChatLLMScore, OpenAIChatAPI, Jinja2PromptTemplate\n&gt;&gt;&gt; language_model = OpenAIChatAPI(model_name=\"gpt-3.5-turbo\")\n&gt;&gt;&gt; template = \"Evaluate the quality of this text.\\n`{{ lm_output }}`\\nPut the score at the end like [[5]].\"\n&gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n&gt;&gt;&gt; system_message = \"This is the system message.\"\n&gt;&gt;&gt; llm_score = ChatLLMScore(language_model, prompt_template, system_message)\n&gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n&gt;&gt;&gt; result = llm_score.evaluate(lm_outputs)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'llm_score': 3.0, 'num_failed_score_parses': 0},\n    instance_details=[\n        {\n            'llm_score': 2,\n            'llm_score_input': [{'role': 'user', 'content': 'Evaluate the quality of this text...'}],\n            'llm_score_output': 'This text is very simple,... Therefore, its quality is average. [[2]]'},\n        {\n            'llm_score': 4,\n            'llm_score_input': [{'role': 'user', 'content': 'Evaluate the quality of this text...'}],\n            'llm_score_output': '... Overall, the quality of the text is good but basic. [[4]]'}\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>class ChatLLMScore(Metric):\n    \"\"\"\n    A metric that evaluates the output of `LanguageModel.batch_generate_chat_response`.\n\n    Args:\n        language_model: An instance of `LanguageModel` to evaluate the output of the model.\n        prompt_template: An instance of `PromptTemplate` to embed the input for the evaluator.\n        system_message: A system message to be prepended to the input for the evaluator.\n        batch_size: The batch size for the evaluator.\n        disable_tqdm: Whether to disable the progress bar.\n        valid_score_range: A tuple of two integers representing the valid score range.\n            If the parsed score is out of the range, it will be ignored.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in task inputs.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import ChatLLMScore, OpenAIChatAPI, Jinja2PromptTemplate\n        &gt;&gt;&gt; language_model = OpenAIChatAPI(model_name=\"gpt-3.5-turbo\")\n        &gt;&gt;&gt; template = \"Evaluate the quality of this text.\\\\n`{{ lm_output }}`\\\\nPut the score at the end like [[5]].\"\n        &gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n        &gt;&gt;&gt; system_message = \"This is the system message.\"\n        &gt;&gt;&gt; llm_score = ChatLLMScore(language_model, prompt_template, system_message)\n        &gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n        &gt;&gt;&gt; result = llm_score.evaluate(lm_outputs)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'llm_score': 3.0, 'num_failed_score_parses': 0},\n            instance_details=[\n                {\n                    'llm_score': 2,\n                    'llm_score_input': [{'role': 'user', 'content': 'Evaluate the quality of this text...'}],\n                    'llm_score_output': 'This text is very simple,... Therefore, its quality is average. [[2]]'},\n                {\n                    'llm_score': 4,\n                    'llm_score_input': [{'role': 'user', 'content': 'Evaluate the quality of this text...'}],\n                    'llm_score_output': '... Overall, the quality of the text is good but basic. [[4]]'}\n            ]\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        system_message: str | PromptTemplate | None = None,\n        batch_size: int = 4,\n        disable_tqdm: bool = False,\n        valid_score_range: tuple[int, int] | None = None,\n        category_key: str | None = None,\n    ) -&gt; None:\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.system_message = system_message\n        self.batch_size = batch_size\n        self.disable_tqdm = disable_tqdm\n        self.valid_score_range = valid_score_range\n        self.category_key = category_key\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]] | None = None,\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if task_inputs_list is None:\n            task_inputs_list = [{} for _ in lm_outputs]\n        if references_list is None:\n            references_list = [[] for _ in lm_outputs]\n\n        evaluator_input_list = prepare_chat_input_for_evaluator(\n            lm_outputs, references_list, task_inputs_list, self.prompt_template, self.system_message\n        )\n        evaluator_output_list: list[str] = generate_evaluations(\n            evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating ChatLLM score\"\n        )\n\n        evaluator_score_list: list[int] = []\n        for evaluator_output in evaluator_output_list:\n            evaluator_score = parse_score_from_evaluator_output(\n                evaluator_output,\n                valid_score_range=self.valid_score_range,\n            )\n            if evaluator_score is None:\n                logger.warning(f\"Failed to parse score from evaluator output: {evaluator_output}\")\n            evaluator_score_list.append(evaluator_score)\n\n        summary = summarize_evaluator_scores(\n            evaluator_score_list,\n            task_inputs_list,\n            self.category_key,\n        )\n\n        return MetricResult(\n            summary,\n            instance_details=[\n                {\"llm_score\": eval_score, \"llm_score_input\": eval_in, \"llm_score_output\": eval_out}\n                for eval_score, eval_in, eval_out in zip(\n                    evaluator_score_list,\n                    evaluator_input_list,\n                    evaluator_output_list,\n                )\n            ],\n        )\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.system_message","title":"system_message  <code>instance-attribute</code>","text":"<pre><code>system_message = system_message\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.disable_tqdm","title":"disable_tqdm  <code>instance-attribute</code>","text":"<pre><code>disable_tqdm = disable_tqdm\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.valid_score_range","title":"valid_score_range  <code>instance-attribute</code>","text":"<pre><code>valid_score_range = valid_score_range\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.__init__","title":"__init__","text":"<pre><code>__init__(language_model: LanguageModel, prompt_template: PromptTemplate, system_message: str | PromptTemplate | None = None, batch_size: int = 4, disable_tqdm: bool = False, valid_score_range: tuple[int, int] | None = None, category_key: str | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    system_message: str | PromptTemplate | None = None,\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    valid_score_range: tuple[int, int] | None = None,\n    category_key: str | None = None,\n) -&gt; None:\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.system_message = system_message\n    self.batch_size = batch_size\n    self.disable_tqdm = disable_tqdm\n    self.valid_score_range = valid_score_range\n    self.category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]] | None = None, task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]] | None = None,\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if task_inputs_list is None:\n        task_inputs_list = [{} for _ in lm_outputs]\n    if references_list is None:\n        references_list = [[] for _ in lm_outputs]\n\n    evaluator_input_list = prepare_chat_input_for_evaluator(\n        lm_outputs, references_list, task_inputs_list, self.prompt_template, self.system_message\n    )\n    evaluator_output_list: list[str] = generate_evaluations(\n        evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating ChatLLM score\"\n    )\n\n    evaluator_score_list: list[int] = []\n    for evaluator_output in evaluator_output_list:\n        evaluator_score = parse_score_from_evaluator_output(\n            evaluator_output,\n            valid_score_range=self.valid_score_range,\n        )\n        if evaluator_score is None:\n            logger.warning(f\"Failed to parse score from evaluator output: {evaluator_output}\")\n        evaluator_score_list.append(evaluator_score)\n\n    summary = summarize_evaluator_scores(\n        evaluator_score_list,\n        task_inputs_list,\n        self.category_key,\n    )\n\n    return MetricResult(\n        summary,\n        instance_details=[\n            {\"llm_score\": eval_score, \"llm_score_input\": eval_in, \"llm_score_output\": eval_out}\n            for eval_score, eval_in, eval_out in zip(\n                evaluator_score_list,\n                evaluator_input_list,\n                evaluator_output_list,\n            )\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return (\n        f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore","title":"LLMScore","text":"<p>Let LanguageModel to evaluate the output of another LanguageModel.</p> <p>You can specify the evaluation criteria in <code>PromptTemplate</code>. The last integer value in the output of the evaluator is used as the evaluation score.</p> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>LanguageModel</code>)           \u2013            <p>An instance of <code>LanguageModel</code> to evaluate the output of the model.</p> </li> <li> <code>prompt_template</code>               (<code>PromptTemplate</code>)           \u2013            <p>An instance of <code>PromptTemplate</code> to embed the input for the evaluator.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The batch size for the evaluator.</p> </li> <li> <code>disable_tqdm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to disable the progress bar.</p> </li> <li> <code>valid_score_range</code>               (<code>tuple[int, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>A tuple of two integers representing the valid score range. If the parsed score is out of the range, it will be ignored.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in task inputs.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import LLMScore, OpenAIChatAPI, Jinja2PromptTemplate\n&gt;&gt;&gt; language_model = OpenAIChatAPI(model_name=\"gpt-3.5-turbo\")\n&gt;&gt;&gt; template = \"Evaluate the quality of this text.\\n`{{ lm_output }}`\\nPut the score at the end like [[5]].\"\n&gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n&gt;&gt;&gt; llm_score = LLMScore(language_model, prompt_template)\n&gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n&gt;&gt;&gt; result = llm_score.evaluate(lm_outputs)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'llm_score': 3.0, 'num_failed_score_parses': 0},\n    instance_details=[\n        {\n            'llm_score': 2,\n            'llm_score_input': 'Evaluate the quality of this text...',\n            'llm_score_output': 'This text is very simple,... Therefore, its quality is average. [[2]]'},\n        {\n            'llm_score': 4,\n            'llm_score_input': 'Evaluate the quality of this text...',\n            'llm_score_output': '... Overall, the quality of the text is good but basic. [[4]]'}\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>class LLMScore(Metric):\n    \"\"\"Let LanguageModel to evaluate the output of another LanguageModel.\n\n    You can specify the evaluation criteria in `PromptTemplate`.\n    The last integer value in the output of the evaluator is used as the evaluation score.\n\n    Args:\n        language_model: An instance of `LanguageModel` to evaluate the output of the model.\n        prompt_template: An instance of `PromptTemplate` to embed the input for the evaluator.\n        batch_size: The batch size for the evaluator.\n        disable_tqdm: Whether to disable the progress bar.\n        valid_score_range: A tuple of two integers representing the valid score range.\n            If the parsed score is out of the range, it will be ignored.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in task inputs.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import LLMScore, OpenAIChatAPI, Jinja2PromptTemplate\n        &gt;&gt;&gt; language_model = OpenAIChatAPI(model_name=\"gpt-3.5-turbo\")\n        &gt;&gt;&gt; template = \"Evaluate the quality of this text.\\\\n`{{ lm_output }}`\\\\nPut the score at the end like [[5]].\"\n        &gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n        &gt;&gt;&gt; llm_score = LLMScore(language_model, prompt_template)\n        &gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n        &gt;&gt;&gt; result = llm_score.evaluate(lm_outputs)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'llm_score': 3.0, 'num_failed_score_parses': 0},\n            instance_details=[\n                {\n                    'llm_score': 2,\n                    'llm_score_input': 'Evaluate the quality of this text...',\n                    'llm_score_output': 'This text is very simple,... Therefore, its quality is average. [[2]]'},\n                {\n                    'llm_score': 4,\n                    'llm_score_input': 'Evaluate the quality of this text...',\n                    'llm_score_output': '... Overall, the quality of the text is good but basic. [[4]]'}\n            ]\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        batch_size: int = 4,\n        disable_tqdm: bool = False,\n        valid_score_range: tuple[int, int] | None = None,\n        category_key: str | None = None,\n    ) -&gt; None:\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.batch_size = batch_size\n        self.disable_tqdm = disable_tqdm\n        self.valid_score_range = valid_score_range\n        self.category_key = category_key\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]] | None = None,\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if task_inputs_list is None:\n            task_inputs_list = [{} for _ in lm_outputs]\n        if references_list is None:\n            references_list = [[] for _ in lm_outputs]\n\n        evaluator_input_list: list[str] = prepare_text_input_for_evaluator(\n            lm_outputs, references_list, task_inputs_list, self.prompt_template\n        )\n        evaluator_output_list: list[str] = generate_evaluations(\n            evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating LLM score\"\n        )\n\n        evaluator_score_list: list[int | None] = []\n        for evaluator_output in evaluator_output_list:\n            evaluator_score = parse_score_from_evaluator_output(\n                evaluator_output,\n                valid_score_range=self.valid_score_range,\n            )\n            if evaluator_score is None:\n                logger.warning(f\"Failed to parse score from evaluator output: {evaluator_output}\")\n            evaluator_score_list.append(evaluator_score)\n\n        summary = summarize_evaluator_scores(\n            evaluator_score_list,\n            task_inputs_list,\n            self.category_key,\n        )\n\n        return MetricResult(\n            summary,\n            instance_details=[\n                {\"llm_score\": eval_score, \"llm_score_input\": eval_in, \"llm_score_output\": eval_out}\n                for eval_score, eval_in, eval_out in zip(\n                    evaluator_score_list,\n                    evaluator_input_list,\n                    evaluator_output_list,\n                )\n            ],\n        )\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.disable_tqdm","title":"disable_tqdm  <code>instance-attribute</code>","text":"<pre><code>disable_tqdm = disable_tqdm\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.valid_score_range","title":"valid_score_range  <code>instance-attribute</code>","text":"<pre><code>valid_score_range = valid_score_range\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.__init__","title":"__init__","text":"<pre><code>__init__(language_model: LanguageModel, prompt_template: PromptTemplate, batch_size: int = 4, disable_tqdm: bool = False, valid_score_range: tuple[int, int] | None = None, category_key: str | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    valid_score_range: tuple[int, int] | None = None,\n    category_key: str | None = None,\n) -&gt; None:\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.batch_size = batch_size\n    self.disable_tqdm = disable_tqdm\n    self.valid_score_range = valid_score_range\n    self.category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]] | None = None, task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]] | None = None,\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if task_inputs_list is None:\n        task_inputs_list = [{} for _ in lm_outputs]\n    if references_list is None:\n        references_list = [[] for _ in lm_outputs]\n\n    evaluator_input_list: list[str] = prepare_text_input_for_evaluator(\n        lm_outputs, references_list, task_inputs_list, self.prompt_template\n    )\n    evaluator_output_list: list[str] = generate_evaluations(\n        evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating LLM score\"\n    )\n\n    evaluator_score_list: list[int | None] = []\n    for evaluator_output in evaluator_output_list:\n        evaluator_score = parse_score_from_evaluator_output(\n            evaluator_output,\n            valid_score_range=self.valid_score_range,\n        )\n        if evaluator_score is None:\n            logger.warning(f\"Failed to parse score from evaluator output: {evaluator_output}\")\n        evaluator_score_list.append(evaluator_score)\n\n    summary = summarize_evaluator_scores(\n        evaluator_score_list,\n        task_inputs_list,\n        self.category_key,\n    )\n\n    return MetricResult(\n        summary,\n        instance_details=[\n            {\"llm_score\": eval_score, \"llm_score_input\": eval_in, \"llm_score_output\": eval_out}\n            for eval_score, eval_in, eval_out in zip(\n                evaluator_score_list,\n                evaluator_input_list,\n                evaluator_output_list,\n            )\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return (\n        f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.output_length_stats.OutputLengthStats","title":"OutputLengthStats","text":"<p>Compute statistics on the length of the outputs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import OutputLengthStats\n&gt;&gt;&gt; output_length_stats = OutputLengthStats()\n&gt;&gt;&gt; lm_outputs = [\"123456\", \"123456789\"]\n&gt;&gt;&gt; result = output_length_stats.evaluate(lm_outputs)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'avg_output_length': 7.5, 'max_output_length': 9, 'min_output_length': 6},\n    instance_details=[{'output_length': 6}, {'output_length': 9}]\n)\n</code></pre> Source code in <code>flexeval/core/metric/output_length_stats.py</code> <pre><code>class OutputLengthStats(Metric):\n    \"\"\"\n    Compute statistics on the length of the outputs.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import OutputLengthStats\n        &gt;&gt;&gt; output_length_stats = OutputLengthStats()\n        &gt;&gt;&gt; lm_outputs = [\"123456\", \"123456789\"]\n        &gt;&gt;&gt; result = output_length_stats.evaluate(lm_outputs)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'avg_output_length': 7.5, 'max_output_length': 9, 'min_output_length': 6},\n            instance_details=[{'output_length': 6}, {'output_length': 9}]\n        )\n    \"\"\"\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]] | None = None,\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        output_length_list = [len(output) for output in lm_outputs]\n        return MetricResult(\n            {\n                \"avg_output_length\": sum(output_length_list) / len(output_length_list),\n                \"max_output_length\": max(output_length_list),\n                \"min_output_length\": min(output_length_list),\n            },\n            instance_details=[{\"output_length\": s} for s in output_length_list],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.output_length_stats.OutputLengthStats.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]] | None = None, task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/output_length_stats.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]] | None = None,\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    output_length_list = [len(output) for output in lm_outputs]\n    return MetricResult(\n        {\n            \"avg_output_length\": sum(output_length_list) / len(output_length_list),\n            \"max_output_length\": max(output_length_list),\n            \"min_output_length\": min(output_length_list),\n        },\n        instance_details=[{\"output_length\": s} for s in output_length_list],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.perspective_api.PerspectiveAPI","title":"PerspectiveAPI","text":"<p>A metric that evaluates text outputs using the Perspective API. Please set <code>PERSPECTIVE_API_KEY</code> in the environment variable.</p> <p>Parameters:</p> <ul> <li> <code>languages</code>               (<code>list[str]</code>)           \u2013            <p>A list of languages to analyze.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import PerspectiveAPI\n&gt;&gt;&gt; perspective_api = PerspectiveAPI(languages=[\"en\"])\n&gt;&gt;&gt; lm_outputs = [\"I love you\", \"I hate you\"]\n&gt;&gt;&gt; result = perspective_api.evaluate(lm_outputs)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'TOXICITY': 0.35407552, ..., 'THREAT': 0.0265799825},\n    instance_details=[\n        {'TOXICITY': 0.02543884, ..., 'THREAT': 0.009204263},\n        {'TOXICITY': 0.6827122, ..., 'THREAT': 0.043955702}\n        ]\n    )\n</code></pre> Source code in <code>flexeval/core/metric/perspective_api.py</code> <pre><code>class PerspectiveAPI(Metric):\n    \"\"\"A metric that evaluates text outputs using the Perspective API.\n    Please set `PERSPECTIVE_API_KEY` in the environment variable.\n\n    Args:\n        languages: A list of languages to analyze.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import PerspectiveAPI\n        &gt;&gt;&gt; perspective_api = PerspectiveAPI(languages=[\"en\"])\n        &gt;&gt;&gt; lm_outputs = [\"I love you\", \"I hate you\"]\n        &gt;&gt;&gt; result = perspective_api.evaluate(lm_outputs)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'TOXICITY': 0.35407552, ..., 'THREAT': 0.0265799825},\n            instance_details=[\n                {'TOXICITY': 0.02543884, ..., 'THREAT': 0.009204263},\n                {'TOXICITY': 0.6827122, ..., 'THREAT': 0.043955702}\n                ]\n            )\n    \"\"\"\n\n    def __init__(self, languages: list[str]) -&gt; None:\n        self.client = discovery.build(\n            \"commentanalyzer\",\n            \"v1alpha1\",\n            developerKey=PERSPECTIVE_API_KEY,\n            discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n            static_discovery=False,\n        )\n        self.languages = languages\n        self.attributes = [\"TOXICITY\", \"SEVERE_TOXICITY\", \"IDENTITY_ATTACK\", \"INSULT\", \"PROFANITY\", \"THREAT\"]\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]] | None = None,\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        instance_details = []\n        for lm_output in lm_outputs:\n            if lm_output == \"\":\n                instance_details.append({att: 0.0 for att in self.attributes})\n                continue\n            analyze_request = {\n                \"comment\": {\"text\": lm_output},\n                \"languages\": self.languages,\n                \"requestedAttributes\": {att: {} for att in self.attributes},\n            }\n            response = retry_on_error(perspectiveapi_call=self.client.comments().analyze(body=analyze_request).execute)\n            instance_details.append(\n                {att: response[\"attributeScores\"][att][\"summaryScore\"][\"value\"] for att in self.attributes},\n            )\n        scores_for_attribute = {att: [] for att in self.attributes}\n        for instance in instance_details:\n            for att in self.attributes:\n                scores_for_attribute[att].append(instance[att])\n        average_scores = {att: np.mean(scores_for_attribute[att]) for att in self.attributes}\n        return MetricResult(average_scores, instance_details=instance_details)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.perspective_api.PerspectiveAPI.client","title":"client  <code>instance-attribute</code>","text":"<pre><code>client = build('commentanalyzer', 'v1alpha1', developerKey=PERSPECTIVE_API_KEY, discoveryServiceUrl='https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1', static_discovery=False)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.perspective_api.PerspectiveAPI.languages","title":"languages  <code>instance-attribute</code>","text":"<pre><code>languages = languages\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.perspective_api.PerspectiveAPI.attributes","title":"attributes  <code>instance-attribute</code>","text":"<pre><code>attributes = ['TOXICITY', 'SEVERE_TOXICITY', 'IDENTITY_ATTACK', 'INSULT', 'PROFANITY', 'THREAT']\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.perspective_api.PerspectiveAPI.__init__","title":"__init__","text":"<pre><code>__init__(languages: list[str]) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/perspective_api.py</code> <pre><code>def __init__(self, languages: list[str]) -&gt; None:\n    self.client = discovery.build(\n        \"commentanalyzer\",\n        \"v1alpha1\",\n        developerKey=PERSPECTIVE_API_KEY,\n        discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n        static_discovery=False,\n    )\n    self.languages = languages\n    self.attributes = [\"TOXICITY\", \"SEVERE_TOXICITY\", \"IDENTITY_ATTACK\", \"INSULT\", \"PROFANITY\", \"THREAT\"]\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.perspective_api.PerspectiveAPI.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]] | None = None, task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/perspective_api.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]] | None = None,\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    instance_details = []\n    for lm_output in lm_outputs:\n        if lm_output == \"\":\n            instance_details.append({att: 0.0 for att in self.attributes})\n            continue\n        analyze_request = {\n            \"comment\": {\"text\": lm_output},\n            \"languages\": self.languages,\n            \"requestedAttributes\": {att: {} for att in self.attributes},\n        }\n        response = retry_on_error(perspectiveapi_call=self.client.comments().analyze(body=analyze_request).execute)\n        instance_details.append(\n            {att: response[\"attributeScores\"][att][\"summaryScore\"][\"value\"] for att in self.attributes},\n        )\n    scores_for_attribute = {att: [] for att in self.attributes}\n    for instance in instance_details:\n        for att in self.attributes:\n            scores_for_attribute[att].append(instance[att])\n    average_scores = {att: np.mean(scores_for_attribute[att]) for att in self.attributes}\n    return MetricResult(average_scores, instance_details=instance_details)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.rouge.ROUGE","title":"ROUGE","text":"<p>An implementation of ROUGE.</p> <p>The calculation is based on the rouge library.</p> <p>Parameters:</p> <ul> <li> <code>tokenizer</code>               (<code>Tokenizer</code>)           \u2013            <p>An instance of <code>Tokenizer</code> to tokenize the input and output strings.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import ROUGE\n&gt;&gt;&gt; from flexeval import WhitespaceTokenizer\n&gt;&gt;&gt; tokenizer = WhitespaceTokenizer()\n&gt;&gt;&gt; rouge = ROUGE(tokenizer)\n&gt;&gt;&gt; lm_outputs = [\"I am a student .\", \"I am a teacher .\"]\n&gt;&gt;&gt; references_list = [[\"I am a student .\", \"I am a learner .\"], [\"I am a teacher .\"]]\n&gt;&gt;&gt; result = rouge.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'rouge1': 0.999999995, 'rouge2': 0.999999995, 'rougeL': 0.999999995},\n    instance_details=[\n        {'rouge1': 0.999999995, 'rouge2': 0.999999995, 'rougeL': 0.999999995},\n        {'rouge1': 0.999999995, 'rouge2': 0.999999995, 'rougeL': 0.999999995}\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/rouge.py</code> <pre><code>class ROUGE(Metric):\n    \"\"\"An implementation of [ROUGE](https://aclanthology.org/W04-1013/).\n\n    The calculation is based on the [rouge](https://github.com/pltrdy/rouge) library.\n\n    Args:\n        tokenizer: An instance of `Tokenizer` to tokenize the input and output strings.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import ROUGE\n        &gt;&gt;&gt; from flexeval import WhitespaceTokenizer\n        &gt;&gt;&gt; tokenizer = WhitespaceTokenizer()\n        &gt;&gt;&gt; rouge = ROUGE(tokenizer)\n        &gt;&gt;&gt; lm_outputs = [\"I am a student .\", \"I am a teacher .\"]\n        &gt;&gt;&gt; references_list = [[\"I am a student .\", \"I am a learner .\"], [\"I am a teacher .\"]]\n        &gt;&gt;&gt; result = rouge.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'rouge1': 0.999999995, 'rouge2': 0.999999995, 'rougeL': 0.999999995},\n            instance_details=[\n                {'rouge1': 0.999999995, 'rouge2': 0.999999995, 'rougeL': 0.999999995},\n                {'rouge1': 0.999999995, 'rouge2': 0.999999995, 'rougeL': 0.999999995}\n            ]\n        )\n    \"\"\"\n\n    def __init__(self, tokenizer: Tokenizer) -&gt; None:\n        self._tokenizer = tokenizer\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]],\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if len(lm_outputs) != len(references_list):\n            msg = (\n                f\"lm_outputs and references_list must have the same length, \"\n                f\"but got {len(lm_outputs)} and {len(references_list)}.\"\n            )\n            raise ValueError(msg)\n\n        # we only need the first reference\n        target_summaries = [references[0] for references in references_list]\n\n        tokenized_lm_outputs = [\" \".join(self._tokenizer.tokenize(lm_output)) for lm_output in lm_outputs]\n        tokenized_target_summaries = [\n            \" \".join(self._tokenizer.tokenize(target_summary)) for target_summary in target_summaries\n        ]\n\n        # replace empty string with \" \" to avoid \"ValueError: Hypothesis is empty\" from rouge\n        tokenized_lm_outputs = [o if o else \" \" for o in tokenized_lm_outputs]\n\n        rouge = RougeCalculator()\n        score_outputs = rouge.get_scores(\n            tokenized_lm_outputs,\n            tokenized_target_summaries,\n        )\n\n        rouge1_list = [o[\"rouge-1\"][\"f\"] for o in score_outputs]\n        rouge2_list = [o[\"rouge-2\"][\"f\"] for o in score_outputs]\n        rouge_l_list = [o[\"rouge-l\"][\"f\"] for o in score_outputs]\n\n        # we only need the f1 score\n        return MetricResult(\n            {\n                \"rouge1\": sum(rouge1_list) / len(rouge1_list),\n                \"rouge2\": sum(rouge2_list) / len(rouge2_list),\n                \"rougeL\": sum(rouge_l_list) / len(rouge_l_list),\n            },\n            instance_details=[\n                {\"rouge1\": r1, \"rouge2\": r2, \"rougeL\": rL} for r1, r2, rL in zip(rouge1_list, rouge2_list, rouge_l_list)\n            ],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.rouge.ROUGE.__init__","title":"__init__","text":"<pre><code>__init__(tokenizer: Tokenizer) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/rouge.py</code> <pre><code>def __init__(self, tokenizer: Tokenizer) -&gt; None:\n    self._tokenizer = tokenizer\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.rouge.ROUGE.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]], task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/rouge.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]],\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if len(lm_outputs) != len(references_list):\n        msg = (\n            f\"lm_outputs and references_list must have the same length, \"\n            f\"but got {len(lm_outputs)} and {len(references_list)}.\"\n        )\n        raise ValueError(msg)\n\n    # we only need the first reference\n    target_summaries = [references[0] for references in references_list]\n\n    tokenized_lm_outputs = [\" \".join(self._tokenizer.tokenize(lm_output)) for lm_output in lm_outputs]\n    tokenized_target_summaries = [\n        \" \".join(self._tokenizer.tokenize(target_summary)) for target_summary in target_summaries\n    ]\n\n    # replace empty string with \" \" to avoid \"ValueError: Hypothesis is empty\" from rouge\n    tokenized_lm_outputs = [o if o else \" \" for o in tokenized_lm_outputs]\n\n    rouge = RougeCalculator()\n    score_outputs = rouge.get_scores(\n        tokenized_lm_outputs,\n        tokenized_target_summaries,\n    )\n\n    rouge1_list = [o[\"rouge-1\"][\"f\"] for o in score_outputs]\n    rouge2_list = [o[\"rouge-2\"][\"f\"] for o in score_outputs]\n    rouge_l_list = [o[\"rouge-l\"][\"f\"] for o in score_outputs]\n\n    # we only need the f1 score\n    return MetricResult(\n        {\n            \"rouge1\": sum(rouge1_list) / len(rouge1_list),\n            \"rouge2\": sum(rouge2_list) / len(rouge2_list),\n            \"rougeL\": sum(rouge_l_list) / len(rouge_l_list),\n        },\n        instance_details=[\n            {\"rouge1\": r1, \"rouge2\": r2, \"rougeL\": rL} for r1, r2, rL in zip(rouge1_list, rouge2_list, rouge_l_list)\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.substring_match.SubstringMatch","title":"SubstringMatch","text":"<p>A metric that calculates how many outputs contain any of the expected substrings.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import SubstringMatch\n&gt;&gt;&gt; substring_match = SubstringMatch()\n&gt;&gt;&gt; lm_outputs = [\"This is a cat .\", \"This is a dog .\"]\n&gt;&gt;&gt; references_list = [[\"cat\", \"dog\"], [\"mouse\"]]\n&gt;&gt;&gt; result = substring_match.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'substring_match': 0.5},\n    instance_details=[{'substring_match': True}, {'substring_match': False}]\n)\n</code></pre> Source code in <code>flexeval/core/metric/substring_match.py</code> <pre><code>class SubstringMatch(Metric):\n    \"\"\"\n    A metric that calculates how many outputs contain any of the expected substrings.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import SubstringMatch\n        &gt;&gt;&gt; substring_match = SubstringMatch()\n        &gt;&gt;&gt; lm_outputs = [\"This is a cat .\", \"This is a dog .\"]\n        &gt;&gt;&gt; references_list = [[\"cat\", \"dog\"], [\"mouse\"]]\n        &gt;&gt;&gt; result = substring_match.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'substring_match': 0.5},\n            instance_details=[{'substring_match': True}, {'substring_match': False}]\n        )\n    \"\"\"\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]],\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if len(lm_outputs) != len(references_list):\n            msg = (\n                f\"lm_outputs and references_list must have the same length, \"\n                f\"but got {len(lm_outputs)} and {len(references_list)}.\"\n            )\n            raise ValueError(msg)\n\n        match_list = [\n            any(substring in lm_output for substring in expected_output)\n            for lm_output, expected_output in zip(lm_outputs, references_list)\n        ]\n\n        return MetricResult(\n            {\"substring_match\": sum(match_list) / len(match_list)},\n            instance_details=[{\"substring_match\": match} for match in match_list],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.substring_match.SubstringMatch.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]], task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/substring_match.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]],\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if len(lm_outputs) != len(references_list):\n        msg = (\n            f\"lm_outputs and references_list must have the same length, \"\n            f\"but got {len(lm_outputs)} and {len(references_list)}.\"\n        )\n        raise ValueError(msg)\n\n    match_list = [\n        any(substring in lm_output for substring in expected_output)\n        for lm_output, expected_output in zip(lm_outputs, references_list)\n    ]\n\n    return MetricResult(\n        {\"substring_match\": sum(match_list) / len(match_list)},\n        instance_details=[{\"substring_match\": match} for match in match_list],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.xer.XER","title":"XER","text":"<p>Calculate the Character Error Rate (CER) and Word Error Rate (WER) between the model outputs and the references. The calculation is based on the jiwer library.</p> <p>Parameters:</p> <ul> <li> <code>tokenizer</code>               (<code>Tokenizer | None</code>, default:                   <code>None</code> )           \u2013            <p>An instance of <code>Tokenizer</code> to tokenize the input and output strings.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import XER\n&gt;&gt;&gt; xer = XER()\n&gt;&gt;&gt; lm_outputs = [\"I am a student .\", \"I am a teacher .\"]\n&gt;&gt;&gt; references_list = [[\"I am a student .\", \"I am a learner .\"], [\"Are you the student ?\"]]\n&gt;&gt;&gt; result = xer.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'cer_score': 0.43243243243243246, 'wer_score': 0.5},\n    instance_details=[{'cer_score': 0.0, 'wer_score': 0.0}, {'cer_score': 0.7619047619047619, 'wer_score': 1.0}\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/xer.py</code> <pre><code>class XER(Metric):\n    \"\"\"\n    Calculate the Character Error Rate (CER) and Word Error Rate (WER) between the model outputs and the references.\n    The calculation is based on the [jiwer](https://github.com/jitsi/jiwer) library.\n\n    Args:\n        tokenizer: An instance of `Tokenizer` to tokenize the input and output strings.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import XER\n        &gt;&gt;&gt; xer = XER()\n        &gt;&gt;&gt; lm_outputs = [\"I am a student .\", \"I am a teacher .\"]\n        &gt;&gt;&gt; references_list = [[\"I am a student .\", \"I am a learner .\"], [\"Are you the student ?\"]]\n        &gt;&gt;&gt; result = xer.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'cer_score': 0.43243243243243246, 'wer_score': 0.5},\n            instance_details=[{'cer_score': 0.0, 'wer_score': 0.0}, {'cer_score': 0.7619047619047619, 'wer_score': 1.0}\n            ]\n        )\n    \"\"\"\n\n    def __init__(self, tokenizer: Tokenizer | None = None) -&gt; None:\n        self.tokenizer = tokenizer\n\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        references_list: list[list[str]],\n        task_inputs_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if len(lm_outputs) != len(references_list):\n            msg = (\n                f\"lm_outputs and references_list must have the same length, \"\n                f\"but got {len(lm_outputs)} and {len(references_list)}.\"\n            )\n            raise ValueError(msg)\n\n        # we only need the first reference\n        references = [references[0] for references in references_list]\n\n        if self.tokenizer:\n            tokenized_lm_outputs = [\" \".join(self.tokenizer.tokenize(lm_output)) for lm_output in lm_outputs]\n            tokenized_references = [\" \".join(self.tokenizer.tokenize(reference)) for reference in references]\n        else:\n            tokenized_lm_outputs = lm_outputs\n            tokenized_references = references\n\n        cer_score = cer(references, lm_outputs)\n        wer_score = wer(tokenized_references, tokenized_lm_outputs)\n\n        return MetricResult(\n            {\n                \"cer_score\": cer_score,\n                \"wer_score\": wer_score,\n            },\n            instance_details=[\n                {\n                    \"cer_score\": cer(reference, lm_output),\n                    \"wer_score\": wer(reference, lm_output),\n                }\n                for lm_output, reference in zip(lm_outputs, references)\n            ],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.xer.XER.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = tokenizer\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.xer.XER.__init__","title":"__init__","text":"<pre><code>__init__(tokenizer: Tokenizer | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/xer.py</code> <pre><code>def __init__(self, tokenizer: Tokenizer | None = None) -&gt; None:\n    self.tokenizer = tokenizer\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.xer.XER.evaluate","title":"evaluate","text":"<pre><code>evaluate(lm_outputs: list[str], references_list: list[list[str]], task_inputs_list: list[dict[str, str]] | None = None) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/xer.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str],\n    references_list: list[list[str]],\n    task_inputs_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if len(lm_outputs) != len(references_list):\n        msg = (\n            f\"lm_outputs and references_list must have the same length, \"\n            f\"but got {len(lm_outputs)} and {len(references_list)}.\"\n        )\n        raise ValueError(msg)\n\n    # we only need the first reference\n    references = [references[0] for references in references_list]\n\n    if self.tokenizer:\n        tokenized_lm_outputs = [\" \".join(self.tokenizer.tokenize(lm_output)) for lm_output in lm_outputs]\n        tokenized_references = [\" \".join(self.tokenizer.tokenize(reference)) for reference in references]\n    else:\n        tokenized_lm_outputs = lm_outputs\n        tokenized_references = references\n\n    cer_score = cer(references, lm_outputs)\n    wer_score = wer(tokenized_references, tokenized_lm_outputs)\n\n    return MetricResult(\n        {\n            \"cer_score\": cer_score,\n            \"wer_score\": wer_score,\n        },\n        instance_details=[\n            {\n                \"cer_score\": cer(reference, lm_output),\n                \"wer_score\": wer(reference, lm_output),\n            }\n            for lm_output, reference in zip(lm_outputs, references)\n        ],\n    )\n</code></pre>"},{"location":"api_reference/PairwiseJudge/","title":"PairwiseJudge","text":""},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.PairwiseJudge","title":"PairwiseJudge","text":"<p>Judge which model is better given two items.</p> <p>The output is a tuple of the winner and the rationale.</p> Source code in <code>flexeval/core/pairwise_comparison/judge/base.py</code> <pre><code>class PairwiseJudge(ABC):\n    \"\"\"Judge which model is better given two items.\n\n    The output is a tuple of the winner and the rationale.\n    \"\"\"\n\n    @abstractmethod\n    def batch_judge(\n        self,\n        batch_model_items: list[tuple[dict[str, Any], dict[str, Any]]],\n    ) -&gt; list[tuple[Winner, str]]:\n        \"\"\"\n        Judge which model is better given a batch of item pairs.\n\n        Args:\n            batch_model_items: A list of tuples, each containing two model items.\n        \"\"\"\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.PairwiseJudge.batch_judge","title":"batch_judge  <code>abstractmethod</code>","text":"<pre><code>batch_judge(batch_model_items: list[tuple[dict[str, Any], dict[str, Any]]]) -&gt; list[tuple[Winner, str]]\n</code></pre> <p>Judge which model is better given a batch of item pairs.</p> <p>Parameters:</p> <ul> <li> <code>batch_model_items</code>               (<code>list[tuple[dict[str, Any], dict[str, Any]]]</code>)           \u2013            <p>A list of tuples, each containing two model items.</p> </li> </ul> Source code in <code>flexeval/core/pairwise_comparison/judge/base.py</code> <pre><code>@abstractmethod\ndef batch_judge(\n    self,\n    batch_model_items: list[tuple[dict[str, Any], dict[str, Any]]],\n) -&gt; list[tuple[Winner, str]]:\n    \"\"\"\n    Judge which model is better given a batch of item pairs.\n\n    Args:\n        batch_model_items: A list of tuples, each containing two model items.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.Winner","title":"Winner","text":"<p>Enum class to indicate the winner of a pairwise comparison.</p> Source code in <code>flexeval/core/pairwise_comparison/judge/base.py</code> <pre><code>class Winner(Enum):\n    \"\"\"\n    Enum class to indicate the winner of a pairwise comparison.\n    \"\"\"\n\n    MODEL1 = \"model1\"\n    MODEL2 = \"model2\"\n    DRAW = \"draw\"\n\n    def __str__(self) -&gt; str:\n        # used when serializing to JSON\n        return self.value\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.Winner.MODEL1","title":"MODEL1  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MODEL1 = 'model1'\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.Winner.MODEL2","title":"MODEL2  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MODEL2 = 'model2'\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.Winner.DRAW","title":"DRAW  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DRAW = 'draw'\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.Winner.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/judge/base.py</code> <pre><code>def __str__(self) -&gt; str:\n    # used when serializing to JSON\n    return self.value\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.llm_judge.ChatLLMPairwiseJudge","title":"ChatLLMPairwiseJudge","text":"<p>Pairwise judge using a chat language model to compare two model outputs.</p> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>LanguageModel</code>)           \u2013            <p>The language model to use for pairwise comparison.</p> </li> <li> <code>prompt_template</code>               (<code>PromptTemplate</code>)           \u2013            <p>The prompt template to embed the model outputs to be compared.</p> </li> <li> <code>system_message</code>               (<code>str | PromptTemplate | None</code>, default:                   <code>None</code> )           \u2013            <p>The system message to prepend to the chat messages.</p> </li> </ul> Source code in <code>flexeval/core/pairwise_comparison/judge/llm_judge.py</code> <pre><code>class ChatLLMPairwiseJudge(PairwiseJudge):\n    \"\"\"\n    Pairwise judge using a chat language model to compare two model outputs.\n\n    Args:\n        language_model: The language model to use for pairwise comparison.\n        prompt_template: The prompt template to embed the model outputs to be compared.\n        system_message: The system message to prepend to the chat messages.\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        system_message: str | PromptTemplate | None = None,\n    ) -&gt; None:\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.system_message = system_message\n\n    @staticmethod\n    def _parse_judge_output(judge_output: str) -&gt; tuple[Winner, str]:\n        \"\"\"Extract the last integer value from the judge output and return the\n        corresponding Winner and its rationale.\n\n        Return `Winner.DRAW` if parsing fails.\n        \"\"\"\n        try:\n            matched = re.findall(r\"(\\d+)\", judge_output)\n            value = int(matched[-1])\n            winner: Winner\n            rationale = judge_output\n            if value == 1:\n                winner = Winner.MODEL1\n            elif value == 2:\n                winner = Winner.MODEL2\n            elif value == 3:\n                winner = Winner.DRAW\n            else:\n                logger.warning(f\"Invalid number {value} was extracted:\\n\\n{judge_output}\")\n                winner = Winner.DRAW\n                rationale = f\"Invalid judge '{value}': {judge_output}\"\n        except (IndexError, ValueError):\n            logger.warning(f\"Failed to extract the judgment result:\\n\\n{judge_output}\")\n            return Winner.DRAW, f\"Parsing failure: {judge_output}\"\n        else:\n            return winner, rationale\n\n    def batch_judge(self, batch_model_items: list[tuple[dict[str, Any], dict[str, Any]]]) -&gt; list[tuple[Winner, str]]:\n        input_chat_messages_list: list[list[dict[str, str]]] = []\n        for model1_item, model2_item in batch_model_items:\n            references = model1_item[\"references\"]\n            prompt_inputs = {\n                \"model1_item\": model1_item,\n                \"model2_item\": model2_item,\n                \"references\": references,\n            }\n            self.prompt_template.embed_inputs(prompt_inputs)\n            judge_input = self.prompt_template.embed_inputs(prompt_inputs)\n            input_chat_messages = [{\"role\": \"user\", \"content\": judge_input}]\n            if self.system_message:\n                if isinstance(self.system_message, str):\n                    system_message = self.system_message\n                else:\n                    system_message = self.system_message.embed_inputs(prompt_inputs)\n                input_chat_messages.insert(\n                    0,\n                    {\"role\": \"system\", \"content\": system_message},\n                )\n            input_chat_messages_list.append(input_chat_messages)\n        judge_outputs = self.language_model.batch_generate_chat_response(input_chat_messages_list)\n        return [self._parse_judge_output(output) for output in judge_outputs]\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.llm_judge.ChatLLMPairwiseJudge.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.llm_judge.ChatLLMPairwiseJudge.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.llm_judge.ChatLLMPairwiseJudge.system_message","title":"system_message  <code>instance-attribute</code>","text":"<pre><code>system_message = system_message\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.llm_judge.ChatLLMPairwiseJudge.__init__","title":"__init__","text":"<pre><code>__init__(language_model: LanguageModel, prompt_template: PromptTemplate, system_message: str | PromptTemplate | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/judge/llm_judge.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    system_message: str | PromptTemplate | None = None,\n) -&gt; None:\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.system_message = system_message\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.llm_judge.ChatLLMPairwiseJudge.batch_judge","title":"batch_judge","text":"<pre><code>batch_judge(batch_model_items: list[tuple[dict[str, Any], dict[str, Any]]]) -&gt; list[tuple[Winner, str]]\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/judge/llm_judge.py</code> <pre><code>def batch_judge(self, batch_model_items: list[tuple[dict[str, Any], dict[str, Any]]]) -&gt; list[tuple[Winner, str]]:\n    input_chat_messages_list: list[list[dict[str, str]]] = []\n    for model1_item, model2_item in batch_model_items:\n        references = model1_item[\"references\"]\n        prompt_inputs = {\n            \"model1_item\": model1_item,\n            \"model2_item\": model2_item,\n            \"references\": references,\n        }\n        self.prompt_template.embed_inputs(prompt_inputs)\n        judge_input = self.prompt_template.embed_inputs(prompt_inputs)\n        input_chat_messages = [{\"role\": \"user\", \"content\": judge_input}]\n        if self.system_message:\n            if isinstance(self.system_message, str):\n                system_message = self.system_message\n            else:\n                system_message = self.system_message.embed_inputs(prompt_inputs)\n            input_chat_messages.insert(\n                0,\n                {\"role\": \"system\", \"content\": system_message},\n            )\n        input_chat_messages_list.append(input_chat_messages)\n    judge_outputs = self.language_model.batch_generate_chat_response(input_chat_messages_list)\n    return [self._parse_judge_output(output) for output in judge_outputs]\n</code></pre>"},{"location":"api_reference/PairwiseScorer/","title":"PairwiseScorer","text":""},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.base.PairwiseScorer","title":"PairwiseScorer","text":"<p>Compute scores for each model given the match results.</p> <p>Each match result is a triple of two model names and the winner.</p> Source code in <code>flexeval/core/pairwise_comparison/scorer/base.py</code> <pre><code>class PairwiseScorer(ABC):\n    \"\"\"Compute scores for each model given the match results.\n\n    Each match result is a triple of two model names and the winner.\n    \"\"\"\n\n    name: str = None\n\n    @abstractmethod\n    def compute_scores(\n        self: PairwiseScorer,\n        match_results: list[tuple[str, str, Winner]],\n    ) -&gt; dict[str, float]:\n        pass\n\n    @classmethod\n    def get_name(cls: type[PairwiseScorer]) -&gt; str:\n        return cls.name if cls.name else cls.__name__\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.base.PairwiseScorer.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = None\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.base.PairwiseScorer.compute_scores","title":"compute_scores  <code>abstractmethod</code>","text":"<pre><code>compute_scores(match_results: list[tuple[str, str, Winner]]) -&gt; dict[str, float]\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/scorer/base.py</code> <pre><code>@abstractmethod\ndef compute_scores(\n    self: PairwiseScorer,\n    match_results: list[tuple[str, str, Winner]],\n) -&gt; dict[str, float]:\n    pass\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.base.PairwiseScorer.get_name","title":"get_name  <code>classmethod</code>","text":"<pre><code>get_name() -&gt; str\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/scorer/base.py</code> <pre><code>@classmethod\ndef get_name(cls: type[PairwiseScorer]) -&gt; str:\n    return cls.name if cls.name else cls.__name__\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer","title":"BradleyTerryScorer","text":"Source code in <code>flexeval/core/pairwise_comparison/scorer/bradley_terry.py</code> <pre><code>class BradleyTerryScorer(PairwiseScorer):\n    name: str = \"bradley_terry\"\n\n    def __init__(\n        self,\n        max_iters: int = 1000,\n        error_tol: float = 1e-3,\n        eps: float = 1e-8,\n        base: float = 10.0,\n        scale: float = 400.0,\n        init_rating: float = 1000.0,\n    ) -&gt; None:\n        self.max_iters = max_iters\n        self.error_tol = error_tol\n        self.eps = eps\n        self.base = base\n        self.scale = scale\n        self.init_rating = init_rating\n\n    def _gen_winloss_matrix(\n        self,\n        match_results: list[tuple[str, str, Winner]],\n    ) -&gt; dict[str, dict[str, float]]:\n        \"\"\"\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001 `matrix[\u30e2\u30c7\u30eb1][\u30e2\u30c7\u30eb2] = &lt;\u30e2\u30c7\u30eb1\u304c\u30e2\u30c7\u30eb2\u306b\u52dd\u3063\u305f\u56de\u6570&gt;` \u3068\u306a\u308b\u3088\u3046\u306a\u8f9e\u66f8\u3092\u8fd4\u3059\"\"\"\n        matrix = defaultdict(lambda: defaultdict(float))\n\n        for model1, model2, winner in match_results:\n            if winner == Winner.MODEL1:\n                matrix[model1][model2] += 1.0\n            elif winner == Winner.MODEL2:\n                matrix[model2][model1] += 1.0\n            elif winner == Winner.DRAW:\n                matrix[model1][model2] += 0.5\n                matrix[model2][model1] += 0.5\n\n        return matrix\n\n    def compute_scores(\n        self,\n        match_results: list[tuple[str, str, Winner]],\n    ) -&gt; dict[str, float]:\n        \"\"\"\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001Bradley-Terry model (MLE) \u3067\u63a8\u5b9a\u3057\u305f\u5404\u30e2\u30c7\u30eb\u306e\u30b9\u30b3\u30a2\u3092\u8fd4\u3059\u3002\"\"\"\n        model_names = sorted(\n            {m[0] for m in match_results} | {m[1] for m in match_results},\n        )\n        winloss_matrix = self._gen_winloss_matrix(match_results)\n\n        # https://jmlr.org/papers/volume24/22-1086/22-1086.pdf#page=5.50 (12)\n        scores = pd.Series(np.ones(len(model_names)), index=model_names)\n        for iters in range(self.max_iters):\n            old_scores = scores.copy()\n            for target_model in scores.keys():  # noqa: SIM118\n                numer = sum(\n                    [\n                        (winloss_matrix[target_model][other_model] * scores[other_model])\n                        / (scores[target_model] + scores[other_model])\n                        for other_model in winloss_matrix[target_model]\n                    ],\n                )\n                denom = sum(\n                    [\n                        (winloss_matrix[other_model][target_model]) / (scores[target_model] + scores[other_model])\n                        for other_model in winloss_matrix[target_model]\n                    ],\n                )\n\n                scores[target_model] = numer / (denom + self.eps)\n\n            scores /= np.exp(np.log(scores).sum()) ** (1 / len(scores))\n\n            if (scores - old_scores).abs().sum() &lt; self.error_tol:\n                logger.info(f\" * Converged after {iters} iterations.\")\n                break\n        else:\n            logger.info(\n                f\" * Max iterations reached ({self.max_iters} iters).\",\n            )\n\n        return (\n            scores.apply(\n                lambda x: self.scale / np.log(self.base) * np.log(x) + self.init_rating,\n            )\n            .sort_values(ascending=False)\n            .to_dict()\n        )\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = 'bradley_terry'\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.max_iters","title":"max_iters  <code>instance-attribute</code>","text":"<pre><code>max_iters = max_iters\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.error_tol","title":"error_tol  <code>instance-attribute</code>","text":"<pre><code>error_tol = error_tol\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps = eps\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.base","title":"base  <code>instance-attribute</code>","text":"<pre><code>base = base\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = scale\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.init_rating","title":"init_rating  <code>instance-attribute</code>","text":"<pre><code>init_rating = init_rating\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.__init__","title":"__init__","text":"<pre><code>__init__(max_iters: int = 1000, error_tol: float = 0.001, eps: float = 1e-08, base: float = 10.0, scale: float = 400.0, init_rating: float = 1000.0) -&gt; None\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/scorer/bradley_terry.py</code> <pre><code>def __init__(\n    self,\n    max_iters: int = 1000,\n    error_tol: float = 1e-3,\n    eps: float = 1e-8,\n    base: float = 10.0,\n    scale: float = 400.0,\n    init_rating: float = 1000.0,\n) -&gt; None:\n    self.max_iters = max_iters\n    self.error_tol = error_tol\n    self.eps = eps\n    self.base = base\n    self.scale = scale\n    self.init_rating = init_rating\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.compute_scores","title":"compute_scores","text":"<pre><code>compute_scores(match_results: list[tuple[str, str, Winner]]) -&gt; dict[str, float]\n</code></pre> <p>\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001Bradley-Terry model (MLE) \u3067\u63a8\u5b9a\u3057\u305f\u5404\u30e2\u30c7\u30eb\u306e\u30b9\u30b3\u30a2\u3092\u8fd4\u3059\u3002</p> Source code in <code>flexeval/core/pairwise_comparison/scorer/bradley_terry.py</code> <pre><code>def compute_scores(\n    self,\n    match_results: list[tuple[str, str, Winner]],\n) -&gt; dict[str, float]:\n    \"\"\"\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001Bradley-Terry model (MLE) \u3067\u63a8\u5b9a\u3057\u305f\u5404\u30e2\u30c7\u30eb\u306e\u30b9\u30b3\u30a2\u3092\u8fd4\u3059\u3002\"\"\"\n    model_names = sorted(\n        {m[0] for m in match_results} | {m[1] for m in match_results},\n    )\n    winloss_matrix = self._gen_winloss_matrix(match_results)\n\n    # https://jmlr.org/papers/volume24/22-1086/22-1086.pdf#page=5.50 (12)\n    scores = pd.Series(np.ones(len(model_names)), index=model_names)\n    for iters in range(self.max_iters):\n        old_scores = scores.copy()\n        for target_model in scores.keys():  # noqa: SIM118\n            numer = sum(\n                [\n                    (winloss_matrix[target_model][other_model] * scores[other_model])\n                    / (scores[target_model] + scores[other_model])\n                    for other_model in winloss_matrix[target_model]\n                ],\n            )\n            denom = sum(\n                [\n                    (winloss_matrix[other_model][target_model]) / (scores[target_model] + scores[other_model])\n                    for other_model in winloss_matrix[target_model]\n                ],\n            )\n\n            scores[target_model] = numer / (denom + self.eps)\n\n        scores /= np.exp(np.log(scores).sum()) ** (1 / len(scores))\n\n        if (scores - old_scores).abs().sum() &lt; self.error_tol:\n            logger.info(f\" * Converged after {iters} iterations.\")\n            break\n    else:\n        logger.info(\n            f\" * Max iterations reached ({self.max_iters} iters).\",\n        )\n\n    return (\n        scores.apply(\n            lambda x: self.scale / np.log(self.base) * np.log(x) + self.init_rating,\n        )\n        .sort_values(ascending=False)\n        .to_dict()\n    )\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.win_rate.WinRateScorer","title":"WinRateScorer","text":"Source code in <code>flexeval/core/pairwise_comparison/scorer/win_rate.py</code> <pre><code>class WinRateScorer(PairwiseScorer):\n    name: str = \"win_rate\"\n\n    def compute_scores(\n        self,\n        match_results: list[tuple[str, str, Winner]],\n    ) -&gt; dict[str, float]:\n        \"\"\"\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001\u5404\u30e2\u30c7\u30eb\u306e\u52dd\u7387\u3092\u8fd4\u3059\u3002\"\"\"\n        match_count_dict: dict[str, float] = defaultdict(float)\n        win_count_dict: dict[str, float] = defaultdict(float)\n\n        for model1, model2, winner in match_results:\n            match_count_dict[model1] += 1\n            match_count_dict[model2] += 1\n            if winner == Winner.MODEL1:\n                win_count_dict[model1] += 1\n            elif winner == Winner.MODEL2:\n                win_count_dict[model2] += 1\n            elif winner == Winner.DRAW:\n                win_count_dict[model1] += 0.5\n                win_count_dict[model2] += 0.5\n\n        win_rate_dict = {}\n        for model in match_count_dict:\n            win_rate_dict[model] = 100 * win_count_dict.get(model, 0.0) / match_count_dict[model]\n\n        return dict(sorted(win_rate_dict.items(), key=lambda x: -x[1]))\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.win_rate.WinRateScorer.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = 'win_rate'\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.win_rate.WinRateScorer.compute_scores","title":"compute_scores","text":"<pre><code>compute_scores(match_results: list[tuple[str, str, Winner]]) -&gt; dict[str, float]\n</code></pre> <p>\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001\u5404\u30e2\u30c7\u30eb\u306e\u52dd\u7387\u3092\u8fd4\u3059\u3002</p> Source code in <code>flexeval/core/pairwise_comparison/scorer/win_rate.py</code> <pre><code>def compute_scores(\n    self,\n    match_results: list[tuple[str, str, Winner]],\n) -&gt; dict[str, float]:\n    \"\"\"\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001\u5404\u30e2\u30c7\u30eb\u306e\u52dd\u7387\u3092\u8fd4\u3059\u3002\"\"\"\n    match_count_dict: dict[str, float] = defaultdict(float)\n    win_count_dict: dict[str, float] = defaultdict(float)\n\n    for model1, model2, winner in match_results:\n        match_count_dict[model1] += 1\n        match_count_dict[model2] += 1\n        if winner == Winner.MODEL1:\n            win_count_dict[model1] += 1\n        elif winner == Winner.MODEL2:\n            win_count_dict[model2] += 1\n        elif winner == Winner.DRAW:\n            win_count_dict[model1] += 0.5\n            win_count_dict[model2] += 0.5\n\n    win_rate_dict = {}\n    for model in match_count_dict:\n        win_rate_dict[model] = 100 * win_count_dict.get(model, 0.0) / match_count_dict[model]\n\n    return dict(sorted(win_rate_dict.items(), key=lambda x: -x[1]))\n</code></pre>"},{"location":"api_reference/PromptTemplate/","title":"PromptTemplate","text":""},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.base.PromptTemplate","title":"PromptTemplate","text":"<p>This class embeds task inputs from <code>GenerationInstance</code> or <code>MultipleChoiceInstance</code> into a text that can be used as a prompt for <code>LanguageModel</code>.</p> Source code in <code>flexeval/core/prompt_template/base.py</code> <pre><code>class PromptTemplate(ABC):\n    \"\"\"\n    This class embeds task inputs from `GenerationInstance` or `MultipleChoiceInstance` into a text that can be used\n    as a prompt for `LanguageModel`.\n    \"\"\"\n\n    @abstractmethod\n    def embed_inputs(self, input_dict: dict[str, Any]) -&gt; str:\n        \"\"\"\n        Embeds the input into a prompt template.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.base.PromptTemplate.embed_inputs","title":"embed_inputs  <code>abstractmethod</code>","text":"<pre><code>embed_inputs(input_dict: dict[str, Any]) -&gt; str\n</code></pre> <p>Embeds the input into a prompt template.</p> Source code in <code>flexeval/core/prompt_template/base.py</code> <pre><code>@abstractmethod\ndef embed_inputs(self, input_dict: dict[str, Any]) -&gt; str:\n    \"\"\"\n    Embeds the input into a prompt template.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.jinja2.Jinja2PromptTemplate","title":"Jinja2PromptTemplate","text":"<p>Embed task inputs using Jinja2 template engine.</p> <p>Parameters:</p> <ul> <li> <code>template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The Jinja2 template to use.</p> </li> <li> <code>template_path</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to a file with the Jinja2 template to use.</p> </li> </ul> Source code in <code>flexeval/core/prompt_template/jinja2.py</code> <pre><code>class Jinja2PromptTemplate(PromptTemplate):\n    \"\"\"\n    Embed task inputs using Jinja2 template engine.\n\n    Args:\n        template: The Jinja2 template to use.\n        template_path: The path to a file with the Jinja2 template to use.\n    \"\"\"\n\n    def __init__(self, template: str | None = None, template_path: str | None = None) -&gt; None:\n        if template is None and template_path is None:\n            msg = \"Either template or template_path must be provided\"\n            raise ValueError(msg)\n        if template is not None and template_path is not None:\n            msg = \"Only one of template or template_path can be provided\"\n            raise ValueError(msg)\n\n        if template_path is not None:\n            with open(template_path) as f:\n                self.template = f.read()\n        else:\n            self.template = template\n\n    def embed_inputs(self, input_dict: dict[str, Any]) -&gt; str:\n        return JINJA2_ENV.from_string(self.template).render(input_dict)\n\n    def __repr__(self) -&gt; str:\n        return f\"Jinja2PromptTemplate(template={self.template!r})\"\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.jinja2.Jinja2PromptTemplate.template","title":"template  <code>instance-attribute</code>","text":"<pre><code>template = read()\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.jinja2.Jinja2PromptTemplate.__init__","title":"__init__","text":"<pre><code>__init__(template: str | None = None, template_path: str | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/prompt_template/jinja2.py</code> <pre><code>def __init__(self, template: str | None = None, template_path: str | None = None) -&gt; None:\n    if template is None and template_path is None:\n        msg = \"Either template or template_path must be provided\"\n        raise ValueError(msg)\n    if template is not None and template_path is not None:\n        msg = \"Only one of template or template_path can be provided\"\n        raise ValueError(msg)\n\n    if template_path is not None:\n        with open(template_path) as f:\n            self.template = f.read()\n    else:\n        self.template = template\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.jinja2.Jinja2PromptTemplate.embed_inputs","title":"embed_inputs","text":"<pre><code>embed_inputs(input_dict: dict[str, Any]) -&gt; str\n</code></pre> Source code in <code>flexeval/core/prompt_template/jinja2.py</code> <pre><code>def embed_inputs(self, input_dict: dict[str, Any]) -&gt; str:\n    return JINJA2_ENV.from_string(self.template).render(input_dict)\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.jinja2.Jinja2PromptTemplate.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/prompt_template/jinja2.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"Jinja2PromptTemplate(template={self.template!r})\"\n</code></pre>"},{"location":"api_reference/ResultRecorder/","title":"ResultRecorder","text":""},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.base.ResultRecorder","title":"ResultRecorder","text":"<p>An abstract base class for recording experiment results, including configuration, metrics, and model outputs.</p> <p>This class defines the interface for different result recording implementations, such as saving to a local directory, uploading to wandb, or integrating with MLflow.</p> Source code in <code>flexeval/core/result_recorder/base.py</code> <pre><code>class ResultRecorder(ABC):\n    \"\"\"\n    An abstract base class for recording experiment results, including configuration,\n    metrics, and model outputs.\n\n    This class defines the interface for different result recording implementations,\n    such as saving to a local directory, uploading to wandb, or integrating with MLflow.\n    \"\"\"\n\n    @abstractmethod\n    def record_config(self, config: dict[str, Any], group: str | None = None) -&gt; None:\n        \"\"\"\n        Record the configuration parameters of the experiment.\n\n        Args:\n            config: A dictionary containing the configuration\n                parameters of the evaluation.\n            group: An optional group name to organize the configuration.\n        \"\"\"\n\n    @abstractmethod\n    def record_metrics(self, metrics: dict[str, Any], group: str | None = None) -&gt; None:\n        \"\"\"\n        Record the evaluation metrics of the experiment.\n\n        Args:\n            metrics: A dictionary containing the evaluation metrics,\n                where keys are metric names and values are the corresponding results.\n            group: An optional group name to organize the metrics.\n        \"\"\"\n\n    @abstractmethod\n    def record_model_outputs(self, model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None:\n        \"\"\"\n        Record the outputs generated by the model during evaluation.\n\n        Args:\n            model_outputs: A list of dictionaries, where each\n                dictionary represents a single model output. The structure of these\n                dictionaries may vary depending on the specific model and task.\n            group: An optional group name to organize the model outputs.\n        \"\"\"\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.base.ResultRecorder.record_config","title":"record_config  <code>abstractmethod</code>","text":"<pre><code>record_config(config: dict[str, Any], group: str | None = None) -&gt; None\n</code></pre> <p>Record the configuration parameters of the experiment.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, Any]</code>)           \u2013            <p>A dictionary containing the configuration parameters of the evaluation.</p> </li> <li> <code>group</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional group name to organize the configuration.</p> </li> </ul> Source code in <code>flexeval/core/result_recorder/base.py</code> <pre><code>@abstractmethod\ndef record_config(self, config: dict[str, Any], group: str | None = None) -&gt; None:\n    \"\"\"\n    Record the configuration parameters of the experiment.\n\n    Args:\n        config: A dictionary containing the configuration\n            parameters of the evaluation.\n        group: An optional group name to organize the configuration.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.base.ResultRecorder.record_metrics","title":"record_metrics  <code>abstractmethod</code>","text":"<pre><code>record_metrics(metrics: dict[str, Any], group: str | None = None) -&gt; None\n</code></pre> <p>Record the evaluation metrics of the experiment.</p> <p>Parameters:</p> <ul> <li> <code>metrics</code>               (<code>dict[str, Any]</code>)           \u2013            <p>A dictionary containing the evaluation metrics, where keys are metric names and values are the corresponding results.</p> </li> <li> <code>group</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional group name to organize the metrics.</p> </li> </ul> Source code in <code>flexeval/core/result_recorder/base.py</code> <pre><code>@abstractmethod\ndef record_metrics(self, metrics: dict[str, Any], group: str | None = None) -&gt; None:\n    \"\"\"\n    Record the evaluation metrics of the experiment.\n\n    Args:\n        metrics: A dictionary containing the evaluation metrics,\n            where keys are metric names and values are the corresponding results.\n        group: An optional group name to organize the metrics.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.base.ResultRecorder.record_model_outputs","title":"record_model_outputs  <code>abstractmethod</code>","text":"<pre><code>record_model_outputs(model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None\n</code></pre> <p>Record the outputs generated by the model during evaluation.</p> <p>Parameters:</p> <ul> <li> <code>model_outputs</code>               (<code>list[dict[str, Any]]</code>)           \u2013            <p>A list of dictionaries, where each dictionary represents a single model output. The structure of these dictionaries may vary depending on the specific model and task.</p> </li> <li> <code>group</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional group name to organize the model outputs.</p> </li> </ul> Source code in <code>flexeval/core/result_recorder/base.py</code> <pre><code>@abstractmethod\ndef record_model_outputs(self, model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None:\n    \"\"\"\n    Record the outputs generated by the model during evaluation.\n\n    Args:\n        model_outputs: A list of dictionaries, where each\n            dictionary represents a single model output. The structure of these\n            dictionaries may vary depending on the specific model and task.\n        group: An optional group name to organize the model outputs.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder","title":"LocalRecorder","text":"<p>A class to record the results in JSON format.</p> <p>Parameters:</p> <ul> <li> <code>output_dir</code>               (<code>str</code>)           \u2013            <p>The directory to save the results.</p> </li> </ul> Source code in <code>flexeval/core/result_recorder/local_recorder.py</code> <pre><code>class LocalRecorder(ResultRecorder):\n    \"\"\"\n    A class to record the results in JSON format.\n\n    Args:\n        output_dir: The directory to save the results.\n    \"\"\"\n\n    def __init__(self, output_dir: str, force: bool = False) -&gt; None:\n        self.output_dir = Path(output_dir)\n        self.force = force\n\n    @staticmethod\n    def _check_output_dir_exists(output_dir: str | PathLike[str], checked_files: list[str]) -&gt; None:\n        output_dir = Path(output_dir)\n        for file_name in checked_files:\n            if (output_dir / file_name).exists():\n                msg = (\n                    f\"`{output_dir / file_name}` already exists. If you want to overwrite it, \"\n                    f\"please specify `--force true` from CLI or `force=True` when initializing the recorder.\"\n                )\n                raise FileExistsError(msg)\n\n    def record_config(self, config: dict[str, Any], group: str | None = None) -&gt; None:\n        output_dir = self.output_dir\n        if group is not None:\n            output_dir = self.output_dir / group\n\n        if not self.force:\n            self._check_output_dir_exists(output_dir, [CONFIG_FILE_NAME])\n\n        save_json(config, output_dir / CONFIG_FILE_NAME)\n        logger.info(f\"Saved the config to {output_dir / CONFIG_FILE_NAME}\")\n\n    def record_metrics(self, metrics: dict[str, Any], group: str | None = None) -&gt; None:\n        output_dir = self.output_dir\n        if group is not None:\n            output_dir = self.output_dir / group\n\n        if not self.force:\n            self._check_output_dir_exists(output_dir, [METRIC_FILE_NAME])\n\n        save_json(metrics, output_dir / METRIC_FILE_NAME)\n        logger.info(f\"Saved the metrics to {output_dir / METRIC_FILE_NAME}\")\n\n    def record_model_outputs(self, model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None:\n        output_dir = self.output_dir\n        if group is not None:\n            output_dir = output_dir / group\n\n        if not self.force:\n            self._check_output_dir_exists(output_dir, [OUTPUTS_FILE_NAME])\n\n        save_jsonl(model_outputs, output_dir / OUTPUTS_FILE_NAME)\n        logger.info(f\"Saved the outputs to {output_dir / OUTPUTS_FILE_NAME}\")\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder.output_dir","title":"output_dir  <code>instance-attribute</code>","text":"<pre><code>output_dir = Path(output_dir)\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder.force","title":"force  <code>instance-attribute</code>","text":"<pre><code>force = force\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder.__init__","title":"__init__","text":"<pre><code>__init__(output_dir: str, force: bool = False) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/local_recorder.py</code> <pre><code>def __init__(self, output_dir: str, force: bool = False) -&gt; None:\n    self.output_dir = Path(output_dir)\n    self.force = force\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder.record_config","title":"record_config","text":"<pre><code>record_config(config: dict[str, Any], group: str | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/local_recorder.py</code> <pre><code>def record_config(self, config: dict[str, Any], group: str | None = None) -&gt; None:\n    output_dir = self.output_dir\n    if group is not None:\n        output_dir = self.output_dir / group\n\n    if not self.force:\n        self._check_output_dir_exists(output_dir, [CONFIG_FILE_NAME])\n\n    save_json(config, output_dir / CONFIG_FILE_NAME)\n    logger.info(f\"Saved the config to {output_dir / CONFIG_FILE_NAME}\")\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder.record_metrics","title":"record_metrics","text":"<pre><code>record_metrics(metrics: dict[str, Any], group: str | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/local_recorder.py</code> <pre><code>def record_metrics(self, metrics: dict[str, Any], group: str | None = None) -&gt; None:\n    output_dir = self.output_dir\n    if group is not None:\n        output_dir = self.output_dir / group\n\n    if not self.force:\n        self._check_output_dir_exists(output_dir, [METRIC_FILE_NAME])\n\n    save_json(metrics, output_dir / METRIC_FILE_NAME)\n    logger.info(f\"Saved the metrics to {output_dir / METRIC_FILE_NAME}\")\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder.record_model_outputs","title":"record_model_outputs","text":"<pre><code>record_model_outputs(model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/local_recorder.py</code> <pre><code>def record_model_outputs(self, model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None:\n    output_dir = self.output_dir\n    if group is not None:\n        output_dir = output_dir / group\n\n    if not self.force:\n        self._check_output_dir_exists(output_dir, [OUTPUTS_FILE_NAME])\n\n    save_jsonl(model_outputs, output_dir / OUTPUTS_FILE_NAME)\n    logger.info(f\"Saved the outputs to {output_dir / OUTPUTS_FILE_NAME}\")\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.wandb_recorder.WandBRecorder","title":"WandBRecorder","text":"<p>A class to record the results to Weights &amp; Biases.</p> <p>Parameters:</p> <ul> <li> <code>init_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>The arguments for the <code>wandb.init</code> function. Please refer to the official documentation for the details.</p> </li> </ul> Source code in <code>flexeval/core/result_recorder/wandb_recorder.py</code> <pre><code>class WandBRecorder(ResultRecorder):\n    \"\"\"\n    A class to record the results to Weights &amp; Biases.\n\n    Args:\n        init_kwargs: The arguments for the `wandb.init` function.\n            Please refer to [the official documentation](https://docs.wandb.ai/ref/python/init) for the details.\n    \"\"\"\n\n    def __init__(\n        self,\n        init_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        import wandb\n\n        self._wandb = wandb\n        init_kwargs = init_kwargs or {}\n        self._wandb.init(**init_kwargs)\n\n    def record_config(self, config: dict[str, Any], group: str | None = None) -&gt; None:\n        if group:\n            self._wandb.config.update({group: config})\n        else:\n            self._wandb.config.update(config)\n\n    def record_metrics(self, metrics: dict[str, Any], group: str | None = None) -&gt; None:\n        if group:\n            self._wandb.summary.update({group: metrics})\n        else:\n            self._wandb.summary.update(metrics)\n\n    def record_model_outputs(self, model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None:\n        table = self._wandb.Table(columns=list(model_outputs[0].keys()))\n\n        for output in model_outputs:\n            table.add_data(*output.values())\n\n        table_name = \"model_outputs\" if group is None else f\"{group}/model_outputs\"\n        self._wandb.log({table_name: table})\n\n    def __del__(self) -&gt; None:\n        self._wandb.finish()\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.wandb_recorder.WandBRecorder.__init__","title":"__init__","text":"<pre><code>__init__(init_kwargs: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/wandb_recorder.py</code> <pre><code>def __init__(\n    self,\n    init_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    import wandb\n\n    self._wandb = wandb\n    init_kwargs = init_kwargs or {}\n    self._wandb.init(**init_kwargs)\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.wandb_recorder.WandBRecorder.record_config","title":"record_config","text":"<pre><code>record_config(config: dict[str, Any], group: str | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/wandb_recorder.py</code> <pre><code>def record_config(self, config: dict[str, Any], group: str | None = None) -&gt; None:\n    if group:\n        self._wandb.config.update({group: config})\n    else:\n        self._wandb.config.update(config)\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.wandb_recorder.WandBRecorder.record_metrics","title":"record_metrics","text":"<pre><code>record_metrics(metrics: dict[str, Any], group: str | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/wandb_recorder.py</code> <pre><code>def record_metrics(self, metrics: dict[str, Any], group: str | None = None) -&gt; None:\n    if group:\n        self._wandb.summary.update({group: metrics})\n    else:\n        self._wandb.summary.update(metrics)\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.wandb_recorder.WandBRecorder.record_model_outputs","title":"record_model_outputs","text":"<pre><code>record_model_outputs(model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/wandb_recorder.py</code> <pre><code>def record_model_outputs(self, model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None:\n    table = self._wandb.Table(columns=list(model_outputs[0].keys()))\n\n    for output in model_outputs:\n        table.add_data(*output.values())\n\n    table_name = \"model_outputs\" if group is None else f\"{group}/model_outputs\"\n    self._wandb.log({table_name: table})\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.wandb_recorder.WandBRecorder.__del__","title":"__del__","text":"<pre><code>__del__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/wandb_recorder.py</code> <pre><code>def __del__(self) -&gt; None:\n    self._wandb.finish()\n</code></pre>"},{"location":"api_reference/RewardModel/","title":"RewardModel","text":""},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.base.RewardModel","title":"RewardModel","text":"<p>Base class for reward models.</p> Source code in <code>flexeval/core/reward_model/base.py</code> <pre><code>class RewardModel(ABC):\n    \"\"\"Base class for reward models.\"\"\"\n\n    @abstractmethod\n    def batch_judge(\n        self,\n        batch_reward_bench_instances: list[RewardBenchInstance],\n    ) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n        \"\"\"Judge a batch of reward bench instances.\n\n        Args:\n            batch_reward_bench_instances (list[RewardBenchInstance]): A list of tuples, each containing two model items.\n\n        Returns:\n            tuple[list[bool], list[Any]]: A tuple with the following elements:\n                - chosen_is_betters: Indicating whether each `chosen` item is considered better by the model.\n                - judge_outputs: A list of outputs (rationale, score, etc....) from the model.\n        \"\"\"\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.base.RewardModel.batch_judge","title":"batch_judge  <code>abstractmethod</code>","text":"<pre><code>batch_judge(batch_reward_bench_instances: list[RewardBenchInstance]) -&gt; tuple[list[bool], list[dict[str, Any]]]\n</code></pre> <p>Judge a batch of reward bench instances.</p> <p>Parameters:</p> <ul> <li> <code>batch_reward_bench_instances</code>               (<code>list[RewardBenchInstance]</code>)           \u2013            <p>A list of tuples, each containing two model items.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[bool], list[dict[str, Any]]]</code>           \u2013            <p>tuple[list[bool], list[Any]]: A tuple with the following elements: - chosen_is_betters: Indicating whether each <code>chosen</code> item is considered better by the model. - judge_outputs: A list of outputs (rationale, score, etc....) from the model.</p> </li> </ul> Source code in <code>flexeval/core/reward_model/base.py</code> <pre><code>@abstractmethod\ndef batch_judge(\n    self,\n    batch_reward_bench_instances: list[RewardBenchInstance],\n) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n    \"\"\"Judge a batch of reward bench instances.\n\n    Args:\n        batch_reward_bench_instances (list[RewardBenchInstance]): A list of tuples, each containing two model items.\n\n    Returns:\n        tuple[list[bool], list[Any]]: A tuple with the following elements:\n            - chosen_is_betters: Indicating whether each `chosen` item is considered better by the model.\n            - judge_outputs: A list of outputs (rationale, score, etc....) from the model.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.log_prob.LogProbRewardModel","title":"LogProbRewardModel","text":"<p>A reward model that judges the quality of a response based on the log probability computed by the auto-regressive language model.</p> Source code in <code>flexeval/core/reward_model/log_prob.py</code> <pre><code>class LogProbRewardModel(RewardModel):\n    \"\"\"\n    A reward model that judges the quality of a response\n    based on the log probability computed by the auto-regressive language model.\n    \"\"\"\n\n    def __init__(self, language_model: LanguageModel) -&gt; None:\n        self.language_model = language_model\n\n    def batch_judge(\n        self,\n        batch_reward_bench_instances: list[RewardBenchInstance],\n    ) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n        if not all(len(instance.chosen) == 1 for instance in batch_reward_bench_instances):\n            msg = \"`chosen` field must have exactly one element.\"\n            raise ValueError(msg)\n        if not all(len(instance.rejected) == 1 for instance in batch_reward_bench_instances):\n            msg = \"`rejected` field must have exactly one element.\"\n            raise ValueError(msg)\n\n        chosen_log_probs = self.language_model.batch_compute_chat_log_probs(\n            prompt_list=[instance.prompt for instance in batch_reward_bench_instances],\n            response_list=[instance.chosen[0] for instance in batch_reward_bench_instances],\n        )\n        rejected_log_probs = self.language_model.batch_compute_chat_log_probs(\n            prompt_list=[instance.prompt for instance in batch_reward_bench_instances],\n            response_list=[instance.rejected[0] for instance in batch_reward_bench_instances],\n        )\n        chosen_is_better = [\n            chosen_log_prob &gt; rejected_log_prob\n            for chosen_log_prob, rejected_log_prob in zip(chosen_log_probs, rejected_log_probs)\n        ]\n        outputs = [\n            {\n                \"chosen_log_prob\": chosen_log_prob,\n                \"rejected_log_prob\": rejected_log_prob,\n            }\n            for chosen_log_prob, rejected_log_prob in zip(chosen_log_probs, rejected_log_probs)\n        ]\n        return chosen_is_better, outputs\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.log_prob.LogProbRewardModel.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.log_prob.LogProbRewardModel.__init__","title":"__init__","text":"<pre><code>__init__(language_model: LanguageModel) -&gt; None\n</code></pre> Source code in <code>flexeval/core/reward_model/log_prob.py</code> <pre><code>def __init__(self, language_model: LanguageModel) -&gt; None:\n    self.language_model = language_model\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.log_prob.LogProbRewardModel.batch_judge","title":"batch_judge","text":"<pre><code>batch_judge(batch_reward_bench_instances: list[RewardBenchInstance]) -&gt; tuple[list[bool], list[dict[str, Any]]]\n</code></pre> Source code in <code>flexeval/core/reward_model/log_prob.py</code> <pre><code>def batch_judge(\n    self,\n    batch_reward_bench_instances: list[RewardBenchInstance],\n) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n    if not all(len(instance.chosen) == 1 for instance in batch_reward_bench_instances):\n        msg = \"`chosen` field must have exactly one element.\"\n        raise ValueError(msg)\n    if not all(len(instance.rejected) == 1 for instance in batch_reward_bench_instances):\n        msg = \"`rejected` field must have exactly one element.\"\n        raise ValueError(msg)\n\n    chosen_log_probs = self.language_model.batch_compute_chat_log_probs(\n        prompt_list=[instance.prompt for instance in batch_reward_bench_instances],\n        response_list=[instance.chosen[0] for instance in batch_reward_bench_instances],\n    )\n    rejected_log_probs = self.language_model.batch_compute_chat_log_probs(\n        prompt_list=[instance.prompt for instance in batch_reward_bench_instances],\n        response_list=[instance.rejected[0] for instance in batch_reward_bench_instances],\n    )\n    chosen_is_better = [\n        chosen_log_prob &gt; rejected_log_prob\n        for chosen_log_prob, rejected_log_prob in zip(chosen_log_probs, rejected_log_probs)\n    ]\n    outputs = [\n        {\n            \"chosen_log_prob\": chosen_log_prob,\n            \"rejected_log_prob\": rejected_log_prob,\n        }\n        for chosen_log_prob, rejected_log_prob in zip(chosen_log_probs, rejected_log_probs)\n    ]\n    return chosen_is_better, outputs\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel","title":"PairwiseJudgeRewardModel","text":"<p>Pairwise judge using a chat language model to compare two model or human outputs.</p> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>LanguageModel</code>)           \u2013            <p>The language model to use for pairwise comparison.             This model is expected to output PairwiseChoice.</p> </li> <li> <code>prompt_template</code>               (<code>PromptTemplate</code>)           \u2013            <p>The prompt template to embed the model outputs to be compared.              Be sure to include {{prompt}}, {{answer_a}}, and {{answer_b}}.</p> </li> <li> <code>system_message</code>               (<code>str | PromptTemplate | None</code>, default:                   <code>None</code> )           \u2013            <p>The system message to prepend to the chat messages.</p> </li> <li> <code>gen_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Generation kwargs for the language model.</p> </li> </ul> Source code in <code>flexeval/core/reward_model/pairwise_judge_reward_model.py</code> <pre><code>class PairwiseJudgeRewardModel(RewardModel):\n    \"\"\"Pairwise judge using a chat language model to compare two model or human\n    outputs.\n\n    Args:\n        language_model: The language model to use for pairwise comparison.\n                        This model is expected to output PairwiseChoice.\n        prompt_template: The prompt template to embed the model outputs to be compared.\n                         Be sure to include {{prompt}}, {{answer_a}}, and {{answer_b}}.\n        system_message: The system message to prepend to the chat messages.\n        gen_kwargs: Generation kwargs for the language model.\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        system_message: str | PromptTemplate | None = None,\n        gen_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        if gen_kwargs is None:\n            gen_kwargs = {}\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.system_message = system_message\n        self.gen_kwargs = gen_kwargs\n\n    def _create_input_chat_messages_list(self, pairwise_instance: PairwiseInstance) -&gt; list[dict[str, str]]:\n        pairwise_instance_asdict = asdict(pairwise_instance)\n        judge_input = self.prompt_template.embed_inputs(pairwise_instance_asdict)\n        input_chat_messages = [{\"role\": \"user\", \"content\": judge_input}]\n        if self.system_message:\n            if isinstance(self.system_message, str):\n                system_message = self.system_message\n            elif isinstance(self.system_message, PromptTemplate):\n                system_message = self.system_message.embed_inputs(pairwise_instance_asdict)\n            else:\n                msg = \"system_message should be str or PromptTemplate.\"\n                raise ValueError(msg)\n            input_chat_messages.insert(\n                0,\n                {\"role\": \"system\", \"content\": system_message},\n            )\n        return input_chat_messages\n\n    def _is_correct_llm_answer(self, llm_answer: str, pairwise_choice: PairwiseChoice) -&gt; bool:\n        # Check if the answer is one of the valid choices.\n        if PairwiseChoice.A.value in PairwiseChoice.B in llm_answer and PairwiseChoice.B in llm_answer:\n            return False\n        if pairwise_choice.value in llm_answer:\n            return True\n        return False\n\n    def batch_judge(\n        self,\n        batch_reward_bench_instances: list[RewardBenchInstance],\n    ) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n        input_chat_messages_list: list[list[dict[str, str]]] = []\n        all_pairwise_instances: list[PairwiseInstance] = []\n        outputs: list[dict[str, Any]] = []\n        for reward_bench_instance in batch_reward_bench_instances:\n            pairwise_instance_answer_a_is_chosen = PairwiseInstance(\n                prompt=reward_bench_instance.prompt,\n                answer_a=reward_bench_instance.chosen,\n                answer_b=reward_bench_instance.rejected,\n                answer_label=PairwiseChoice.A,\n            )\n            input_chat_messages_a_is_chosen = self._create_input_chat_messages_list(\n                pairwise_instance_answer_a_is_chosen\n            )\n            input_chat_messages_list.append(input_chat_messages_a_is_chosen)\n\n            pairwise_instance_answer_b_is_chosen = PairwiseInstance(\n                prompt=reward_bench_instance.prompt,\n                answer_a=reward_bench_instance.rejected,\n                answer_b=reward_bench_instance.chosen,\n                answer_label=PairwiseChoice.B,\n            )\n            input_chat_messages_b_is_chosen = self._create_input_chat_messages_list(\n                pairwise_instance_answer_b_is_chosen\n            )\n            input_chat_messages_list.append(input_chat_messages_b_is_chosen)\n            all_pairwise_instances += [pairwise_instance_answer_a_is_chosen, pairwise_instance_answer_b_is_chosen]\n\n            output = {\n                \"llm_inputs\": [input_chat_messages_a_is_chosen, input_chat_messages_b_is_chosen],\n            }\n            outputs.append(output)\n        judge_outputs = self.language_model.batch_generate_chat_response(input_chat_messages_list, **self.gen_kwargs)\n        chosen_is_betters: list[bool] = [\n            self._is_correct_llm_answer(judge_output, shuffle_pairwise_instance.answer_label)\n            for judge_output, shuffle_pairwise_instance in zip(judge_outputs, all_pairwise_instances)\n        ]\n\n        if len(outputs) * 2 != len(chosen_is_betters):\n            msg = \"The number of outputs should be twice the number of inputs.\"\n            raise ValueError(msg)\n\n        for i in range(len(outputs)):\n            outputs[i][\"llm_outputs\"] = [judge_outputs[i * 2], judge_outputs[i * 2 + 1]]\n            outputs[i][\"evaluation_results\"] = [chosen_is_betters[i * 2], chosen_is_betters[i * 2 + 1]]\n\n        return chosen_is_betters, outputs\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel.system_message","title":"system_message  <code>instance-attribute</code>","text":"<pre><code>system_message = system_message\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel.gen_kwargs","title":"gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>gen_kwargs = gen_kwargs\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel.__init__","title":"__init__","text":"<pre><code>__init__(language_model: LanguageModel, prompt_template: PromptTemplate, system_message: str | PromptTemplate | None = None, gen_kwargs: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/reward_model/pairwise_judge_reward_model.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    system_message: str | PromptTemplate | None = None,\n    gen_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    if gen_kwargs is None:\n        gen_kwargs = {}\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.system_message = system_message\n    self.gen_kwargs = gen_kwargs\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel.batch_judge","title":"batch_judge","text":"<pre><code>batch_judge(batch_reward_bench_instances: list[RewardBenchInstance]) -&gt; tuple[list[bool], list[dict[str, Any]]]\n</code></pre> Source code in <code>flexeval/core/reward_model/pairwise_judge_reward_model.py</code> <pre><code>def batch_judge(\n    self,\n    batch_reward_bench_instances: list[RewardBenchInstance],\n) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n    input_chat_messages_list: list[list[dict[str, str]]] = []\n    all_pairwise_instances: list[PairwiseInstance] = []\n    outputs: list[dict[str, Any]] = []\n    for reward_bench_instance in batch_reward_bench_instances:\n        pairwise_instance_answer_a_is_chosen = PairwiseInstance(\n            prompt=reward_bench_instance.prompt,\n            answer_a=reward_bench_instance.chosen,\n            answer_b=reward_bench_instance.rejected,\n            answer_label=PairwiseChoice.A,\n        )\n        input_chat_messages_a_is_chosen = self._create_input_chat_messages_list(\n            pairwise_instance_answer_a_is_chosen\n        )\n        input_chat_messages_list.append(input_chat_messages_a_is_chosen)\n\n        pairwise_instance_answer_b_is_chosen = PairwiseInstance(\n            prompt=reward_bench_instance.prompt,\n            answer_a=reward_bench_instance.rejected,\n            answer_b=reward_bench_instance.chosen,\n            answer_label=PairwiseChoice.B,\n        )\n        input_chat_messages_b_is_chosen = self._create_input_chat_messages_list(\n            pairwise_instance_answer_b_is_chosen\n        )\n        input_chat_messages_list.append(input_chat_messages_b_is_chosen)\n        all_pairwise_instances += [pairwise_instance_answer_a_is_chosen, pairwise_instance_answer_b_is_chosen]\n\n        output = {\n            \"llm_inputs\": [input_chat_messages_a_is_chosen, input_chat_messages_b_is_chosen],\n        }\n        outputs.append(output)\n    judge_outputs = self.language_model.batch_generate_chat_response(input_chat_messages_list, **self.gen_kwargs)\n    chosen_is_betters: list[bool] = [\n        self._is_correct_llm_answer(judge_output, shuffle_pairwise_instance.answer_label)\n        for judge_output, shuffle_pairwise_instance in zip(judge_outputs, all_pairwise_instances)\n    ]\n\n    if len(outputs) * 2 != len(chosen_is_betters):\n        msg = \"The number of outputs should be twice the number of inputs.\"\n        raise ValueError(msg)\n\n    for i in range(len(outputs)):\n        outputs[i][\"llm_outputs\"] = [judge_outputs[i * 2], judge_outputs[i * 2 + 1]]\n        outputs[i][\"evaluation_results\"] = [chosen_is_betters[i * 2], chosen_is_betters[i * 2 + 1]]\n\n    return chosen_is_betters, outputs\n</code></pre>"},{"location":"api_reference/StringProcessor/","title":"StringProcessor","text":""},{"location":"api_reference/StringProcessor/#flexeval.core.metric.string_processor.base.StringProcessor","title":"StringProcessor","text":"<p>An interface class used to process the model's output before evaluation. Typically used in <code>Metric</code>.</p> Source code in <code>flexeval/core/metric/string_processor/base.py</code> <pre><code>class StringProcessor(ABC):\n    \"\"\"An interface class used to process the model's output before evaluation.\n    Typically used in `Metric`.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, text: str) -&gt; str:\n        \"\"\"\n        Process the input text.\n\n        Args:\n            text: The text to process.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.metric.string_processor.base.StringProcessor.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> <p>Process the input text.</p> <p>Parameters:</p> <ul> <li> <code>text</code>               (<code>str</code>)           \u2013            <p>The text to process.</p> </li> </ul> Source code in <code>flexeval/core/metric/string_processor/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, text: str) -&gt; str:\n    \"\"\"\n    Process the input text.\n\n    Args:\n        text: The text to process.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.metric.string_processor.aio.AIONormalizer","title":"AIONormalizer","text":"<p>StringProcessor used for AI\u738b (AI king) question answering task. This is adapted from the official script.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import AIONormalizer\n&gt;&gt;&gt; processor = AIONormalizer()\n&gt;&gt;&gt; text = \"\u300c\u86f9\u5316(\u3088\u3046\u304b)\u300d\"\n&gt;&gt;&gt; normalized_text = processor(text)\n&gt;&gt;&gt; print(normalized_text)\n\u86f9\u5316\n</code></pre> Source code in <code>flexeval/core/metric/string_processor/aio.py</code> <pre><code>class AIONormalizer(StringProcessor):\n    \"\"\"StringProcessor used for AI\u738b (AI king) question answering task.\n    This is adapted from\n    [the official script](https://github.com/cl-tohoku/aio4-bpr-baseline/blob/c5a226296b5e1c403268016dc7136147bbb515fe/compute_score.py).\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import AIONormalizer\n        &gt;&gt;&gt; processor = AIONormalizer()\n        &gt;&gt;&gt; text = \"\u300c\u86f9\u5316(\u3088\u3046\u304b)\u300d\"\n        &gt;&gt;&gt; normalized_text = processor(text)\n        &gt;&gt;&gt; print(normalized_text)\n        \u86f9\u5316\n    \"\"\"\n\n    def __call__(self, text: str) -&gt; str:\n        # substitute some symbols that will not be replaced by unicode normalization\n        text = text.replace(\"\uff5e\", \"\u301c\")\n\n        # unicode normalization\n        text = unicodedata.normalize(\"NFKC\", text)\n\n        # lowercase alphabetical characters\n        text = text.lower()\n\n        # remove kagi-kakkos\n        text = re.sub(r\"\u300c(.*?)\u300d\", r\"\\1\", text)\n        text = re.sub(r\"\u300e(.*?)\u300f\", r\"\\1\", text)\n\n        # remove some punctuation marks\n        text = text.replace(\"\u30fb\", \"\")\n        text = text.replace(\"=\", \"\")\n        text = text.replace(\"-\", \"\")\n\n        # compress whitespaces\n        text = re.sub(r\"\\s+\", \"\", text).strip()\n\n        # remove parenthesis: \u86f9\u5316(\u3088\u3046\u304b)\u3000\u2192\u3000\u86f9\u5316\n        return re.sub(r\"\\((.*?)\\)\", \"\", text)\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.metric.string_processor.aio.AIONormalizer.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/string_processor/aio.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    # substitute some symbols that will not be replaced by unicode normalization\n    text = text.replace(\"\uff5e\", \"\u301c\")\n\n    # unicode normalization\n    text = unicodedata.normalize(\"NFKC\", text)\n\n    # lowercase alphabetical characters\n    text = text.lower()\n\n    # remove kagi-kakkos\n    text = re.sub(r\"\u300c(.*?)\u300d\", r\"\\1\", text)\n    text = re.sub(r\"\u300e(.*?)\u300f\", r\"\\1\", text)\n\n    # remove some punctuation marks\n    text = text.replace(\"\u30fb\", \"\")\n    text = text.replace(\"=\", \"\")\n    text = text.replace(\"-\", \"\")\n\n    # compress whitespaces\n    text = re.sub(r\"\\s+\", \"\", text).strip()\n\n    # remove parenthesis: \u86f9\u5316(\u3088\u3046\u304b)\u3000\u2192\u3000\u86f9\u5316\n    return re.sub(r\"\\((.*?)\\)\", \"\", text)\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.metric.string_processor.last_line.LastLineExtractor","title":"LastLineExtractor","text":"<p>Extract the last line from a string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import LastLineExtractor\n&gt;&gt;&gt; processor = LastLineExtractor()\n&gt;&gt;&gt; text = \"Answer\\nFUJI-YAMA\"\n&gt;&gt;&gt; print(processor(text))\nFUJI-YAMA\n</code></pre> Source code in <code>flexeval/core/metric/string_processor/last_line.py</code> <pre><code>class LastLineExtractor(StringProcessor):\n    \"\"\"Extract the last line from a string.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import LastLineExtractor\n        &gt;&gt;&gt; processor = LastLineExtractor()\n        &gt;&gt;&gt; text = \"Answer\\\\nFUJI-YAMA\"\n        &gt;&gt;&gt; print(processor(text))\n        FUJI-YAMA\n    \"\"\"\n\n    def __call__(self, text: str) -&gt; str:\n        return text.split(\"\\n\")[-1]\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.metric.string_processor.last_line.LastLineExtractor.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/string_processor/last_line.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    return text.split(\"\\n\")[-1]\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.metric.string_processor.noop.NoopNormalizer","title":"NoopNormalizer","text":"<p>A processor that does nothing. Some metrics apply normalization to both the LM outputs and references by default. If you want to explicitly disable normalization for the references, you can use this processor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import ExactMatch, NoopNormalizer, RegexExtractor\n&gt;&gt;&gt; metric = ExactMatch(processor=RegexExtractor(r\"\\d+\"), reference_processor=NoopNormalizer())\n&gt;&gt;&gt; lm_output = \"The answer is 10.\"\n&gt;&gt;&gt; reference = \"10\"\n&gt;&gt;&gt; print(metric.evaluate([lm_output], [[reference]]))\n</code></pre> Source code in <code>flexeval/core/metric/string_processor/noop.py</code> <pre><code>class NoopNormalizer(StringProcessor):\n    r\"\"\"\n    A processor that does nothing.\n    Some metrics apply normalization to both the LM outputs and references by default.\n    If you want to explicitly disable normalization for the references, you can use this processor.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import ExactMatch, NoopNormalizer, RegexExtractor\n        &gt;&gt;&gt; metric = ExactMatch(processor=RegexExtractor(r\"\\d+\"), reference_processor=NoopNormalizer())\n        &gt;&gt;&gt; lm_output = \"The answer is 10.\"\n        &gt;&gt;&gt; reference = \"10\"\n        &gt;&gt;&gt; print(metric.evaluate([lm_output], [[reference]]))\n\n    \"\"\"\n\n    def __call__(self, text: str) -&gt; str:\n        return text\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.metric.string_processor.noop.NoopNormalizer.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/string_processor/noop.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    return text\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.metric.string_processor.regex.RegexExtractor","title":"RegexExtractor","text":"<p>StringProcessor that extracts the last match of a regex pattern. Useful to extract an answer after a step-by-step derivation.</p> <p>Parameters:</p> <ul> <li> <code>pattern</code>               (<code>str</code>)           \u2013            <p>The regex pattern to extract.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import RegexExtractor\n&gt;&gt;&gt; processor = RegexExtractor(r\"Answer: (.*)\")\n&gt;&gt;&gt; text = \"Step 1: 3 + 2 = 5\\nStep 2: 5 \u00d7 4 = 20\\nAnswer: 20\"\n&gt;&gt;&gt; print(processor(text))\n20\n</code></pre> Source code in <code>flexeval/core/metric/string_processor/regex.py</code> <pre><code>class RegexExtractor(StringProcessor):\n    \"\"\"\n    StringProcessor that extracts the last match of a regex pattern.\n    Useful to extract an answer after a step-by-step derivation.\n\n    Args:\n        pattern: The regex pattern to extract.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import RegexExtractor\n        &gt;&gt;&gt; processor = RegexExtractor(r\"Answer: (.*)\")\n        &gt;&gt;&gt; text = \"Step 1: 3 + 2 = 5\\\\nStep 2: 5 \u00d7 4 = 20\\\\nAnswer: 20\"\n        &gt;&gt;&gt; print(processor(text))\n        20\n    \"\"\"\n\n    def __init__(self, pattern: str) -&gt; None:\n        self._pattern = re.compile(pattern, flags=re.DOTALL)\n\n    def __call__(self, text: str) -&gt; str:\n        found = self._pattern.findall(text)\n        if not found:\n            return \"\"\n        return found[-1]\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.metric.string_processor.regex.RegexExtractor.__init__","title":"__init__","text":"<pre><code>__init__(pattern: str) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/string_processor/regex.py</code> <pre><code>def __init__(self, pattern: str) -&gt; None:\n    self._pattern = re.compile(pattern, flags=re.DOTALL)\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.metric.string_processor.regex.RegexExtractor.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/string_processor/regex.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    found = self._pattern.findall(text)\n    if not found:\n        return \"\"\n    return found[-1]\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.metric.string_processor.string_strip.StringStrip","title":"StringStrip","text":"<p>Strip leading and trailing whitespaces from a string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import StringStrip\n&gt;&gt;&gt; processor = StringStrip()\n&gt;&gt;&gt; text = \" ABC\"\n&gt;&gt;&gt; normalized_text = processor(text)\n&gt;&gt;&gt; print(normalized_text)\nABC\n</code></pre> Source code in <code>flexeval/core/metric/string_processor/string_strip.py</code> <pre><code>class StringStrip(StringProcessor):\n    \"\"\"Strip leading and trailing whitespaces from a string.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import StringStrip\n        &gt;&gt;&gt; processor = StringStrip()\n        &gt;&gt;&gt; text = \" ABC\"\n        &gt;&gt;&gt; normalized_text = processor(text)\n        &gt;&gt;&gt; print(normalized_text)\n        ABC\n    \"\"\"\n\n    def __call__(self, text: str) -&gt; str:\n        return text.strip()\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.metric.string_processor.string_strip.StringStrip.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/string_processor/string_strip.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    return text.strip()\n</code></pre>"},{"location":"api_reference/TextDataset/","title":"TextDataset","text":""},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.base.TextDataset","title":"TextDataset","text":"<p>This class represents a dataset of text examples.</p> Source code in <code>flexeval/core/text_dataset/base.py</code> <pre><code>class TextDataset(Sequence[str], ABC):\n    \"\"\"\n    This class represents a dataset of text examples.\n    \"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        pass\n\n    @abstractmethod\n    def __getitem__(self, item: int) -&gt; str:\n        pass\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.base.TextDataset.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/text_dataset/base.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    pass\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.base.TextDataset.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(item: int) -&gt; str\n</code></pre> Source code in <code>flexeval/core/text_dataset/base.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, item: int) -&gt; str:\n    pass\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.base.TextDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/text_dataset/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.hf.HFTextDataset","title":"HFTextDataset","text":"<p>This class represents a dataset of text examples loaded from Hugging Face datasets.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The name of the dataset to load.</p> </li> <li> <code>split</code>               (<code>str</code>)           \u2013            <p>The split of the dataset to load.</p> </li> <li> <code>text_template</code>               (<code>str</code>)           \u2013            <p>A Jinja2 template for the text.</p> </li> <li> <code>subset</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The subset of the dataset to load.</p> </li> <li> <code>keep_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to filter certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.</p> </li> <li> <code>remove_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to remove certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.</p> </li> <li> <code>dataset_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Additional keyword arguments for <code>datasets.load_dataset</code>.</p> </li> </ul> Source code in <code>flexeval/core/text_dataset/hf.py</code> <pre><code>class HFTextDataset(TextDataset):\n    \"\"\"\n    This class represents a dataset of text examples loaded from Hugging Face datasets.\n\n    Args:\n        path: The name of the dataset to load.\n        split: The split of the dataset to load.\n        text_template: A Jinja2 template for the text.\n        subset: The subset of the dataset to load.\n        keep_conditions: A dictionary to indicate the condition to filter certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.\n        remove_conditions: A dictionary to indicate the condition to remove certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.\n        dataset_kwargs: Additional keyword arguments for `datasets.load_dataset`.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        split: str,\n        text_template: str,\n        subset: str | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n        dataset_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        dataset_kwargs = dataset_kwargs or {}\n        self.dataset = datasets.load_dataset(path, split=split, name=subset, **dataset_kwargs)\n\n        keep_conditions = keep_conditions or {}\n        for template_str, value_to_keep in keep_conditions.items():\n            filter_template = JINJA2_ENV.from_string(template_str)\n            self.dataset = self.dataset.filter(lambda x, t=filter_template, v=value_to_keep: t.render(**x) == v)\n        remove_conditions = remove_conditions or {}\n        for template_str, value_to_remove in remove_conditions.items():\n            filter_template = JINJA2_ENV.from_string(template_str)\n            self.dataset = self.dataset.filter(lambda x, t=filter_template, v=value_to_remove: t.render(**x) != v)\n\n        self.text_template = JINJA2_ENV.from_string(text_template)\n\n    def __len__(self) -&gt; int:\n        return len(self.dataset)\n\n    def __getitem__(self, i: int) -&gt; str:\n        item = self.dataset[i]\n        return self.text_template.render(**item)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.hf.HFTextDataset.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset = filter(lambda x, t=filter_template, v=value_to_remove: render(**x) != v)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.hf.HFTextDataset.text_template","title":"text_template  <code>instance-attribute</code>","text":"<pre><code>text_template = from_string(text_template)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.hf.HFTextDataset.__init__","title":"__init__","text":"<pre><code>__init__(path: str, split: str, text_template: str, subset: str | None = None, keep_conditions: dict[str, str] | None = None, remove_conditions: dict[str, str] | None = None, dataset_kwargs: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/text_dataset/hf.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    split: str,\n    text_template: str,\n    subset: str | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    dataset_kwargs = dataset_kwargs or {}\n    self.dataset = datasets.load_dataset(path, split=split, name=subset, **dataset_kwargs)\n\n    keep_conditions = keep_conditions or {}\n    for template_str, value_to_keep in keep_conditions.items():\n        filter_template = JINJA2_ENV.from_string(template_str)\n        self.dataset = self.dataset.filter(lambda x, t=filter_template, v=value_to_keep: t.render(**x) == v)\n    remove_conditions = remove_conditions or {}\n    for template_str, value_to_remove in remove_conditions.items():\n        filter_template = JINJA2_ENV.from_string(template_str)\n        self.dataset = self.dataset.filter(lambda x, t=filter_template, v=value_to_remove: t.render(**x) != v)\n\n    self.text_template = JINJA2_ENV.from_string(text_template)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.hf.HFTextDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/text_dataset/hf.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.dataset)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.hf.HFTextDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; str\n</code></pre> Source code in <code>flexeval/core/text_dataset/hf.py</code> <pre><code>def __getitem__(self, i: int) -&gt; str:\n    item = self.dataset[i]\n    return self.text_template.render(**item)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.jsonl.JsonlTextDataset","title":"JsonlTextDataset","text":"<p>This class represents a dataset of text examples loaded from a JSONL file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | PathLike[str]</code>)           \u2013            <p>The path to the JSONL file.</p> </li> <li> <code>field</code>               (<code>str</code>)           \u2013            <p>The field to extract from the JSONL file.</p> </li> </ul> Source code in <code>flexeval/core/text_dataset/jsonl.py</code> <pre><code>class JsonlTextDataset(TextDataset):\n    \"\"\"\n    This class represents a dataset of text examples loaded from a JSONL file.\n\n    Args:\n        path: The path to the JSONL file.\n        field: The field to extract from the JSONL file.\n    \"\"\"\n\n    def __init__(self, path: str | PathLike[str], field: str) -&gt; None:\n        self._text_list: list[str] = []\n        with open(path) as f:\n            for line in f:\n                item = json.loads(line)\n                self._text_list.append(item[field])\n\n    def __len__(self) -&gt; int:\n        return len(self._text_list)\n\n    def __getitem__(self, item: int) -&gt; str:\n        return self._text_list[item]\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.jsonl.JsonlTextDataset.__init__","title":"__init__","text":"<pre><code>__init__(path: str | PathLike[str], field: str) -&gt; None\n</code></pre> Source code in <code>flexeval/core/text_dataset/jsonl.py</code> <pre><code>def __init__(self, path: str | PathLike[str], field: str) -&gt; None:\n    self._text_list: list[str] = []\n    with open(path) as f:\n        for line in f:\n            item = json.loads(line)\n            self._text_list.append(item[field])\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.jsonl.JsonlTextDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/text_dataset/jsonl.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._text_list)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.jsonl.JsonlTextDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(item: int) -&gt; str\n</code></pre> Source code in <code>flexeval/core/text_dataset/jsonl.py</code> <pre><code>def __getitem__(self, item: int) -&gt; str:\n    return self._text_list[item]\n</code></pre>"},{"location":"api_reference/Tokenizer/","title":"Tokenizer","text":""},{"location":"api_reference/Tokenizer/#flexeval.core.metric.tokenizer.base.Tokenizer","title":"Tokenizer","text":"<p>Tokenizer interface.</p> <p>Tokenizers are used to split text into tokens. Typically, this is used in <code>Metric</code> that requires word-level statistics.</p> Source code in <code>flexeval/core/metric/tokenizer/base.py</code> <pre><code>class Tokenizer(ABC):\n    \"\"\"\n    Tokenizer interface.\n\n    Tokenizers are used to split text into tokens.\n    Typically, this is used in `Metric` that requires word-level statistics.\n    \"\"\"\n\n    @abstractmethod\n    def tokenize(self, text: str) -&gt; list[str]:\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.metric.tokenizer.base.Tokenizer.tokenize","title":"tokenize  <code>abstractmethod</code>","text":"<pre><code>tokenize(text: str) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/metric/tokenizer/base.py</code> <pre><code>@abstractmethod\ndef tokenize(self, text: str) -&gt; list[str]:\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.metric.tokenizer.mecab.MecabTokenizer","title":"MecabTokenizer","text":"<p>MeCab tokenizer for Japanese text.</p> Source code in <code>flexeval/core/metric/tokenizer/mecab.py</code> <pre><code>class MecabTokenizer(Tokenizer):\n    \"\"\"\n    MeCab tokenizer for Japanese text.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        import fugashi\n\n        self._tagger = fugashi.Tagger(\"-Owakati\")\n\n    def tokenize(self, text: str) -&gt; list[str]:\n        tokens = self._tagger(text)\n        return [token.surface for token in tokens]\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.metric.tokenizer.mecab.MecabTokenizer.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/tokenizer/mecab.py</code> <pre><code>def __init__(self) -&gt; None:\n    import fugashi\n\n    self._tagger = fugashi.Tagger(\"-Owakati\")\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.metric.tokenizer.mecab.MecabTokenizer.tokenize","title":"tokenize","text":"<pre><code>tokenize(text: str) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/metric/tokenizer/mecab.py</code> <pre><code>def tokenize(self, text: str) -&gt; list[str]:\n    tokens = self._tagger(text)\n    return [token.surface for token in tokens]\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.metric.tokenizer.sacrebleu_tokenizer.SacreBleuTokenizer","title":"SacreBleuTokenizer","text":"<p>A tokenizer imported from uses the sacrebleu library.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the tokenizer.</p> </li> </ul> Source code in <code>flexeval/core/metric/tokenizer/sacrebleu_tokenizer.py</code> <pre><code>class SacreBleuTokenizer(Tokenizer):\n    \"\"\"\n    A tokenizer imported from uses the sacrebleu library.\n\n    Args:\n        name: The name of the tokenizer.\n    \"\"\"\n\n    def __init__(self, name: str) -&gt; None:\n        self.tokenizer = _get_tokenizer(name)()\n\n    def tokenize(self, text: str) -&gt; list[str]:\n        return self.tokenizer(text).split(\" \")\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.metric.tokenizer.sacrebleu_tokenizer.SacreBleuTokenizer.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = _get_tokenizer(name)()\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.metric.tokenizer.sacrebleu_tokenizer.SacreBleuTokenizer.__init__","title":"__init__","text":"<pre><code>__init__(name: str) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/tokenizer/sacrebleu_tokenizer.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    self.tokenizer = _get_tokenizer(name)()\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.metric.tokenizer.sacrebleu_tokenizer.SacreBleuTokenizer.tokenize","title":"tokenize","text":"<pre><code>tokenize(text: str) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/metric/tokenizer/sacrebleu_tokenizer.py</code> <pre><code>def tokenize(self, text: str) -&gt; list[str]:\n    return self.tokenizer(text).split(\" \")\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.metric.tokenizer.whitespace.WhitespaceTokenizer","title":"WhitespaceTokenizer","text":"<p>A simple whitespace tokenizer.</p> Source code in <code>flexeval/core/metric/tokenizer/whitespace.py</code> <pre><code>class WhitespaceTokenizer(Tokenizer):\n    \"\"\"\n    A simple whitespace tokenizer.\n    \"\"\"\n\n    def tokenize(self, text: str) -&gt; list[str]:\n        return text.split()\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.metric.tokenizer.whitespace.WhitespaceTokenizer.tokenize","title":"tokenize","text":"<pre><code>tokenize(text: str) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/metric/tokenizer/whitespace.py</code> <pre><code>def tokenize(self, text: str) -&gt; list[str]:\n    return text.split()\n</code></pre>"},{"location":"api_reference/utils/","title":"Utils","text":""},{"location":"api_reference/utils/#flexeval.utils.module_utils.instantiate_from_config","title":"instantiate_from_config","text":"<pre><code>instantiate_from_config(config_path: str, overrides: dict[str, Any] | None = None) -&gt; Module\n</code></pre> <p>Instantiates a module from a jsonnet config file.</p> <p>Parameters:</p> <ul> <li> <code>config_path</code>               (<code>str</code>)           \u2013            <p>The path to the jsonnet config file.</p> </li> <li> <code>overrides</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of overrides to apply to the config.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The instantiated module.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import instantiate_from_config\n&gt;&gt;&gt; eval_setup = instantiate_from_config(\"aio\")\n</code></pre> Source code in <code>flexeval/utils/module_utils.py</code> <pre><code>def instantiate_from_config(\n    config_path: str,\n    overrides: dict[str, Any] | None = None,\n) -&gt; Module:\n    \"\"\"\n    Instantiates a module from a jsonnet config file.\n\n    Args:\n        config_path: The path to the jsonnet config file.\n        overrides: A dictionary of overrides to apply to the config.\n\n    Returns:\n        The instantiated module.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import instantiate_from_config\n        &gt;&gt;&gt; eval_setup = instantiate_from_config(\"aio\")\n    \"\"\"\n    resolved_config_path = ConfigNameResolver()(config_path)\n\n    if resolved_config_path is None:\n        msg = f'Config name \"{config_path}\" not found in the specified path nor in the preset config directories.'\n        raise ValueError(msg)\n\n    config = json.loads(_jsonnet.evaluate_file(resolved_config_path))\n    module_class = getattr(flexeval, config[\"class_path\"])\n\n    parser = ArgumentParser(parser_mode=\"jsonnet\")\n    parser.add_argument(\"--module\", type=module_class, required=True, enable_path=True)\n\n    args_to_parse = [\"--module\", resolved_config_path]\n    overrides = overrides or {}\n    for key, value in overrides.items():\n        args_to_parse += [f\"--module.{key}\", str(value)]\n\n    args = parser.parse_args(args_to_parse)\n    instantiated_config = parser.instantiate_classes(args)\n    return instantiated_config.module\n</code></pre>"},{"location":"how_to/","title":"How to","text":"<ul> <li>Configure Few-Shot Examples</li> <li>Evaluate with LLM Judges</li> <li>Implement Your Own Module</li> </ul>"},{"location":"how_to/configure_few_shot_examples/","title":"Configure Few-Shot Examples","text":"<p>The logic to configure few-shot examples is implemented in the <code>FewShotGenerator</code> classes.</p>"},{"location":"how_to/configure_few_shot_examples/#change-the-number-of-shots","title":"Change the number of shots","text":""},{"location":"how_to/configure_few_shot_examples/#overriding-the-arguments","title":"Overriding the arguments","text":"<p>Most presets use <code>RandomFewShotGenerator</code> to generate few-shot examples. From command line, the number of shots can be changed using the <code>--eval_setup.few_shot_generator.num_shots</code> argument.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa\" \\\n  --eval_setup.few_shot_generator.num_shots 3\n</code></pre>"},{"location":"how_to/configure_few_shot_examples/#editing-the-configuration-file","title":"Editing the configuration file","text":"<p>First, save the configuration file to a local file.</p> <pre><code>flexeval_presets commonsense_qa &gt; commonsense_qa_custom.jsonnet\n</code></pre> <p>Then, edit the <code>num_shots</code> field in the <code>few_shot_generator</code> section.</p> <p>Finally, run the evaluation with the custom configuration file.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa_custom.jsonnet\" \n</code></pre>"},{"location":"how_to/configure_few_shot_examples/#change-the-sampling-method","title":"Change the sampling method","text":"<p>Sometime, you may want to change the sampling method for few-shot examples. In that case, you can change the <code>FewShotGenerator</code> class.</p> <p>For example, see the config file from <code>flexeval_presets twitter_sentiment</code>. It uses <code>BalancedFewShotGenerator</code> to generate few-shot examples. This classes samples examples so that the number of labels (the first element of the <code>references</code> field) is balanced.</p> <p>See API Reference for other available classes.</p>"},{"location":"how_to/evaluate_with_llm_judges/","title":"Evaluate with LLM Judges","text":"<p>Evaluation of chat model is difficult since the response is open-ended and manually evaluating the responses is not scalable. One solution is to use a LLM as an evaluator.</p>"},{"location":"how_to/evaluate_with_llm_judges/#single-judge-evaluation","title":"Single Judge Evaluation","text":"<p>First, we need to generate responses from the chat model. In this example, we use ChatGPT with the following command:</p> <pre><code>export OPENAI_API_KEY=\"YOUR_API_KEY\"\n\nflexeval_lm \\\n  --language_model OpenAIChatAPI \\\n  --language_model.model \"gpt-3.5-turbo\" \\\n  --eval_setup \"mt-en\" \\\n  --save_dir \"results/mt-en_gpt3.5-turbo\"\n</code></pre> <p>Now you have the model outputs in <code>results/mt-en-gpt3.5-turbo/outputs.jsonl</code>.</p> <p>Let's evaluate the responses with GPT4. The LLM evaluation is implemented as a <code>Metric</code> class and we will use a preset metric named <code>assistant_eval_en_single_turn</code>. You can check its configuration with the following command:</p> <pre><code>flexeval_presets assistant_eval_en_single_turn\n</code></pre> <p>In this metric, GPT4 is asked to rate the responses with the score from 1 to 10. The score is extracted as the last digit found in the evaluator's output.</p> <p>Tip</p> <p>To take a closer look at the prompt template, combine pipeline with <code>jsonnet</code> and <code>jq</code>:</p> <pre><code>flexeval_presets assistant_eval_en_single_turn | jsonnet - | jq -r \".init_args.prompt_template.init_args.template\"\n</code></pre> <p>Perform automatic evaluation with GPT4 with the following command:</p> <pre><code>flexeval_file \\\n   --eval_file \"results/mt-en-gpt3.5-turbo/outputs.jsonl\" \\\n   --metrics \"assistant_eval_en_single_turn\" \\\n   --save_dir \"results/mt-en_gpt3.5-turbo/eval_by_gpt\"\n</code></pre> <p>\u2615\ufe0f It may take a while to finish the evaluation...</p> <p>By hitting <code>cat results/mt-en-gpt3.5-turbo/eval_by_gpt/metrics.json</code>, you can see the evaluation result like <code>{\"llm_score\": 7.795}</code>. The evaluation for each response is stored in <code>results/mt-en-gpt3.5-turbo/eval_by_gpt/outputs.jsonl</code>.</p> <p>You can check the output of the evaluator LLM in the <code>llm_score_output</code> field.</p> <pre><code>head -n 1 results/mt-en-gpt3.5-turbo/eval_by_gpt/outputs.jsonl | jq -r \".llm_output\"\n</code></pre> <p>Info</p> <p><code>flexeval_file</code> just runs the same evaluation as <code>flexeval_lm</code> but with the given file. So, theoretically, you can perform the same evaluation with <code>flexeval_lm</code> in one go:</p> <pre><code>flexeval_lm \\\n  --language_model OpenAIChatAPI \\\n  --language_model.model \"gpt-3.5-turbo\" \\\n  --eval_setup \"mt-en\" \\\n  --metrics+=\"assistant_eval_en_single_turn\" \\\n  --save_dir \"results/mt-en_gpt3.5-turbo\"\n</code></pre> <p>Yet, we recommend separate the response generation (<code>flexeval_lm</code>) and evaluation (<code>flexeval_file</code>) so that you don't lost the response by some errors in the evaluation process.</p>"},{"location":"how_to/evaluate_with_llm_judges/#pairwise-judge-evaluation","title":"Pairwise Judge Evaluation","text":"<p>Sometimes, evaluating chat models individually cannot capture a subtle difference between models. In such cases, pairwise evaluation is useful.</p> <p>The overview of process is generating the responses using <code>flexeval_lm</code> and evaluating them with <code>flexeval_pairwise</code>.</p> <p>In this example, we will compare the responses from GPT3.5 and GPT-4o.</p> <p>First, generate the responses with GPT3.5. You can skip this if you have already generated the responses.</p> <pre><code>export OPENAI_API_KEY=\"YOUR_API_KEY\"\n\nflexeval_lm \\\n  --language_model OpenAIChatAPI \\\n  --language_model.model \"gpt-3.5-turbo\" \\\n  --eval_setup \"mt-en\" \\\n  --save_dir \"results/mt-en_gpt3.5-turbo\"\n</code></pre> <p>Generate the responses with GPT-4o.</p> <pre><code>flexeval_lm \\\n  --language_model OpenAIChatAPI \\\n  --language_model.model \"gpt-4o\" \\\n  --eval_setup \"mt-en\" \\\n  --save_dir \"results/mt-en_gpt-4o\"\n</code></pre> <p>Now, compare the responses with GPT-4.</p> <pre><code>flexeval_pairwise \\\n  --lm_output_paths.gpt_3_5 \"results/mt-en_gpt3.5-turbo/outputs.jsonl\"  \\\n  --lm_output_paths.gpt_4o \"results/mt-en_gpt-4o/outputs.jsonl\"  \\\n  --judge \"assistant_judge_en_single_turn\" \\\n  --save_dir \"results/mt-en_gpt3.5_vs_gpt4o\"\n</code></pre> <p>\u2615\ufe0f It may take a while to finish the evaluation...</p> <p>You can see the result in <code>results/mt-en_gpt3.5_vs_gpt4o/scores.json</code>.</p> <pre><code>{\n    \"win_rate\": {\n        \"gpt_4o\": 85.3125,\n        \"gpt_3_5\": 14.6875\n    },\n    \"bradley_terry\": {\n        \"gpt_4o\": 1152.8129577165919,\n        \"gpt_3_5\": 847.1870422834081\n    }\n}\n</code></pre> <p>The <code>win_rate</code> shows the percentage of wins of each model. The <code>bradley_terry</code> shows the Bradley-Terry score of each model.</p>"},{"location":"how_to/evaluate_with_llm_judges/#whats-next","title":"What's Next?","text":"<ul> <li>To define your own LLM evaluator, see Configuration Guide.</li> </ul>"},{"location":"how_to/implement_your_own_module/","title":"Implement Your Own Module","text":"<p>FlexEval is designed to be highly extensible, allowing you to implement your own modules without changing the core codebase. Whether you want to add new language models, evaluation setups, prompt templates, or metrics, FlexEval provides a flexible framework for you to do so.</p> <p>In this guide, we'll walk through the process of implementing your own module in FlexEval.</p>"},{"location":"how_to/implement_your_own_module/#step-1-identify-the-module-type","title":"Step 1: Identify the Module Type","text":"<p>First, determine which type of module you want to implement. FlexEval supports the modules listed in the API Reference.</p> <p>For this guide, we'll focus on implementing a new <code>Metric</code>.</p>"},{"location":"how_to/implement_your_own_module/#step-2-create-your-module","title":"Step 2: Create Your Module","text":"<p>Create a new Python file and inherit the appropriate base class provided by <code>flexeval</code>.</p> <pre><code>mkdir custom_modules\ntouch custom_modules/__init__.py\ntouch custom_modules/my_custom_metric.py\n</code></pre> <p>Note</p> <p>Make sure the module is importable from your program by adding an <code>__init__.py</code> file in the directory.</p> <p>All you need to do is implement the required methods based on the module type. Let's create a simple custom metric that calculates the length ratio of the generated text to the reference text.</p> <p><code>custom_modules/my_custom_metric.py</code>:</p> <pre><code>from flexeval import Metric, MetricResult\n\n\nclass MyCustomMetric(Metric):\n    \"\"\"\n    My custom metric implementation.\n    This class reports the length ratio of the generated text to the reference text.\n    \"\"\"\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        task_inputs_list: list[dict[str, str]],\n        references_list: list[list[str]],\n    ) -&gt; MetricResult:\n        length_ratios = [\n            len(lm_output) / len(references[0])  # Assuming a single reference\n            for lm_output, references in zip(lm_outputs, references_list)\n        ]\n\n        return MetricResult(\n            {\"length_ratio\": sum(length_ratios) / len(length_ratios)},\n            instance_details=[{\"length_ratio\": ratio} for ratio in length_ratios],\n        )\n</code></pre>"},{"location":"how_to/implement_your_own_module/#step-3-specify-the-module-in-the-configuration","title":"Step 3: Specify the Module in the Configuration","text":"<p>Now you can use your custom metric to run evaluations.</p> <p>It can be specified in the configuration file as follows:</p> <pre><code>{\n  class_path: 'Generation',\n  init_args: {\n    metrics: [\n      {class_path: \"custom_modules.my_custom_metric.MyCustomMetric\" }\n    ]\n  }\n}\n</code></pre> <p>Note</p> <p>Make sure the <code>class_path</code> is a full import path to the module. Unlike the core modules, the program cannot locate your custom module without the full path.</p> <p>Or add it from the command line:</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa\" \\\n  --eval_setup.metrics+=\"custom_modules.my_custom_metric.MyCustomMetric\"\n</code></pre> <p>Info</p> <p>The argument <code>--eval_setup.metrics</code> can take a list of metric classes. You can use <code>+=</code> to add your custom metric to the existing metrics in the config.</p> <p>You will see the new metric <code>length_ratio</code> in the results.</p> <p>Now you've successfully implemented your own module \ud83c\udf89.</p> <p>If you believe your module would be useful for others, consider contributing it to the official repository.</p>"},{"location":"preset_configs/","title":"Preset Configs","text":"<p>You can check the config using the following command: <pre><code>flexeval_presets &lt;config_name&gt;\n</code></pre></p>"},{"location":"preset_configs/#evalsetup","title":"EvalSetup","text":""},{"location":"preset_configs/#code_chat","title":"code_chat","text":"<ul> <li>mbpp_chat</li> </ul>"},{"location":"preset_configs/#code_generation","title":"code_generation","text":"<ul> <li>jhumaneval</li> <li>jhumaneval_tab_indent</li> <li>mbpp</li> <li>mbpp_tab_indent</li> <li>openai_humaneval</li> <li>openai_humaneval_tab_indent</li> </ul>"},{"location":"preset_configs/#en_chat","title":"en_chat","text":"<ul> <li>mt-en</li> <li>vicuna-en</li> </ul>"},{"location":"preset_configs/#en_generation","title":"en_generation","text":"<ul> <li>babi</li> <li>commonsense_qa</li> <li>gsm8k</li> <li>squad_v1</li> <li>trivia_qa</li> <li>twitter_sentiment</li> </ul>"},{"location":"preset_configs/#en_multiple_choice","title":"en_multiple_choice","text":"<ul> <li>arc_challenge</li> <li>arc_easy</li> <li>commonsense_qa_mc</li> <li>hellaswag</li> <li>openbookqa</li> <li>piqa</li> <li>xwinograd_en</li> </ul>"},{"location":"preset_configs/#en_perplexity","title":"en_perplexity","text":"<ul> <li>tiny_shakespeare</li> </ul>"},{"location":"preset_configs/#ja_chat","title":"ja_chat","text":"<ul> <li>aio_chat</li> <li>elyza_tasks_100</li> <li>mgsm_ja_chat</li> <li>mt-ja</li> <li>rakuda-v2-ja</li> <li>vicuna-ja</li> </ul>"},{"location":"preset_configs/#ja_generation","title":"ja_generation","text":"<ul> <li>aio</li> <li>jcommonsenseqa</li> <li>jnli</li> <li>jsquad</li> <li>mgsm_ja</li> <li>wrime_pos_neg</li> <li>xlsum_ja</li> </ul>"},{"location":"preset_configs/#ja_multiple_choice","title":"ja_multiple_choice","text":"<ul> <li>jcommonsenseqa_mc</li> <li>xwinograd_ja</li> </ul>"},{"location":"preset_configs/#translation","title":"translation","text":"<ul> <li>wmt20_en_ja</li> <li>wmt20_ja_en</li> </ul>"},{"location":"preset_configs/#translation_chat","title":"translation_chat","text":"<ul> <li>wmt20_en_ja_chat</li> <li>wmt20_ja_en_chat</li> </ul>"},{"location":"preset_configs/#metric","title":"Metric","text":"<ul> <li>assistant_eval_en_single_turn</li> <li>assistant_eval_ja_single_turn</li> <li>elyza_tasks_100_eval</li> </ul>"},{"location":"preset_configs/#pairwisejudge","title":"PairwiseJudge","text":"<ul> <li>assistant_judge_en_single_turn</li> <li>assistant_judge_ja_single_turn</li> </ul>"},{"location":"preset_configs/EvalSetup/","title":"EvalSetup","text":""},{"location":"preset_configs/EvalSetup/#code_chat","title":"code_chat","text":"<ul> <li>mbpp_chat</li> </ul>"},{"location":"preset_configs/EvalSetup/#code_generation","title":"code_generation","text":"<ul> <li>jhumaneval</li> <li>jhumaneval_tab_indent</li> <li>mbpp</li> <li>mbpp_tab_indent</li> <li>openai_humaneval</li> <li>openai_humaneval_tab_indent</li> </ul>"},{"location":"preset_configs/EvalSetup/#en_chat","title":"en_chat","text":"<ul> <li>mt-en</li> <li>vicuna-en</li> </ul>"},{"location":"preset_configs/EvalSetup/#en_generation","title":"en_generation","text":"<ul> <li>babi</li> <li>commonsense_qa</li> <li>gsm8k</li> <li>squad_v1</li> <li>trivia_qa</li> <li>twitter_sentiment</li> </ul>"},{"location":"preset_configs/EvalSetup/#en_multiple_choice","title":"en_multiple_choice","text":"<ul> <li>arc_challenge</li> <li>arc_easy</li> <li>commonsense_qa_mc</li> <li>hellaswag</li> <li>openbookqa</li> <li>piqa</li> <li>xwinograd_en</li> </ul>"},{"location":"preset_configs/EvalSetup/#en_perplexity","title":"en_perplexity","text":"<ul> <li>tiny_shakespeare</li> </ul>"},{"location":"preset_configs/EvalSetup/#ja_chat","title":"ja_chat","text":"<ul> <li>aio_chat</li> <li>elyza_tasks_100</li> <li>mgsm_ja_chat</li> <li>mt-ja</li> <li>rakuda-v2-ja</li> <li>vicuna-ja</li> </ul>"},{"location":"preset_configs/EvalSetup/#ja_generation","title":"ja_generation","text":"<ul> <li>aio</li> <li>jcommonsenseqa</li> <li>jnli</li> <li>jsquad</li> <li>mgsm_ja</li> <li>wrime_pos_neg</li> <li>xlsum_ja</li> </ul>"},{"location":"preset_configs/EvalSetup/#ja_multiple_choice","title":"ja_multiple_choice","text":"<ul> <li>jcommonsenseqa_mc</li> <li>xwinograd_ja</li> </ul>"},{"location":"preset_configs/EvalSetup/#translation","title":"translation","text":"<ul> <li>wmt20_en_ja</li> <li>wmt20_ja_en</li> </ul>"},{"location":"preset_configs/EvalSetup/#translation_chat","title":"translation_chat","text":"<ul> <li>wmt20_en_ja_chat</li> <li>wmt20_ja_en_chat</li> </ul>"},{"location":"preset_configs/EvalSetup/code_chat/","title":"Code chat","text":""},{"location":"preset_configs/EvalSetup/code_chat/#mbpp_chat","title":"mbpp_chat","text":"<p>Mostly Basic Python Problems (MBPP) is a dataset of crowd-sourced programming problems. This is a evaluation setup for chat LLMs.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Program Synthesis with Large Language Models <pre><code>local dataset_base_args = {\n  class_path: 'HFChatDataset',\n  init_args: {\n    path: 'mbpp',\n    subset: 'sanitized',\n    input_template: std.stripChars(|||\n      Generate a Python function that satisfies the following question and test cases.\n      ## Question\n      {{ prompt }}\n      ## Test cases\n      ```python\n      {{ test_list | join('\\n') }}\n      ```\n    |||, '\\n'),\n  },\n};\n\n{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'test', reference_list_template: '{{ test_list | join(\"\\n\") }}' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'prompt', reference_template: '```python\\n{{ code }}\\n```' } },\n        num_shots: 3,\n      },\n    },\n    metrics: [\n      { class_path: 'CodeEval', init_args: { processor: { class_path: 'RegexExtractor', init_args: { pattern: '```python\\n(.*?)\\n```' } } } },\n    ],\n    gen_kwargs: { max_new_tokens: 512 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/code_generation/","title":"Code generation","text":""},{"location":"preset_configs/EvalSetup/code_generation/#jhumaneval","title":"jhumaneval","text":"<p>Zero-shot Python code generation task in Japanese.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>LLM \u306f\u65e5\u672c\u8a9e\u8ffd\u52a0\u5b66\u7fd2\u306b\u3088\u308a\u8a00\u8a9e\u9593\u77e5\u8b58\u8ee2\u79fb\u3092\u8d77\u3053\u3059\u306e\u304b\uff1f <pre><code>{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFGenerationDataset',\n      init_args: {\n        path: 'kogi-jwu/jhumaneval',\n        split: 'test',\n        reference_template: '{{ test }}\\n\\ncheck({{ entry_point }})\\n',\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: '{{ prompt }}',\n      },\n    },\n    metrics: [\n      { class_path: 'CodeEval', init_args: { code_template: '{{ prompt }}{{ lm_output }}' } },\n    ],\n    gen_kwargs: { max_new_tokens: 512, stop_sequences: ['\\nclass', '\\ndef', '\\n#', '\\n@', '\\nprint', '\\nif', '\\n```'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/code_generation/#jhumaneval_tab_indent","title":"jhumaneval_tab_indent","text":"<p>Zero-shot Python code generation task in Japanese.</p> <p>This is a version of jhumaneval preprocessed to replace indentation spaces with tabs. Some models (e.g., Llama) seems to have trouble with spaces in the prompt. <pre><code>local original_config = import './jhumaneval.jsonnet';\n\noriginal_config {\n  init_args+: {\n    eval_dataset+: {\n      init_args+: {\n        reference_template: '{{ test | replace(\"    \", \"\\t\") }}\\n\\ncheck({{ entry_point }})\\n',\n      },\n    },\n    prompt_template+: {\n      init_args+: {\n        template: \"{{ prompt | replace('    ', '\\t') }}\",\n      },\n    },\n    metrics: [\n      { class_path: 'CodeEval', init_args: { code_template: '{{ prompt | replace(\"    \", \"\\t\") }}{{ lm_output }}' } },\n    ],\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/EvalSetup/code_generation/#mbpp","title":"mbpp","text":"<p>Mostly Basic Python Problems (MBPP) is a dataset of crowd-sourced programming problems.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Program Synthesis with Large Language Models <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'mbpp',\n    subset: 'sanitized',\n    reference_list_template: '{{ test_list }}',\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'test' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'prompt' } },\n        num_shots: 3,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          {% for item in few_shot_data %}\n          ## Question\n          {{ item.prompt }}\n          ## Test cases\n          ```python\n          {{ item.test_list | join('\\n') }}\n          ```\n          ## Code\n          ```python\n          {{ item.code }}\n          ```\n          {% endfor %}\n          ## Question\n          {{ prompt }}\n          ## Test cases\n          ```python\n          {{ test_list | join('\\n') }}\n          ```\n          ## Code\n          ```python\n        |||,\n      },\n    },\n    metrics: [\n      { class_path: 'CodeEval' },\n    ],\n    gen_kwargs: { max_new_tokens: 512, stop_sequences: ['```'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/code_generation/#mbpp_tab_indent","title":"mbpp_tab_indent","text":"<p>Mostly Basic Python Problems (MBPP) is a dataset of crowd-sourced programming problems.</p> <p>This is a version of openai_humaneval preprocessed to replace indentation spaces with tabs. Some models (e.g., Llama) seems to have trouble with spaces in the prompt. <pre><code>local original_config = import './mbpp.jsonnet';\n\noriginal_config {\n  init_args+: {\n    prompt_template+: {\n      init_args+: {\n        template: |||\n          {% for item in few_shot_data %}\n          ## Question\n          {{ item.prompt }}\n          ## Test cases\n          ```python\n          {{ item.test_list | join('\\n') }}\n          ```\n          ## Code\n          ```python\n          {{ item.code | replace('    ', '\\t') }}\n          ```\n          {% endfor %}\n          ## Question\n          {{ prompt }}\n          ## Test cases\n          ```python\n          {{ test_list | join('\\n') }}\n          ```\n          ## Code\n          ```python\n        |||,\n      },\n    },\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/EvalSetup/code_generation/#openai_humaneval","title":"openai_humaneval","text":"<p>Zero-shot Python code generation task developed by OpenAI.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Evaluating Large Language Models Trained on Code <pre><code>{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFGenerationDataset',\n      init_args: {\n        path: 'openai_humaneval',\n        split: 'test',\n        reference_template: '{{ test }}\\n\\ncheck({{ entry_point }})\\n',\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: '{{ prompt }}',\n      },\n    },\n    metrics: [\n      { class_path: 'CodeEval', init_args: { code_template: '{{ prompt }}{{ lm_output }}' } },\n    ],\n    gen_kwargs: { max_new_tokens: 512, stop_sequences: ['\\nclass', '\\ndef', '\\n#', '\\n@', '\\nprint', '\\nif', '\\n```'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/code_generation/#openai_humaneval_tab_indent","title":"openai_humaneval_tab_indent","text":"<p>Zero-shot Python code generation task developed by OpenAI.</p> <p>This is a version of openai_humaneval preprocessed to replace indentation spaces with tabs. Some models (e.g., Llama) seems to have trouble with spaces in the prompt. <pre><code>local original_config = import './openai_humaneval.jsonnet';\n\noriginal_config {\n  init_args+: {\n    eval_dataset+: {\n      init_args+: {\n        reference_template: '{{ test | replace(\"    \", \"\\t\") }}\\n\\ncheck({{ entry_point }})\\n',\n      },\n    },\n    prompt_template+: {\n      init_args+: {\n        template: '{{ prompt | replace(\"    \", \"\\t\") }}',\n      },\n    },\n    metrics: [\n      { class_path: 'CodeEval', init_args: { code_template: '{{ prompt | replace(\"    \", \"\\t\") }}{{ lm_output }}' } },\n    ],\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/EvalSetup/en_chat/","title":"En chat","text":""},{"location":"preset_configs/EvalSetup/en_chat/#mt-en","title":"mt-en","text":"<p>Multi-Turn Benchmark for large language models.</p> <p>References:</p> <ul> <li>Data Source</li> <li>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena <pre><code>{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: {\n      class_path: 'ChatbotBench',\n      init_args: {\n        path_or_name: 'mt-en',\n        ref_path_or_name: 'mt-en-ref-gpt4',\n      },\n    },\n    metrics: [\n      { class_path: 'OutputLengthStats' },\n    ],\n    gen_kwargs: { max_new_tokens: 1024 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_chat/#vicuna-en","title":"vicuna-en","text":"<p>Vicuna Benchmark for large language models.</p> <p>References:</p> <ul> <li>Data Source <pre><code>{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: {\n      class_path: 'ChatbotBench',\n      init_args: {\n        path_or_name: 'vicuna-en',\n        ref_path_or_name: 'vicuna-en-ref-gpt4',\n      },\n    },\n    metrics: [\n      { class_path: 'OutputLengthStats' },\n    ],\n    gen_kwargs: { max_new_tokens: 1024 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_generation/","title":"En generation","text":""},{"location":"preset_configs/EvalSetup/en_generation/#babi","title":"babi","text":"<p>Synthetic question answering dataset with reasoning questions.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'Muennighoff/babi',\n    reference_template: '{{ answer }}',\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 3,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          {% for item in few_shot_data %}\n          Passage: {{ item.passage | trim }}\n          Question: {{ item.question }}\n          Answer: \"{{ item.references[0] }}\"\n          {% endfor %}\n          Passage: {{ passage | trim }}\n          Question: {{ question }}\n        ||| + 'Answer: \"',\n      },\n    },\n    metrics: [\n      { class_path: 'CharF1' },\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 32, stop_sequences: ['\"'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_generation/#commonsense_qa","title":"commonsense_qa","text":"<p>CommonsenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers. This is a setup for generating answers based on the choices provided.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'tau/commonsense_qa',\n    reference_template: '{% set answer_index = choices.label.index(answerKey) %}{{ choices.text[answer_index] }}',\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 2,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          Choose the correct answer from the choices.\n          {% for item in few_shot_data %}\n          Choices:\n          0. \"{{ item.choices.text[0] }}\"\n          1. \"{{ item.choices.text[1] }}\"\n          2. \"{{ item.choices.text[2] }}\"\n          3. \"{{ item.choices.text[3] }}\"\n          4. \"{{ item.choices.text[4] }}\"\n          Question: {{ item.question }}\n          Answer: \"{{ item.references[0] }}\"\n          {% endfor %}\n          Choices:\n          0. \"{{ choices.text[0] }}\"\n          1. \"{{ choices.text[1] }}\"\n          2. \"{{ choices.text[2] }}\"\n          3. \"{{ choices.text[3] }}\"\n          4. \"{{ choices.text[4] }}\"\n          Question: {{question}}\n        ||| + 'Answer: \"',\n      },\n    },\n    metrics: [\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 40, stop_sequences: ['\"'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_generation/#gsm8k","title":"gsm8k","text":"<p>GSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.</p> <p>References:</p> <ul> <li>[Hugging Face Dataset](https://huggingface.co/datasets/gsm8k]</li> <li>Training Verifiers to Solve Math Word Problems <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'gsm8k',\n    subset: 'main',\n    reference_template: '{{ answer | regex_replace(\"&lt;&lt;.*?&gt;&gt;\", \"\") }}',\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'test' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 4,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          {% for item in few_shot_data %}\n          Q: {{ item.question }}\n          A: {{ item.references[0] }}\n          {% endfor %}\n          Q: {{ question }}\n        ||| + 'A:',\n      },\n    },\n    metrics: [\n      { class_path: 'ExactMatch', init_args: { processor: { class_path: 'RegexExtractor', init_args: { pattern: '-?[0-9.,]+' } } } },\n    ],\n    gen_kwargs: { max_new_tokens: 256, stop_sequences: ['Q:'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_generation/#squad_v1","title":"squad_v1","text":"<p>Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>SQuAD: 100,000+ Questions for Machine Comprehension of Text <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'rajpurkar/squad',\n    reference_list_template: '{{ answers.text }}',\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 2,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n\n        template: |||\n          {% for item in few_shot_data %}\n          Context: {{ item.context | trim }}\n          Question: {{ item.question }}\n          Answer: \"{{ item.references[0] }}\"\n          {% endfor %}\n          Context: {{ context | trim }}\n          Question: {{ question }}\n        ||| + 'Answer: \"',\n      },\n    },\n    metrics: [\n      { class_path: 'CharF1' },\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 32, stop_sequences: ['\"'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_generation/#trivia_qa","title":"trivia_qa","text":"<p>TriviaqQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaqQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'trivia_qa',\n    subset: 'rc.nocontext',\n    reference_list_template: '{{ answer.aliases }}',\n  },\n};\n\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 0,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n\n        template: |||\n          {% for item in few_shot_data %}\n          Question: {{ item.question }}\n          Answer: \"{{ item.references[0] }}\"\n          {% endfor %}\n          Question: {{ question }}\n        ||| + 'Answer: \"',\n      },\n    },\n    metrics: [\n      { class_path: 'CharF1' },\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 32, stop_sequences: ['\"'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_generation/#twitter_sentiment","title":"twitter_sentiment","text":"<p>TSATC: Twitter Sentiment Analysis Training Corpus. This dataset is a preprocessed version of the original dataset. See the hugging face dataset page for more information.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Twitter Sentiment Analysis Training Corpus (Dataset) <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'carblacac/twitter-sentiment-analysis',\n    reference_template: \"{{ ['Positive', 'Negative'][feeling] }}\",\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'test' } },\n    few_shot_generator: {\n      class_path: 'BalancedFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 4,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          Classify the sentiment of the following tweet.\n          {% for item in few_shot_data %}\n          Tweet: {{ item.text }}\n          Sentiment: `{{ item.references[0] }}`\n          {% endfor %}\n          Tweet: {{ text }}\n        ||| + 'Sentiment: `',\n      },\n    },\n    metrics: [\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 8, stop_sequences: ['`'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/","title":"En multiple choice","text":""},{"location":"preset_configs/EvalSetup/en_multiple_choice/#arc_challenge","title":"arc_challenge","text":"<p>The ARC dataset contains 7,787 genuine grade-school level, multiple-choice science questions, assembled to encourage research in advanced question-answering. The dataset is partitioned into a Challenge Set and an Easy Set, and this is the Challenge Set.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge <pre><code>local dataset_base_args = {\n  path: 'allenai/ai2_arc',\n  subset: 'ARC-Challenge',\n  choices_templates: [\n    '{% if choices.text | length &gt; 0 %}{{ choices.text[0] }}{% endif %}',\n    '{% if choices.text | length &gt; 1 %}{{ choices.text[1] }}{% endif %}',\n    '{% if choices.text | length &gt; 2 %}{{ choices.text[2] }}{% endif %}',\n    '{% if choices.text | length &gt; 3 %}{{ choices.text[3] }}{% endif %}',\n    '{% if choices.text | length &gt; 4 %}{{ choices.text[4] }}{% endif %}',\n  ],\n  # answerKey is one of A, B, C, D, E, 1, 2, 3, 4\n  answer_index_template: '{% if answerKey == \"A\" %}0{% elif answerKey == \"B\" %}1{% elif answerKey == \"C\" %}2{% elif answerKey == \"D\" %}3{% elif answerKey == \"E\" %}3{% else %}{{ answerKey | int - 1 }}{% endif %}',\n  whitespace_before_choices: true,\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'test' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 4,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n\n        template: |||\n          {% for item in few_shot_data %}\n          Question: {{ item.question }}\n          Answer:{{ item.choices[item.answer_index] }}\n          {% endfor %}\n          Question: {{ question }}\n        ||| + 'Answer:',\n      },\n    },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/#arc_easy","title":"arc_easy","text":"<p>The ARC dataset contains 7,787 genuine grade-school level, multiple-choice science questions, assembled to encourage research in advanced question-answering. The dataset is partitioned into a Challenge Set and an Easy Set, and this is the Easy Set.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge <pre><code>local dataset_base_args = {\n  path: 'allenai/ai2_arc',\n  subset: 'ARC-Easy',\n  choices_templates: [\n    '{% if choices.text | length &gt; 0 %}{{ choices.text[0] }}{% endif %}',\n    '{% if choices.text | length &gt; 1 %}{{ choices.text[1] }}{% endif %}',\n    '{% if choices.text | length &gt; 2 %}{{ choices.text[2] }}{% endif %}',\n    '{% if choices.text | length &gt; 3 %}{{ choices.text[3] }}{% endif %}',\n    '{% if choices.text | length &gt; 4 %}{{ choices.text[4] }}{% endif %}',\n  ],\n  # answerKey is one of A, B, C, D, E, 1, 2, 3, 4\n  answer_index_template: '{% if answerKey == \"A\" %}0{% elif answerKey == \"B\" %}1{% elif answerKey == \"C\" %}2{% elif answerKey == \"D\" %}3{% elif answerKey == \"E\" %}3{% else %}{{ answerKey | int - 1 }}{% endif %}',\n  whitespace_before_choices: true,\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'test' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 4,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n\n        template: |||\n          {% for item in few_shot_data %}\n          Question: {{ item.question }}\n          Answer:{{ item.choices[item.answer_index] }}\n          {% endfor %}\n          Question: {{ question }}\n        ||| + 'Answer:',\n      },\n    },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/#commonsense_qa_mc","title":"commonsense_qa_mc","text":"<p>CommonsenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers. This is a setup for multiple choice where the model chooses the correct answer based on the log-probabilities of the choices.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge <pre><code>local dataset_base_args = {\n  path: 'tau/commonsense_qa',\n  choices_templates: ['{{ choices.text[0] }}', '{{ choices.text[1] }}', '{{ choices.text[2] }}', '{{ choices.text[3] }}', '{{ choices.text[4] }}'],\n  answer_index_template: '{% if answerKey == \"A\" %}0{% elif answerKey == \"B\" %}1{% elif answerKey == \"C\" %}2{% elif answerKey == \"D\" %}3{% elif answerKey == \"E\" %}4{% endif %}',\n  whitespace_before_choices: true,\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'validation' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 4,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n\n        template: |||\n          {% for item in few_shot_data %}\n          Question: {{ item.question }}\n          Answer:{{ item.choices[item.answer_index] }}\n          {% endfor %}\n          Question: {{ question }}\n        ||| + 'Answer:',\n      },\n    },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/#hellaswag","title":"hellaswag","text":"<p>Hellaswag is a dataset for physically situated commonsense reasoning. The dataset is constructed through adversarial filtering to make it challenging for models to perform well.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>HellaSwag: Can a Machine Really Finish Your Sentence? <pre><code>local dataset_base_args = {\n  path: 'Rowan/hellaswag',\n  choices_templates: ['{{ endings[0] }}', '{{ endings[1] }}', '{{ endings[2] }}', '{{ endings[3] }}'],\n  answer_index_template: '{{ label }}',\n  whitespace_before_choices: true,\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'validation' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 4,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          {% for item in few_shot_data %}\n          {{ item.ctx }}{{ item.choices[item.answer_index] }}\n          {% endfor %}\n        ||| + '{{ ctx }}',\n      },\n    },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/#openbookqa","title":"openbookqa","text":"<p>OpenBookQA contains questions that require multi-step reasoning, use of additional common and commonsense knowledge, and rich text comprehension.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering <pre><code>local dataset_base_args = {\n  path: 'allenai/openbookqa',\n  subset: 'main',\n  choices_templates: ['{{ choices.text[0] }}', '{{ choices.text[1] }}', '{{ choices.text[2] }}', '{{ choices.text[3] }}'],\n  answer_index_template: '{% if answerKey == \"A\" %}0{% elif answerKey == \"B\" %}1{% elif answerKey == \"C\" %}2{% elif answerKey == \"D\" %}3{% endif %}',\n  whitespace_before_choices: true,\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'test' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 4,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n\n        template: |||\n          {% for item in few_shot_data %}\n          Question: {{ item.question_stem }}\n          Answer:{{ item.choices[item.answer_index] }}\n          {% endfor %}\n          Question: {{ question_stem }}\n        ||| + 'Answer:',\n      },\n    },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/#piqa","title":"piqa","text":"<p>The PIQA dataset introduces the task of physical commonsense reasoning and a corresponding benchmark dataset</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>PIQA: Reasoning about Physical Commonsense in Natural Language <pre><code>local dataset_base_args = {\n  path: 'ybisk/piqa',\n  choices_templates: ['{{ sol1 }}', '{{ sol2 }}'],\n  answer_index_template: '{{ label }}',\n  whitespace_before_choices: true,\n  dataset_kwargs: { trust_remote_code: true },\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'validation' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 4,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          {% for item in few_shot_data %}\n          {{ item.goal }}{{ item.choices[item.answer_index] }}\n          {% endfor %}\n        ||| + '{{ goal }}',\n      },\n    },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/#xwinograd_en","title":"xwinograd_en","text":"<p>XWinograd is a multilingual collection of Winograd Schemas in six languages that can be used for evaluation of cross-lingual commonsense reasoning capabilities. This is an English subset of the dataset.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>It\u2019s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning <pre><code>{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: {\n        path: 'Muennighoff/xwinograd',\n        subset: 'en',\n        split: 'test',\n        choices_templates: [\n          '{{ option1 }}{{ sentence.split(\"_\")[1] }}',\n          '{{ option2 }}{{ sentence.split(\"_\")[1] }}',\n        ],\n        answer_index_template: '{{ answer | int - 1 }}',\n        input_templates: { context: '{{ sentence.split(\"_\")[0] }}' },\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: '{{ context }}',\n      },\n    },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_perplexity/","title":"En perplexity","text":""},{"location":"preset_configs/EvalSetup/en_perplexity/#tiny_shakespeare","title":"tiny_shakespeare","text":"<p>40,000 lines of Shakespeare from a variety of Shakespeare's plays. Featured in Andrej Karpathy's blog post 'The Unreasonable Effectiveness of Recurrent Neural Networks'.</p> <p>References:</p> <ul> <li>Hugging Face Dataset <pre><code>{\n  class_path: 'Perplexity',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFTextDataset',\n      init_args: {\n        path: 'karpathy/tiny_shakespeare',\n        split: 'test',\n        text_template: '{{ text }}',\n      },\n    },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_chat/","title":"Ja chat","text":""},{"location":"preset_configs/EvalSetup/ja_chat/#aio_chat","title":"aio_chat","text":"<p>AI\u738b (AI king) is a Japanese quiz dataset developed for research and competition purposes. This is a evaluation setup for chat LLMs.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>AI\u738b \u301c\u30af\u30a4\u30baAI\u65e5\u672c\u4e00\u6c7a\u5b9a\u6226\u301c</li> <li>JAQKET: \u30af\u30a4\u30ba\u3092\u984c\u6750\u306b\u3057\u305f\u65e5\u672c\u8a9e QA \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u69cb\u7bc9 <pre><code>local dataset_base_args = {\n  class_path: 'HFChatDataset',\n  init_args: {\n    path: 'llm-book/aio',\n    input_template: '{{ question }}',\n    reference_list_template: '{{ answers }}',\n    dataset_kwargs: { trust_remote_code: true },\n  },\n};\n\n{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 4,\n      },\n    },\n    metrics: [\n      { class_path: 'CharF1', init_args: { processor: { class_path: 'AIONormalizer' } } },\n      { class_path: 'ExactMatch', init_args: { processor: { class_path: 'AIONormalizer' } } },\n    ],\n    gen_kwargs: { max_new_tokens: 32 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_chat/#elyza_tasks_100","title":"elyza_tasks_100","text":"<p>A dataset for evaluating instruction-tuned models developed by ELYZA Inc.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>\u516c\u5f0f\u30d6\u30ed\u30b0 <pre><code>{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFChatDataset',\n      init_args: {\n        path: 'elyza/ELYZA-tasks-100',\n        split: 'test',\n        input_template: '{{ input }}',\n        reference_template: '{{ output }}',\n        extra_info_templates: { eval_aspect: '{{ eval_aspect }}' },\n      },\n    },\n    metrics: [\n      { class_path: 'OutputLengthStats' },\n    ],\n    gen_kwargs: { max_new_tokens: 1024 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_chat/#mgsm_ja_chat","title":"mgsm_ja_chat","text":"<p>Multilingual Grade School Math Benchmark (MGSM) is a benchmark of grade-school math problems. This is a Japanese subset of the benchmark. This is a evaluation setup for chat LLMs.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Language Models are Multilingual Chain-of-Thought Reasoners <pre><code>local dataset_base_args = {\n  class_path: 'HFChatDataset',\n  init_args: {\n    path: 'juletxara/mgsm',\n    subset: 'ja',\n    reference_template: '{{ answer }}',\n  },\n};\n\n{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'test', input_template: '\u554f\u984c: {{ question }}' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train', input_template: '{{ question }}' } },\n        num_shots: 4,\n      },\n    },\n    metrics: [\n      { class_path: 'ExactMatch', init_args: { processor: { class_path: 'RegexExtractor', init_args: { pattern: '-?[0-9.,]+' } } } },\n    ],\n    gen_kwargs: { max_new_tokens: 256 },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_chat/#mt-ja","title":"mt-ja","text":"<p>Multi-Turn Benchmark for large language models in Japanese.</p> <p>References:</p> <ul> <li>Data Source <pre><code>{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: {\n      class_path: 'ChatbotBench',\n      init_args: {\n        path_or_name: 'mt-ja',\n        ref_path_or_name: 'mt-ja-ref-gpt4',\n      },\n    },\n    metrics: [\n      { class_path: 'OutputLengthStats' },\n    ],\n    gen_kwargs: { max_new_tokens: 1024 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_chat/#rakuda-v2-ja","title":"rakuda-v2-ja","text":"<p>Rakuda benckmark concists of a set of 40 questions in Japanese about Japanese-specific topics designed to evaluate the capabilities of AI Assistants in Japanese.</p> <p>References:</p> <ul> <li>Original Repository</li> <li>Hugging Face Dataset <pre><code>{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: {\n      class_path: 'ChatbotBench',\n      init_args: {\n        path_or_name: 'rakuda-v2-ja',\n      },\n    },\n    metrics: [\n      { class_path: 'OutputLengthStats' },\n    ],\n    gen_kwargs: { max_new_tokens: 1024 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_chat/#vicuna-ja","title":"vicuna-ja","text":"<p>Vicuna Benchmark for large language models in Japanese.</p> <p>References:</p> <ul> <li>Data Source <pre><code>{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: {\n      class_path: 'ChatbotBench',\n      init_args: {\n        path_or_name: 'vicuna-ja',\n        ref_path_or_name: 'vicuna-ja-ref-gpt4',\n      },\n    },\n    metrics: [\n      { class_path: 'OutputLengthStats' },\n    ],\n    gen_kwargs: { max_new_tokens: 1024 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/","title":"Ja generation","text":""},{"location":"preset_configs/EvalSetup/ja_generation/#aio","title":"aio","text":"<p>AI\u738b (AI king) is a Japanese quiz dataset developed for research and competition purposes.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>AI\u738b \u301c\u30af\u30a4\u30baAI\u65e5\u672c\u4e00\u6c7a\u5b9a\u6226\u301c</li> <li>JAQKET: \u30af\u30a4\u30ba\u3092\u984c\u6750\u306b\u3057\u305f\u65e5\u672c\u8a9e QA \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u69cb\u7bc9 <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'sbintuitions/aio-extended-answers',\n    split: 'validation',\n    reference_list_template: '{{ answers }}',\n  },\n};\n\nlocal template_ = '{{ question }}\u7b54\u3048\u306f\u300c';\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args,\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: { template: template_, },\n    },\n    metrics: [\n      {\n        class_path: 'CharF1',\n        init_args: {\n          processor: { class_path: 'AIONormalizer' },\n          reference_processor: { class_path: 'AIONormalizer' },\n        },\n      },\n      {\n        class_path: 'ExactMatch',\n        init_args: {\n          processor: { class_path: 'AIONormalizer' },\n          reference_processor: { class_path: 'AIONormalizer' },\n        },\n      },\n    ],\n    gen_kwargs: { max_new_tokens: 64, stop_sequences: ['\u300d'], },\n    batch_size: 1,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/#jcommonsenseqa","title":"jcommonsenseqa","text":"<p>JCommonsenseQA is a Japanese version of CommonsenseQA, which is a multiple-choice question answering dataset that requires commonsense reasoning ability. The dataset is built using crowdsourcing with seeds extracted from the knowledge base ConceptNet. This is a setup for generating answers based on the choices provided.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Original Repository</li> <li>JGLUE: Japanese General Language Understanding Evaluation</li> <li>JGLUE: \u65e5\u672c\u8a9e\u8a00\u8a9e\u7406\u89e3\u30d9\u30f3\u30c1\u30de\u30fc\u30af <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'sbintuitions/JCommonsenseQA',\n    split: 'validation',\n    reference_template: '{% set choices = [choice0, choice1, choice2, choice3, choice4] %}{{ choices[label] }}',\n  },\n};\n\nlocal template_ = |||\n  \u4ee5\u4e0b\u306f\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3068\u3001\u8ffd\u52a0\u306e\u80cc\u666f\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u5165\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u8981\u6c42\u3092\u9069\u5207\u306b\u6e80\u305f\u3059\u56de\u7b54\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n  ### \u6307\u793a\n  \u8cea\u554f\u3068\u56de\u7b54\u306e\u9078\u629e\u80a2\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308a\u3001\u9078\u629e\u80a2\u304b\u3089\u56de\u7b54\u3092\u9078\u629e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u4ed6\u306b\u306f\u4f55\u3082\u542b\u3081\u306a\u3044\u3053\u3068\u3092\u53b3\u5b88\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n  ### \u5165\u529b\uff1a\n  \u8cea\u554f\uff1a\u4e3b\u306b\u5b50\u3069\u3082\u5411\u3051\u306e\u3082\u306e\u3067\u3001\u30a4\u30e9\u30b9\u30c8\u306e\u3064\u3044\u305f\u7269\u8a9e\u304c\u66f8\u304b\u308c\u3066\u3044\u308b\u3082\u306e\u306f\u3069\u308c\uff1f\n  \u9078\u629e\u80a2\uff1a\u4e16\u754c,\u5199\u771f\u96c6,\u7d75\u672c,\u8ad6\u6587,\u56f3\u9451\n  ### \u56de\u7b54\uff1a\n  \u7d75\u672c\n\n  ### \u5165\u529b\uff1a\n  \u8cea\u554f\uff1a\u672a\u6210\u5e74\u8005\u3092\u76e3\u8b77\u30fb\u6559\u80b2\u3057\uff0c\u5f7c\u3089\u3092\u76e3\u7763\u3057\uff0c\u5f7c\u3089\u306e\u8ca1\u7523\u4e0a\u306e\u5229\u76ca\u3092\u5b88\u308b\u6cd5\u5f8b\u4e0a\u306e\u7fa9\u52d9\u3092\u3082\u3064\u4eba\u306f\uff1f\n  \u9078\u629e\u80a2\uff1a\u6d6e\u6d6a\u8005,\u4fdd\u8b77\u8005,\u304a\u574a\u3055\u3093,\u5b97\u6559\u8005,\u9810\u8a00\u8005\n  ### \u56de\u7b54\uff1a\n  \u4fdd\u8b77\u8005\n\n  ### \u5165\u529b\uff1a\n  \u8cea\u554f\uff1a\u6570\u5b57\u306e\uff11\u3092\u8868\u3059\u3068\u304d\u306b\u4f7f\u3046\u4f53\u306f\uff1f\n  \u9078\u629e\u80a2\uff1a\u80f8,\u8089\u7403,\u80cc\u4e2d,\u4eba\u5dee\u3057\u6307,\u89aa\u6307\n  ### \u56de\u7b54\uff1a\n  \u4eba\u5dee\u3057\u6307\n\n  ### \u5165\u529b\uff1a\n  \u8cea\u554f\uff1a\u706b\u3092\u8d77\u3053\u3059\u3068\u3042\u3089\u308f\u308c\u308b\u3082\u304f\u3082\u304f\u3059\u308b\u3082\u306e\u306f\uff1f\n  \u9078\u629e\u80a2\uff1a\u6b6f\u306e\u5909\u8272,\u30ac\u30b9,\u4e2d\u6bd2,\u7206\u767a,\u7159\n  ### \u56de\u7b54\uff1a\n  \u7159\n\n  ### \u5165\u529b\uff1a\n  \u8cea\u554f\uff1a{{ question }}\n  \u9078\u629e\u80a2\uff1a{{ choice0 }},{{ choice1 }},{{ choice2 }},{{ choice3 }},{{ choice4 }}\n  ### \u56de\u7b54\uff1a\n|||;\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args,\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: { template: template_, },\n    },\n    metrics: [\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 64, stop_sequences: ['\\n\\n'], },\n    batch_size: 1,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/#jnli","title":"jnli","text":"<p>JNLI is a Japanese version of the NLI (Natural Language Inference) dataset. The sentence pairs are extracted from image captions and annotated by crowd workers.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Original Repository</li> <li>JGLUE: Japanese General Language Understanding Evaluation</li> <li>JGLUE: \u65e5\u672c\u8a9e\u8a00\u8a9e\u7406\u89e3\u30d9\u30f3\u30c1\u30de\u30fc\u30af <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'llm-book/JGLUE',\n    subset: 'JNLI',\n    reference_template: \"{{ ['\\\"\u542b\u610f\\\"', '\\\"\u77db\u76fe\\\"', '\\\"\u4e2d\u7acb\\\"'][label] }}\",\n    dataset_kwargs: { trust_remote_code: true },\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'BalancedFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 3,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          \u524d\u63d0\u3068\u4eee\u8aac\u306e\u95a2\u4fc2\u3092\u300c\u4e2d\u7acb\u300d\u3001\u300c\u542b\u610f\u300d\u3001\u300c\u77db\u76fe\u300d\u306e\u4e2d\u304b\u3089\u56de\u7b54\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n          {% for item in few_shot_data %}\n          \u524d\u63d0\uff1a\u300c{{ item.sentence1 }}\u300d\n          \u4eee\u8aac\uff1a\u300c{{ item.sentence2 }}\u300d\n          \u95a2\u4fc2\uff1a\u300c{{ item.references[0] }}\u300d\n          {% endfor %}\n          \u524d\u63d0\uff1a\u300c{{ sentence1 }}\u300d\n          \u4eee\u8aac\uff1a\u300c{{ sentence2 }}\u300d\n        ||| + '\u95a2\u4fc2\uff1a\u300c',\n      },\n    },\n    metrics: [\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 6, stop_sequences: ['\u524d\u63d0', '\u300d'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/#jsquad","title":"jsquad","text":"<p>JSQuAD is a Japanese version of SQuAD, one of the datasets of reading comprehension. The passages are extracted from Japanese Wikipedia, and the questions and answers are created by crowd workers.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Original Repository</li> <li>JGLUE: Japanese General Language Understanding Evaluation</li> <li>JGLUE: \u65e5\u672c\u8a9e\u8a00\u8a9e\u7406\u89e3\u30d9\u30f3\u30c1\u30de\u30fc\u30af <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'sbintuitions/JSQuAD',\n    split: 'validation',\n    reference_list_template: '{{ answers.text }}',\n  },\n};\n\nlocal template_ = |||\n  \u4ee5\u4e0b\u306f\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3068\u3001\u8ffd\u52a0\u306e\u80cc\u666f\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u5165\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u8981\u6c42\u3092\u9069\u5207\u306b\u6e80\u305f\u3059\u56de\u7b54\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n  ### \u6307\u793a\n  \u8cea\u554f\u306b\u5bfe\u3059\u308b\u56de\u7b54\u3092\u6587\u7ae0\u304b\u3089\u4e00\u8a00\u3067\u62bd\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306f\u540d\u8a5e\u3067\u7b54\u3048\u3066\u304f\u3060\u3055\u3044\u3002 \u305d\u308c\u4ee5\u5916\u306b\u306f\u4f55\u3082\u542b\u3081\u306a\u3044\u3053\u3068\u3092\u53b3\u5b88\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n  ### \u5165\u529b\uff1a\n  \u6587\u7ae0\uff1a\u8056\u6b66\u5929\u7687 [SEP] \u6587\u6b66\u5929\u7687\u306e\u7b2c\u4e00\u7687\u5b50\u3068\u3057\u3066\u751f\u307e\u308c\u305f\u304c\u3001\u6176\u96f24\u5e746\u670815\u65e5\uff08707\u5e747\u670818\u65e5\uff09\u306b7\u6b73\u3067\u7236\u3068\u6b7b\u5225\u3001\u6bcd\u30fb\u5bae\u5b50\u3082\u5fc3\u7684\u969c\u5bb3\u306b\u9665\u3063\u305f\u305f\u3081\u3001\u305d\u306e\u5f8c\u306f\u9577\u3089\u304f\u4f1a\u3046\u3053\u3068\u306f\u306a\u304b\u3063\u305f\u3002\u7269\u5fc3\u304c\u3064\u3044\u3066\u4ee5\u5f8c\u306e\u5929\u7687\u304c\u75c5\u6c17\u306e\u5e73\u7652\u3057\u305f\u6bcd\u3068\u306e\u5bfe\u9762\u3092\u679c\u305f\u3057\u305f\u306e\u306f\u9f6237\u306e\u3068\u304d\u3067\u3042\u3063\u305f\u3002\u3053\u306e\u305f\u3081\u3001\u540c\u5e747\u670817\u65e5\uff08707\u5e748\u670818\u65e5\uff09\u3001\u7236\u65b9\u306e\u7956\u6bcd\u30fb\u5143\u660e\u5929\u7687\uff08\u5929\u667a\u5929\u7687\u7687\u5973\uff09\u304c\u4e2d\u7d99\u304e\u306e\u5929\u7687\u3068\u3057\u3066\u5373\u4f4d\u3057\u305f\u3002\u548c\u92857\u5e746\u670825\u65e5\uff08714\u5e748\u67089\u65e5\uff09\u306b\u306f\u9996\u7687\u5b50\u306e\u5143\u670d\u304c\u884c\u308f\u308c\u3066\u540c\u65e5\u6b63\u5f0f\u306b\u7acb\u592a\u5b50\u3055\u308c\u308b\u3082\u3001\u75c5\u5f31\u3067\u3042\u3063\u305f\u3053\u3068\u3001\u7687\u89aa\u52e2\u529b\u3068\u5916\u621a\u3067\u3042\u308b\u85e4\u539f\u6c0f\u3068\u306e\u5bfe\u7acb\u3082\u3042\u308a\u3001\u5373\u4f4d\u306f\u5148\u5ef6\u3070\u3057\u306b\u3055\u308c\u3001\u7fcc\u970a\u4e80\u5143\u5e749\u67082\u65e5\uff08715\u5e7410\u67083\u65e5\uff09\u306b\u4f2f\u6bcd\uff08\u6587\u6b66\u5929\u7687\u306e\u59c9\uff09\u30fb\u5143\u6b63\u5929\u7687\u304c\u300c\u4e2d\u7d99\u304e\u306e\u4e2d\u7d99\u304e\u300d\u3068\u3057\u3066\u7687\u4f4d\u3092\u7d99\u3050\u3053\u3068\u306b\u306a\u3063\u305f\u300224\u6b73\u306e\u3068\u304d\u306b\u5143\u6b63\u5929\u7687\u3088\u308a\u7687\u4f4d\u3092\u8b72\u3089\u308c\u3066\u5373\u4f4d\u3059\u308b\u3053\u3068\u306b\u306a\u308b\u3002\n  \u8cea\u554f\uff1a\u6587\u6b66\u5929\u7687\u306e\u7b2c\u4e00\u7687\u5b50\u3068\u3057\u3066\u751f\u307e\u308c\u305f\u306e\u306f\uff1f\n  ### \u56de\u7b54\uff1a\n  \u8056\u6b66\u5929\u7687\n\n  ### \u5165\u529b\uff1a\n  \u6587\u7ae0\uff1a\u901a\u79f0 [SEP] \u4eba\u540d\u3068\u3057\u3066\u306e\u901a\u79f0\u306f\u901a\u308a\u540d\u3001\u4e8c\u3064\u540d\u3001\u7570\u540d\u3001\u306a\u3069\u3068\u547c\u3070\u308c\u308b\u4e8b\u3082\u3042\u308b\u3002\u8fd1\u4e16\u307e\u3067\u306f\u3001\u672c\u540d\uff08\u5b9f\u540d\uff09\u306f\u300c\u300d\u3068\u547c\u3070\u308c\u3001\u516c\u8a00\u306f\u907f\u3051\u308b\u7fd2\u6163\u304c\u3042\u3063\u305f\u3002\u305d\u306e\u305f\u3081\u3001\u4eba\u3092\u547c\u3076\u6642\u306f\u300c\u4eee\u540d\u300d\u300c\u5b57\u300d\u306a\u3069\u306e\u901a\u79f0\u3001\u5b98\u8077\u540d\u3092\u7528\u3044\u308b\u306e\u304c\u4e00\u822c\u7684\u3060\u3063\u305f\u3002\u4eca\u65e5\u3067\u3082\u300c\u7dcf\u7406\u300d\u300c\u5927\u81e3\u300d\u300c\u793e\u9577\u300d\u300c\u5c02\u52d9\u300d\u306a\u3069\u3068\u547c\u3073\u304b\u3051\u306b\u4f7f\u3046\u306e\u304c\u3053\u308c\u306b\u3042\u305f\u308b\u3002\n  \u8cea\u554f\uff1a\u4eba\u540d\u3068\u3057\u3066\u306e\u901a\u79f0\u306f\u4f55\u3068\u547c\u3070\u308c\u3066\u3044\u308b\u304b\n  ### \u56de\u7b54\uff1a\n  \u901a\u308a\u540d\u3001\u4e8c\u3064\u540d\u3001\u7570\u540d\n\n  ### \u5165\u529b\uff1a\n  \u6587\u7ae0\uff1a\u5742\u672c\u9f8d\u4e00 [SEP] 2014\u5e747\u670810\u65e5\u3001\u6240\u5c5e\u4e8b\u52d9\u6240\u30a8\u30a4\u30d9\u30c3\u30af\u30b9\u30fb\u30df\u30e5\u30fc\u30b8\u30c3\u30af\u30fb\u30af\u30ea\u30a8\u30a4\u30c6\u30a3\u30f4\u304b\u3089\u4e2d\u54bd\u982d\u764c\u3067\u3042\u308b\u3053\u3068\u3001\u7642\u990a\u306b\u5c02\u5ff5\u3059\u308b\u305f\u3081\u306b\u30b3\u30f3\u30b5\u30fc\u30c8\u6d3b\u52d5\u306a\u3069\u3092\u4e2d\u6b62\u3059\u308b\u65e8\u304c\u767a\u8868\u3055\u308c\u305f\u3002\u304b\u3064\u3066\u306f\u30a4\u30f3\u30bf\u30d3\u30e5\u30fc\u306a\u3069\u3067\u5ea6\u3005\u81ea\u8eab\u306e\u5065\u5eb7\u72b6\u614b\u3084\u4f53\u529b\u306b\u81ea\u4fe1\u3092\u8868\u3057\u3066\u304a\u308a\u3001\u30b3\u30f3\u30b5\u30fc\u30c8\u7b49\u516c\u6f14\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3092\u81ea\u8eab\u306e\u5065\u5eb7\u306b\u8d77\u56e0\u3059\u308b\u7406\u7531\u3067\u30ad\u30e3\u30f3\u30bb\u30eb\u3057\u305f\u3053\u3068\u304c\u306a\u304b\u3063\u305f\u3002\n  \u8cea\u554f\uff1a\u5742\u672c\u9f8d\u4e00\u304c\u7642\u990a\u306b\u5c02\u5ff5\u3059\u308b\u305f\u3081\u30b3\u30f3\u30b5\u30fc\u30c8\u6d3b\u52d5\u306a\u3069\u3092\u4e2d\u6b62\u3059\u308b\u3068\u767a\u8868\u3057\u305f\u306e\u306f\u3044\u3064\u304b\u3002\n  ### \u56de\u7b54\uff1a\n  2014\u5e747\u670810\u65e5\n\n  ### \u5165\u529b\uff1a\n  \u6587\u7ae0\uff1a\u30ea\u30ea\u30fc\u30d5 [SEP] \u30d7\u30ec\u30c3\u30b7\u30e3\u30fc\u306e\u6bd4\u8f03\u7684\u304b\u304b\u3089\u306a\u3044\u72b6\u614b\u3067\u6295\u3052\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3001\u82e5\u624b\u6295\u624b\u306e\u30c6\u30b9\u30c8\u306e\u5834\u3068\u3057\u305f\u308a\u3001\u6545\u969c\u660e\u3051\u3084\u767b\u677f\u9593\u9694\u306e\u958b\u3044\u305f\u6295\u624b\u3092\u8abf\u6574\u76ee\u7684\u3067\u767b\u677f\u3055\u305b\u308b\u3053\u3068\u3082\u3042\u308b\u3002\u6557\u6226\u51e6\u7406\u3067\u3042\u3063\u3066\u3082\u597d\u6295\u3059\u308c\u3070\u6b21\u56de\u304b\u3089\u5148\u767a\u3084\u63a5\u6226\u3067\u306e\u4e2d\u7d99\u304e\u306b\u8d77\u7528\u3055\u308c\u308b\u3088\u3046\u306b\u306a\u308b\u5834\u5408\u3082\u3042\u308a\u3001\u5e78\u3044\u6253\u7dda\u306e\u63f4\u8b77\u3092\u53d7\u3051\u3066\u30c1\u30fc\u30e0\u304c\u9006\u8ee2\u3059\u308c\u3070\u52dd\u5229\u6295\u624b\u306b\u8f1d\u304f\u3053\u3068\u3082\u3042\u308b\u3002\n  \u8cea\u554f\uff1a\u6253\u7dda\u306e\u63f4\u8b77\u3092\u53d7\u3051\u3066\u30c1\u30fc\u30e0\u304c\u9006\u8ee2\u3059\u308b\u3068\u3069\u3093\u306a\u6295\u624b\u306b\u306a\u308b\uff1f\n  ### \u56de\u7b54\uff1a\n  \u52dd\u5229\u6295\u624b\n\n  ### \u5165\u529b\uff1a\n  \u6587\u7ae0\uff1a{{ context }}\n  \u8cea\u554f\uff1a{{ question }}\n  ### \u56de\u7b54\uff1a\n|||;\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args,\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: { template: template_, },\n    },\n    metrics: [\n      { class_path: 'CharF1' },\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 64, stop_sequences: ['\\n\\n'], },\n    batch_size: 1,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/#mgsm_ja","title":"mgsm_ja","text":"<p>Multilingual Grade School Math Benchmark (MGSM) is a benchmark of grade-school math problems. This is a Japanese subset of the benchmark.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Language Models are Multilingual Chain-of-Thought Reasoners <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'juletxara/mgsm',\n    subset: 'ja',\n    reference_template: '{{ answer_number }}',\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'test' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 4,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          {% for item in few_shot_data %}\n          {{ item.question }}\n          {{ item.answer }}\n          {% endfor %}\n          \u554f\u984c: {{ question }}\n        ||| + '\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306e\u7b54\u3048:',\n      },\n    },\n    metrics: [\n      { class_path: 'ExactMatch', init_args: { processor: { class_path: 'RegexExtractor', init_args: { pattern: '-?[0-9.,]+' } } } },\n    ],\n    gen_kwargs: { max_new_tokens: 256, stop_sequences: ['\u554f\u984c:'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/#wrime_pos_neg","title":"wrime_pos_neg","text":"<p>WRIME (dataset of Writers\u2019 and Readers\u2019 Intensities of eMotion for their Estimation) is constructed by annotating Internet posts with both the writer\u2019s subjective emotional intensity and the reader\u2019s objective one. This setup converts the original dataset into binary sentiment classification.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Original Repository</li> <li>WRIME: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations</li> <li>A Japanese Dataset for Subjective and Objective Sentiment Polarity Classification in Micro Blog Domain <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'llm-book/wrime-sentiment',\n    reference_template: \"{{ ['\\\"\u30dd\u30b8\u30c6\u30a3\u30d6\\\"', '\\\"\u30cd\u30ac\u30c6\u30a3\u30d6\\\"'][label] }}\",\n    dataset_kwargs: { trust_remote_code: true },\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'BalancedFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 4,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          \u6587\u306e\u6975\u6027\u306b\u3064\u3044\u3066\u300c\u30dd\u30b8\u30c6\u30a3\u30d6\u300d\u304b\u300c\u30cd\u30ac\u30c6\u30a3\u30d6\u300d\u304b\u3067\u7b54\u3048\u3066\u304f\u3060\u3055\u3044\u3002\n          {% for item in few_shot_data %}\n          \u6587\uff1a{{ item.sentence }}\n          \u6975\u6027\uff1a\u300c{{ item.references[0] }}\u300d\n          {% endfor %}\n          \u6587\uff1a{{sentence}}\n        ||| + '\u6975\u6027\uff1a\u300c',\n      },\n    },\n    metrics: [\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 8, stop_sequences: ['\u300d'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/#xlsum_ja","title":"xlsum_ja","text":"<p>XLSum is a comprehensive and diverse dataset comprising 1.35 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. This is a Japanese subset of the dataset.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Original Repository</li> <li>XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'csebuetnlp/xlsum',\n    subset: 'japanese',\n    reference_template: '{{ summary }}',\n  },\n};\n\n{\n  // as we deal with LLMs with short context window, we set max_text_length and max_summary_length\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'BalancedFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 1,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          \u6587\u7ae0\u3092\uff11\u301c\uff13\u6587\u3067\u8981\u7d04\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n          {% for item in few_shot_data %}\n          \u6587\u7ae0: {{ item.text }}\n          \u8981\u7d04: {{ item.references[0] }}\n          {% endfor %}\n          \u6587\u7ae0: {{ text }}\n        ||| + '\u8981\u7d04:',\n      },\n    },\n    metrics: [\n      {\n        class_path: 'ROUGE',\n        init_args: { tokenizer: { class_path: 'SacreBleuTokenizer', init_args: { name: 'ja-mecab' } } },\n      },\n    ],\n    gen_kwargs: { max_new_tokens: 100, stop_sequences: ['\\n'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_multiple_choice/","title":"Ja multiple choice","text":""},{"location":"preset_configs/EvalSetup/ja_multiple_choice/#jcommonsenseqa_mc","title":"jcommonsenseqa_mc","text":"<p>JCommonsenseQA is a Japanese version of CommonsenseQA, which is a multiple-choice question answering dataset that requires commonsense reasoning ability. The dataset is built using crowdsourcing with seeds extracted from the knowledge base ConceptNet. This is a setup for multiple choice where the model chooses the correct answer based on the log-probabilities of the choices.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Original Repository</li> <li>JGLUE: Japanese General Language Understanding Evaluation</li> <li>JGLUE: \u65e5\u672c\u8a9e\u8a00\u8a9e\u7406\u89e3\u30d9\u30f3\u30c1\u30de\u30fc\u30af <pre><code>local dataset_base_args = {\n  path: 'llm-book/JGLUE',\n  subset: 'JCommonsenseQA',\n  choices_templates: ['{{ choice0 }}', '{{ choice1 }}', '{{ choice2 }}', '{{ choice3 }}', '{{ choice4 }}'],\n  answer_index_template: '{{ label }}',\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'validation' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 0,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          {% for item in few_shot_data %}\n          \u554f\u984c\uff1a{{ item.question }}\n          \u56de\u7b54\uff1a\u300c{{ item.choices[item.answer_index] }}\u300d\n          {% endfor %}\n          \u554f\u984c\uff1a{{question}}\n        ||| + '\u56de\u7b54\uff1a\u300c',\n      },\n    },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_multiple_choice/#xwinograd_ja","title":"xwinograd_ja","text":"<p>XWinograd is a multilingual collection of Winograd Schemas in six languages that can be used for evaluation of cross-lingual commonsense reasoning capabilities. This is an Japanese subset of the dataset.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>It\u2019s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning <pre><code>{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: {\n        path: 'Muennighoff/xwinograd',\n        subset: 'jp',\n        split: 'test',\n        choices_templates: [\n          '{{ option1 }}{{ sentence.split(\"_\")[1] }}',\n          '{{ option2 }}{{ sentence.split(\"_\")[1] }}',\n        ],\n        answer_index_template: '{{ answer | int - 1 }}',\n        input_templates: { context: '{{ sentence.split(\"_\")[0] }}' },\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: '{{ context }}',\n      },\n    },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/translation/","title":"Translation","text":""},{"location":"preset_configs/EvalSetup/translation/#wmt20_en_ja","title":"wmt20_en_ja","text":"<p>This dataset is created as a test set for the WMT20 shared task on news translation. This is English to Japanese translation.</p> <p>References:</p> <ul> <li>Data Source</li> <li>2020 Fifth Conference on Machine Translation (WMT20) <pre><code>local dataset = {\n  class_path: 'SacreBleuDataset',\n  init_args: { name: 'wmt20', langpair: 'en-ja' },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset,\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        // Use the eval dataset for few-shot data,\n        // but `RandomFewShotGenerator` will avoid using the same few-shot isntances as the input.\n        dataset: dataset,\n        num_shots: 4,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          {% for item in few_shot_data %}\n          En: `{{ item.source }}`\n          Ja: `{{ item.references[0] }}`\n          {% endfor %}\n          En: `{{ source }}`\n        ||| + 'Ja: `',\n      },\n    },\n    metrics: [\n      { class_path: 'BLEU', init_args: { tokenize_option: 'ja-mecab' } },\n    ],\n    gen_kwargs: { max_new_tokens: 128, stop_sequences: ['`'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/translation/#wmt20_ja_en","title":"wmt20_ja_en","text":"<p>This dataset is created as a test set for the WMT20 shared task on news translation. This is Japanese to English translation.</p> <p>References:</p> <ul> <li>Data Source</li> <li>2020 Fifth Conference on Machine Translation (WMT20) <pre><code>local dataset = {\n  class_path: 'SacreBleuDataset',\n  init_args: { name: 'wmt20', langpair: 'ja-en' },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset,\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        // Use the eval dataset for few-shot data,\n        // but `RandomFewShotGenerator` will avoid using the same few-shot isntances as the input.\n        dataset: dataset,\n        num_shots: 4,\n      },\n    },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: |||\n          {% for item in few_shot_data %}\n          Ja: `{{ item.source }}`\n          En: `{{ item.references[0] }}`\n          {% endfor %}\n          Ja: `{{ source }}`\n        ||| + 'En: `',\n      },\n    },\n    metrics: [\n      { class_path: 'BLEU', init_args: { tokenize_option: 'intl' } },\n    ],\n    gen_kwargs: { max_new_tokens: 128, stop_sequences: ['`'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/translation_chat/","title":"Translation chat","text":""},{"location":"preset_configs/EvalSetup/translation_chat/#wmt20_en_ja_chat","title":"wmt20_en_ja_chat","text":"<p>This dataset is created as a test set for the WMT20 shared task on news translation. This is English to Japanese translation. This is a evaluation setup for chat LLMs.</p> <p>References:</p> <ul> <li>Data Source</li> <li>2020 Fifth Conference on Machine Translation (WMT20) <pre><code>local dataset = {\n  class_path: 'SacreBleuChatDataset',\n  init_args: { name: 'wmt20', langpair: 'en-ja' },\n};\n\n{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: dataset,\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        // Use the eval dataset for few-shot data,\n        // but `RandomFewShotGenerator` will avoid using the same few-shot isntances as the input.\n        dataset: dataset,\n        num_shots: 4,\n      },\n    },\n    metrics: [\n      { class_path: 'BLEU', init_args: { tokenize_option: 'ja-mecab' } },\n    ],\n    gen_kwargs: { max_new_tokens: 128, stop_sequences: ['`'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/translation_chat/#wmt20_ja_en_chat","title":"wmt20_ja_en_chat","text":"<p>This dataset is created as a test set for the WMT20 shared task on news translation. This is Japanese to English translation. This is a evaluation setup for chat LLMs.</p> <p>References:</p> <ul> <li>Data Source</li> <li>2020 Fifth Conference on Machine Translation (WMT20) <pre><code>local dataset = {\n  class_path: 'SacreBleuChatDataset',\n  init_args: { name: 'wmt20', langpair: 'ja-en' },\n};\n\n{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: dataset,\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        // Use the eval dataset for few-shot data,\n        // but `RandomFewShotGenerator` will avoid using the same few-shot isntances as the input.\n        dataset: dataset,\n        num_shots: 4,\n      },\n    },\n    metrics: [\n      { class_path: 'BLEU', init_args: { tokenize_option: 'intl' } },\n    ],\n    gen_kwargs: { max_new_tokens: 128, stop_sequences: ['`'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/Metric/","title":"Metric","text":""},{"location":"preset_configs/Metric/#assistant_eval_en_single_turn","title":"assistant_eval_en_single_turn","text":"<p>This is a configuration for evaluting the quality of responses generated by an AI assistant. Originally used to generate scores for MT-bench or Vicuna-bench.</p> <p>Adapted from lm-sys/FastChat. <pre><code>{\n  class_path: 'ChatLLMScore',\n  init_args: {\n    language_model: { class_path: 'OpenAIChatAPI', init_args: { model: 'gpt-4-turbo-2024-04-09' } },\n    valid_score_range: [1, 10],\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: std.stripChars(|||\n          [Instruction]\n          {% if references|length &gt; 0 -%}\n          Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n          {%- else -%}\n          Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n          {%- endif %}\n\n          [Question]\n          {{ messages[0][\"content\"] }}\n\n          {% if references|length &gt; 0 -%}\n          [The Start of Reference Answer]\n          {{ references[0] }}\n          [The End of Reference Answer]\n          {% endif -%}\n          [The Start of Assistant's Answer]\n          {% if messages|length == 1 %}{{ lm_output }}{% else %}{{ messages[1][\"content\"] }}{% endif %}\n          [The End of Assistant's Answer]\n        |||, '\\n'),\n      },\n    },\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/Metric/#assistant_eval_ja_single_turn","title":"assistant_eval_ja_single_turn","text":"<p>This is a configuration for evaluting the quality of responses generated by an AI assistant. Originally used to generate scores for the Japanese versions of MT-bench or Vicuna-bench.</p> <p>Translated and adapted from lm-sys/FastChat. <pre><code>{\n  class_path: 'ChatLLMScore',\n  init_args: {\n    language_model: { class_path: 'OpenAIChatAPI', init_args: { model: 'gpt-4-turbo-2024-04-09' } },\n    valid_score_range: [1, 10],\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: std.stripChars(|||\n          [\u6307\u793a]\n          {% if references|length &gt; 0 -%}\n          \u4ee5\u4e0b\u306b\u8868\u793a\u3055\u308c\u308b\u30e6\u30fc\u30b6\u306e\u8cea\u554f\u306b\u5bfe\u3059\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u5fdc\u7b54\u306e\u54c1\u8cea\u3092\u8a55\u4fa1\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u8a55\u4fa1\u306f\u6b63\u78ba\u3055\u3068\u6709\u7528\u6027\u3092\u8003\u616e\u3059\u3079\u304d\u3067\u3059\u3002\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u306e\u8a00\u8a9e\u306f\u3001\u30e6\u30fc\u30b6\u304c\u4f7f\u7528\u3057\u3066\u3044\u308b\u8a00\u8a9e\u3068\u4e00\u81f4\u3057\u3066\u3044\u308b\u3079\u304d\u3067\u3001\u305d\u3046\u3067\u306a\u3044\u5834\u5408\u306f\u6e1b\u70b9\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002\u53c2\u7167\u56de\u7b54\u3068\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u304c\u4e0e\u3048\u3089\u308c\u307e\u3059\u3002\u3042\u306a\u305f\u306e\u8a55\u4fa1\u306f\u3001\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u3068\u53c2\u7167\u56de\u7b54\u3092\u6bd4\u8f03\u3059\u308b\u3053\u3068\u304b\u3089\u59cb\u3081\u3066\u304f\u3060\u3055\u3044\u3002\u30df\u30b9\u3092\u7279\u5b9a\u3057\u3001\u8a02\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u3067\u304d\u308b\u3060\u3051\u5ba2\u89b3\u7684\u3067\u3042\u308b\u3053\u3068\u3002\u8a55\u4fa1\u306e\u8aac\u660e\u3092\u3057\u305f\u5f8c\u3001\"[[rating]]\"\u3068\u3044\u3046\u5f62\u5f0f\u3067\u30011\u304b\u308910\u307e\u3067\u306e\u6574\u6570\u306e\u8a55\u4fa1\u5024\u3092\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\uff08\u4f8b \"rating\uff1a[[5]]\"\uff09\u3002\n          {%- else -%}\n          \u4ee5\u4e0b\u306b\u8868\u793a\u3055\u308c\u308b\u30e6\u30fc\u30b6\u306e\u8cea\u554f\u306b\u5bfe\u3059\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u5fdc\u7b54\u306e\u54c1\u8cea\u3092\u516c\u5e73\u306b\u8a55\u4fa1\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u8a55\u4fa1\u306f\u3001\u5fdc\u7b54\u306e\u6709\u7528\u6027\u3001\u95a2\u9023\u6027\u3001\u6b63\u78ba\u6027\u3001\u6df1\u3055\u3001\u5275\u9020\u6027\u3001\u8a73\u7d30\u5ea6\u306a\u3069\u306e\u8981\u7d20\u3092\u8003\u616e\u3059\u3079\u304d\u3067\u3059\u3002\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u306e\u8a00\u8a9e\u306f\u3001\u30e6\u30fc\u30b6\u304c\u4f7f\u7528\u3057\u3066\u3044\u308b\u8a00\u8a9e\u3068\u4e00\u81f4\u3057\u3066\u3044\u308b\u3079\u304d\u3067\u3001\u305d\u3046\u3067\u306a\u3044\u5834\u5408\u306f\u6e1b\u70b9\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002\u8a55\u4fa1\u306f\u77ed\u3044\u8aac\u660e\u304b\u3089\u59cb\u3081\u3066\u304f\u3060\u3055\u3044\u3002\u3067\u304d\u308b\u3060\u3051\u5ba2\u89b3\u7684\u3067\u3042\u308b\u3053\u3068\u3002\u8a55\u4fa1\u306e\u8aac\u660e\u3092\u3057\u305f\u5f8c\u3001\"[[rating]]\"\u3068\u3044\u3046\u5f62\u5f0f\u3067\u30011\u304b\u308910\u307e\u3067\u306e\u6574\u6570\u306e\u8a55\u4fa1\u5024\u3092\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\uff08\u4f8b \"rating\uff1a[[5]]\"\uff09\u3002\n          {%- endif %}\n\n          [\u30e6\u30fc\u30b6\u306e\u8cea\u554f]\n          {{ messages[0][\"content\"] }}\n\n          {% if references|length &gt; 0 -%}\n          [\u53c2\u8003\u56de\u7b54\u306e\u958b\u59cb]\n          {{ references[0] }}\n          [\u53c2\u8003\u56de\u7b54\u306e\u7d42\u4e86]\n          {% endif -%}\n          [\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u958b\u59cb]\n          {% if messages|length == 1 %}{{ lm_output }}{% else %}{{ messages[1][\"content\"] }}{% endif %}\n          [\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u7d42\u4e86]\n        |||, '\\n'),\n      },\n    },\n    system_message: '\u3042\u306a\u305f\u306f\u512a\u79c0\u306a\u52a9\u624b\u3067\u3059\u3002',\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/Metric/#elyza_tasks_100_eval","title":"elyza_tasks_100_eval","text":"<p>This is a config of the ChatLLMScore class designed to evaluate chat assistants with ELYZA-tasks-100. The template is adapted from the blog post ELYZA\u304c\u516c\u958b\u3057\u305f\u65e5\u672c\u8a9eLLM\u300cELYZA-japanese-Llama-2-7b\u300d\u306b\u3064\u3044\u3066\u306e\u89e3\u8aac : (2) \u8a55\u4fa1\u7de8. <pre><code>{\n  class_path: 'ChatLLMScore',\n  init_args: {\n    language_model: { class_path: 'OpenAIChatAPI', init_args: { model: 'gpt-4-turbo-2024-04-09' } },\n    valid_score_range: [1, 5],\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: std.stripChars(|||\n          \u3042\u306a\u305f\u306f\u63a1\u70b9\u8005\u3067\u3059\u3002\n\n          \u554f\u984c, \u6b63\u89e3\u4f8b, \u63a1\u70b9\u57fa\u6e96, \u56de\u7b54 \u304c\u4e0e\u3048\u3089\u308c\u307e\u3059\u3002\n\n          \u63a1\u70b9\u57fa\u6e96\u3068\u6b63\u89e3\u4f8b\u3092\u53c2\u8003\u306b\u3057\u3066\u3001\u56de\u7b54\u30921,2,3,4,5\u306e5\u6bb5\u968e\u3067\u63a1\u70b9\u3057\u3001\u6570\u5b57\u306e\u307f\u3092\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n          # \u554f\u984c\n          {{ messages[-1][\"content\"] }}\n\n          # \u6b63\u89e3\u4f8b\n          {{ references[0] }}\n\n          # \u63a1\u70b9\u57fa\u6e96\n          \u57fa\u672c\u7684\u306a\u63a1\u70b9\u57fa\u6e96\n          - 1\u70b9: \u8aa4\u3063\u3066\u3044\u308b\u3001 \u6307\u793a\u306b\u5f93\u3048\u3066\u3044\u306a\u3044\n          - 2\u70b9: \u8aa4\u3063\u3066\u3044\u308b\u304c\u3001\u65b9\u5411\u6027\u306f\u5408\u3063\u3066\u3044\u308b\n          - 3\u70b9: \u90e8\u5206\u7684\u306b\u8aa4\u3063\u3066\u3044\u308b\u3001 \u90e8\u5206\u7684\u306b\u5408\u3063\u3066\u3044\u308b\n          - 4\u70b9: \u5408\u3063\u3066\u3044\u308b\n          - 5\u70b9: \u5f79\u306b\u7acb\u3064\n\n          \u57fa\u672c\u7684\u306a\u6e1b\u70b9\u9805\u76ee\n          - \u4e0d\u81ea\u7136\u306a\u65e5\u672c\u8a9e: -1\u70b9\n          - \u90e8\u5206\u7684\u306b\u4e8b\u5b9f\u3068\u7570\u306a\u308b\u5185\u5bb9\u3092\u8ff0\u3079\u3066\u3044\u308b: -1\u70b9\n          - \u300c\u502b\u7406\u7684\u306b\u7b54\u3048\u3089\u308c\u307e\u305b\u3093\u300d\u306e\u3088\u3046\u306b\u904e\u5ea6\u306b\u5b89\u5168\u6027\u3092\u6c17\u306b\u3057\u3066\u3057\u307e\u3063\u3066\u3044\u308b: 2\u70b9\u306b\u3059\u308b\n\n          \u554f\u984c\u56fa\u6709\u306e\u63a1\u70b9\u57fa\u6e96\n          {{ eval_aspect }}\n\n          # \u56de\u7b54\n          {{ lm_output }}\n        |||, '\\n'),\n      },\n    },\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/PairwiseJudge/","title":"PairwiseJudge","text":""},{"location":"preset_configs/PairwiseJudge/#assistant_judge_en_single_turn","title":"assistant_judge_en_single_turn","text":"<p>This is a configuration for evaluting the quality of responses generated by an AI assistant. Originally used to generate scores for MT-bench or Vicuna-bench.</p> <p>Adapted from lm-sys/FastChat. <pre><code>{\n  class_path: 'ChatLLMPairwiseJudge',\n  init_args: {\n    language_model: { class_path: 'OpenAIChatAPI', init_args: { model: 'gpt-4-turbo-2024-04-09' } },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: std.stripChars(|||\n          {% set question = model1_item[\"task_inputs\"][\"messages\"][0][\"content\"] -%}\n          {% set model1_messages = model1_item[\"task_inputs\"][\"messages\"] -%}\n          {% set model2_messages = model2_item[\"task_inputs\"][\"messages\"] -%}\n          [Instruction]\n          {% if references|length &gt; 0 -%}\n          Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer, assistant A's answer, and assistant B's answer. Your job is to evaluate which assistant's answer is better. Begin your evaluation by comparing both assistants' answers with the reference answer. Identify and correct any mistakes. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[1]]\" if assistant 1 is better, \"[[2]]\" if assistant 2 is better, and \"[[3]]\" for a tie.\n          {%- else -%}\n          Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[1]]\" if assistant 1 is better, \"[[2]]\" if assistant 2 is better, and \"[[3]]\" for a tie.\n          {%- endif %}\n\n          [Question]\n          {{ question }}\n\n          {% if references|length &gt; 0 -%}\n          [The Start of Reference Answer]\n          {{ references[0] }}\n          [The End of Reference Answer]\n          {% endif -%}\n          [The Start of Assistant 1's Answer]\n          {% if model1_messages|length == 1 %}{{ model1_item[\"lm_output\"] }}{% else %}{{ model1_messages[1][\"content\"] }}{% endif %}\n          [The End of Assistant's Answer]\n          [The Start of Assistant 2's Answer]\n          {% if model2_messages|length == 1 %}{{ model2_item[\"lm_output\"] }}{% else %}{{ model2_messages[1][\"content\"] }}{% endif %}\n          [The End of Assistant's Answer]\n        |||, '\\n'),\n      },\n    },\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/PairwiseJudge/#assistant_judge_ja_single_turn","title":"assistant_judge_ja_single_turn","text":"<p>This is a configuration for evaluting the quality of responses generated by an AI assistant. Originally used to generate scores for the Japanese versions of MT-bench or Vicuna-bench.</p> <p>Translated and adapted from lm-sys/FastChat. <pre><code>{\n  class_path: 'ChatLLMPairwiseJudge',\n  init_args: {\n    language_model: { class_path: 'OpenAIChatAPI', init_args: { model: 'gpt-4-turbo-2024-04-09' } },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: std.stripChars(|||\n          {% set question = model1_item[\"task_inputs\"][\"messages\"][0][\"content\"] -%}\n          {% set model1_messages = model1_item[\"task_inputs\"][\"messages\"] -%}\n          {% set model2_messages = model2_item[\"task_inputs\"][\"messages\"] -%}\n\n          [\u30e6\u30fc\u30b6\u306e\u8cea\u554f]\n          {{ question }}\n\n          {% if references|length &gt; 0 -%}\n          [\u53c2\u8003\u56de\u7b54\u306e\u958b\u59cb]\n          {{ references[0] }}\n          [\u53c2\u8003\u56de\u7b54\u306e\u7d42\u4e86]\n          {% endif -%}\n          [\u30a2\u30b7\u30b9\u30bf\u30f3\u30c81\u306e\u56de\u7b54\u958b\u59cb]\n          {% if model1_messages|length == 1 %}{{ model1_item[\"lm_output\"] }}{% else %}{{ model1_messages[1][\"content\"] }}{% endif %}\n          [\u30a2\u30b7\u30b9\u30bf\u30f3\u30c81\u306e\u56de\u7b54\u7d42\u4e86]\n          [\u30a2\u30b7\u30b9\u30bf\u30f3\u30c82\u306e\u56de\u7b54\u958b\u59cb]\n          {% if model2_messages|length == 1 %}{{ model2_item[\"lm_output\"] }}{% else %}{{ model2_messages[1][\"content\"] }}{% endif %}\n          [\u30a2\u30b7\u30b9\u30bf\u30f3\u30c82\u306e\u56de\u7b54\u7d42\u4e86]\n        |||, '\\n'),\n      },\n    },\n    system_message: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: std.stripChars(|||\n          {% if references|length &gt; 0 -%}\n          \u3042\u306a\u305f\u306f\u3001\u56de\u7b54\u306e\u8cea\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u305f\u3081\u306e\u5be9\u5224\u54e1\u3067\u3059\u3002\u4ee5\u4e0b\u306b\u793a\u3055\u308c\u308b\u30e6\u30fc\u30b6\u30fc\u306e\u8cea\u554f\u306b\u5bfe\u3059\u308b2\u3064\u306eAI\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u5fdc\u7b54\u306e\u54c1\u8cea\u3092\u8a55\u4fa1\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u5185\u5bb9\u304c\u30e6\u30fc\u30b6\u30fc\u306e\u6307\u793a\u306b\u5f93\u3063\u3066\u304a\u308a\u3001\u30e6\u30fc\u30b6\u30fc\u306e\u8cea\u554f\u306b\u3088\u308a\u3088\u304f\u7b54\u3048\u3066\u3044\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3092\u9078\u3093\u3067\u304f\u3060\u3055\u3044\u3002\u53c2\u7167\u56de\u7b54\u3001\u30a2\u30b7\u30b9\u30bf\u30f3\u30c81\u306e\u56de\u7b54\u3001\u30a2\u30b7\u30b9\u30bf\u30f3\u30c82\u306e\u56de\u7b54\u304c\u4e0e\u3048\u3089\u308c\u308b\u306e\u3067\u3001\u3069\u3061\u3089\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u304c\u512a\u308c\u3066\u3044\u308b\u304b\u3092\u8a55\u4fa1\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u8a55\u4fa1\u306e\u969b\u306b\u306f\u3001\u307e\u305a\u305d\u308c\u305e\u308c\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u3092\u53c2\u7167\u56de\u7b54\u3068\u6bd4\u8f03\u3057\u3001\u56de\u7b54\u306e\u8aa4\u308a\u3092\u898b\u3064\u3051\u3066\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u7acb\u5834\u304c\u504f\u3089\u306a\u3044\u3088\u3046\u306b\u3057\u3001\u56de\u7b54\u306e\u63d0\u793a\u9806\u304c\u3042\u306a\u305f\u306e\u5224\u65ad\u306b\u5f71\u97ff\u3057\u306a\u3044\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u9577\u3055\u304c\u8a55\u4fa1\u306b\u5f71\u97ff\u3057\u306a\u3044\u3053\u3068\u3001\u7279\u5b9a\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u540d\u524d\u3092\u597d\u307e\u306a\u3044\u3053\u3068\u3001\u3067\u304d\u308b\u3060\u3051\u5ba2\u89b3\u7684\u3067\u3042\u308b\u3053\u3068\u3001\u306b\u6c17\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\u3002\u8aac\u660e\u306e\u5f8c\u306b\u3001\u6700\u7d42\u7684\u306a\u5224\u65ad\u3092\u4ee5\u4e0b\u306e\u5f62\u5f0f\u306b\u5f93\u3063\u3066\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\uff1a\u30a2\u30b7\u30b9\u30bf\u30f3\u30c81\u304c\u512a\u308c\u3066\u3044\u308c\u3070[[1]]\u3001\u30a2\u30b7\u30b9\u30bf\u30f3\u30c82\u304c\u512a\u308c\u3066\u3044\u308c\u3070[[2]]\u3001\u540c\u70b9\u306e\u5834\u5408\u306f[[3]]\n          {%- else -%}\n          \u3042\u306a\u305f\u306f\u3001\u56de\u7b54\u306e\u8cea\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u305f\u3081\u306e\u5be9\u5224\u54e1\u3067\u3059\u3002\u4ee5\u4e0b\u306b\u793a\u3055\u308c\u308b\u30e6\u30fc\u30b6\u30fc\u306e\u8cea\u554f\u306b\u5bfe\u3059\u308b2\u3064\u306eAI\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u5fdc\u7b54\u306e\u54c1\u8cea\u3092\u8a55\u4fa1\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u5185\u5bb9\u304c\u30e6\u30fc\u30b6\u30fc\u306e\u6307\u793a\u306b\u5f93\u3063\u3066\u304a\u308a\u3001\u30e6\u30fc\u30b6\u30fc\u306e\u8cea\u554f\u306b\u3088\u308a\u3088\u304f\u7b54\u3048\u3066\u3044\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3092\u9078\u3093\u3067\u304f\u3060\u3055\u3044\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u56de\u7b54\u306e\u6709\u7528\u6027\u3001\u95a2\u9023\u6027\u3001\u6b63\u78ba\u6027\u3001\u6df1\u3055\u3001\u5275\u9020\u6027\u3001\u8a73\u7d30\u30ec\u30d9\u30eb\u306a\u3069\u306e\u8981\u7d20\u3092\u8003\u616e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u8a55\u4fa1\u306e\u969b\u306b\u306f\u3001\u307e\u305a2\u3064\u306e\u56de\u7b54\u3092\u6bd4\u8f03\u3057\u3001\u7c21\u5358\u306a\u8aac\u660e\u3092\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u7acb\u5834\u304c\u504f\u3089\u306a\u3044\u3088\u3046\u306b\u3057\u3001\u56de\u7b54\u306e\u63d0\u793a\u9806\u304c\u3042\u306a\u305f\u306e\u5224\u65ad\u306b\u5f71\u97ff\u3057\u306a\u3044\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u9577\u3055\u304c\u8a55\u4fa1\u306b\u5f71\u97ff\u3057\u306a\u3044\u3053\u3068\u3001\u7279\u5b9a\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u540d\u524d\u3092\u597d\u307e\u306a\u3044\u3053\u3068\u3001\u3067\u304d\u308b\u3060\u3051\u5ba2\u89b3\u7684\u3067\u3042\u308b\u3053\u3068\u3001\u306b\u6c17\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\u3002\u8aac\u660e\u306e\u5f8c\u306b\u3001\u6700\u7d42\u7684\u306a\u5224\u65ad\u3092\u4ee5\u4e0b\u306e\u5f62\u5f0f\u306b\u5f93\u3063\u3066\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\uff1a\u30a2\u30b7\u30b9\u30bf\u30f3\u30c81\u304c\u512a\u308c\u3066\u3044\u308c\u3070[[1]]\u3001\u30a2\u30b7\u30b9\u30bf\u30f3\u30c82\u304c\u512a\u308c\u3066\u3044\u308c\u3070[[2]]\u3001\u540c\u70b9\u306e\u5834\u5408\u306f[[3]]\n          {%- endif %}\n        |||, '\\n'),\n      },\n    },\n  },\n}\n</code></pre></p>"}]}