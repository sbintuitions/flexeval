{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"FlexEval","text":"<p>Flexible evaluation tool for language models. Easy to extend, highly customizable!</p> <p>With FlexEval, you can evaluate language models with:</p> <ul> <li>Zero/few-shot prompt tasks</li> <li>Open-ended text-generation benchmarks such as MT-Bench with automatic evaluation using GPT-4</li> <li>Log-probability-based multiple-choice tasks</li> <li>Computing perplexity of text data</li> </ul> <p>... and more!</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Flexibility: <code>flexeval</code> is flexible in terms of the evaluation setup and the language model to be evaluated.</li> <li>Modularity: The core components of <code>flexeval</code> are easily extensible and replaceable.</li> <li>Clarity: The results of evaluation are clear and all the details are saved.</li> <li>Reproducibility: <code>flexeval</code> should be reproducible, with the ability to save and load configurations and results.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install flexeval\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>The following minimal example evaluates the hugging face model <code>sbintuitions/tiny-lm</code> with the <code>commonsense_qa</code> task.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa\" \\\n  --save_dir \"results/commonsense_qa\"\n</code></pre> <p>(The model used in the example is solely for debugging purposes and does not perform well. Try switching to your favorite model!)</p> <p>The results saved in <code>--saved_dir</code> contain:</p> <ul> <li><code>config.json</code>: The configuration of the evaluation, which can be used to replicate the evaluation.</li> <li><code>metrics.json</code>: The evaluation metrics.</li> <li><code>outputs.jsonl</code>: The outputs of the language model that comes with instance-level metrics.</li> </ul> <p>You can flexibly customize the evaluation by specifying command-line arguments or configuration files. Besides the Transformers model, you can also evaluate models via OpenAI ChatGPT and vLLM, and other models can be readily added!</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Run <code>flexeval_presets</code> to check the list of off-the-shelf presets in addition to <code>commonsense_qa</code>. You can find the details in the Preset Configs section.</li> <li>See Getting Started to check the tutorial examples for other kinds of tasks.</li> <li>See the Configuration Guide to set up your evaluation.</li> </ul>"},{"location":"configuration_guide/","title":"How to configure your own evaluation","text":""},{"location":"configuration_guide/#overview","title":"Overview","text":"<p><code>flexeval</code> allows you to evaluate any language model with any task, any prompt, and any metric via the <code>flexeval_lm</code> command. The CLI command is implemented on jsonargparse, which allows a flexible configuration either by CLI arguments or by a configuration file.</p> <p>There are many ways to write configuration files, but for now let's see how to define a config for the argument <code>--eval_setup</code>. You can check the configuration for preset setups by running the following command:</p> <pre><code>flexeval_presets commonsense_qa\n</code></pre> <p>This command will show the configuration for the <code>commonsense_qa</code> setup. The content is written in the jsonnet format, which is a superset of JSON.</p> <p>Tip</p> <p>If you want to convert it to JSON, install <code>jsonnet</code> command and run <code>flexeval_presets commonsense_qa | jsonnet -</code>.</p> <p>The skeleton of the configuration is as follows:</p> <pre><code>{\n  \"class_path\": \"Generation\",\n  \"init_args\": {\n    \"eval_dataset\": {\"class_path\": \"HFGenerationDataset\", \"init_args\": ...},\n    \"prompt_template\": {\"class_path\": \"Jinja2PromptTemplate\", \"init_args\": ...},\n    \"gen_kwargs\": {\"max_new_tokens\": 32, \"stop_sequences\": [\"\u300d\"]},\n    \"metrics\": [{\"class_path\": \"CharF1\"}, {\"class_path\": \"ExactMatch\"}],\n    \"batch_size\": 4\n  }\n}\n</code></pre> <p>The fields <code>class_path</code> and <code>init_args</code> directly mirror the initialization of the specified class.</p> <p>At the top level, <code>\"class_path\": \"Generation\"</code> specifies what kind of <code>EvalSetup</code> to use. Currently, there are four types of <code>EvalSetup</code>: <code>Generation</code>, <code>ChatResponse</code>, <code>MultipleChoice</code>, and <code>Perplexity</code>.</p> <p>Then, <code>Generation</code> is composed of the following components:</p> <ul> <li><code>eval_dataset</code>: The dataset to evaluate. You can choose from concrete classes inheriting <code>GenerationDataset</code>. Most presets use <code>HFGenerationDataset</code>, which load datasets from Hugging Face Hub.</li> <li><code>prompt_template</code>: The template to generate prompts fed to the language model. We have <code>Jinja2PromptTemplate</code>, which uses Jinja2 to embed the data from <code>GenerationDataset</code> into the prompt.</li> <li><code>gen_kwargs</code>: The keyword arguments passed to <code>LanguageModel.batch_complete_text</code>. For example, <code>max_new_tokens</code> and <code>stop_sequences</code> are used to control the generation process. Acceptable arguments depend on the underlying implementation of the generation function (e.g., <code>generate()</code> in <code>transformers</code>).</li> <li><code>metrics</code>: The metrics to compute. You can choose from concrete classes inheriting <code>Metric</code>. These modules take the outputs of the language model, the references, and dataset values, and compute the metrics.</li> </ul> <p>Please refer to the API reference for available classes and their arguments.</p>"},{"location":"configuration_guide/#customizing-the-configuration","title":"Customizing the Configuration","text":"<p>Writing a configuration file from scratch is a bit cumbersome, so we recommend starting from the preset configurations and modifying them as needed.</p> <pre><code>flexeval_presets commonsense_qa &gt; my_config.jsonnet\n</code></pre> <p>Then, pass your config file to <code>--eval_setup</code> argument.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"my_config.jsonnet\"\n</code></pre> <p>Info</p> <p>Under the hood, the preset name like <code>commonsense_qa</code> is resolved to the corresponding configuration file under <code>flexeval/preset_configs</code> in the library.</p>"},{"location":"configuration_guide/#argument-overrides","title":"Argument Overrides","text":"<p>jsonargparse allows you to flexibly combine configuration files and CLI arguments. You can override the argument values by specifying them in the CLI.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa\" \\\n  --eval_setup.batch_size 8\n</code></pre> <p>The value of <code>--eval_setup.batch_size</code> overrides the value defined in the config file of <code>commonsense_qa</code>.</p>"},{"location":"configuration_guide/#whats-next","title":"What's Next?","text":"<ul> <li>Proceed to How to to find examples that suit your needs.</li> <li>Look at the API reference to see the available classes and their arguments.</li> </ul>"},{"location":"design_principles/","title":"Design Principle","text":"<p><code>flexeval</code> is designed according to the following principles:</p> <ul> <li>Flexibility: <code>flexeval</code> should be flexible in terms of the evaluation setup and the language model to be evaluated.</li> <li>Modularity: The core components of <code>flexeval</code> should be easily extensible and replaceable.</li> <li>Clarity: The results of evaluation should be clear and easy to understand its configuration.</li> <li>Reproducibility: <code>flexeval</code> should be reproducible, with the ability to save and load configurations and results.</li> </ul> <p>To achieve flexibility and modularity, the core logic is implemented with abstract interfaces, and the concrete implementations are provided when running each CLI command.</p> <p>Thanks to jsonargparse, we can transparently specify the configuration of every component either via CLI arguments or jsonnet config files. Thus, when you want to use your own module, all you have to do is implement a concrete class inheriting the right interface and specify it in the configuration, without modifying the existing code.</p> <p>To achieve clarity and reproducibility, <code>flexeval</code> saves the configuration and the evaluation results in a directory specified by <code>--save_dir</code>. The resulting <code>config.json</code> file contains everything needed to replicate the evaluation, configuration of all modules, the version of <code>flexeval</code> and the installed packages.</p> <p>It is often a case that a small preprocessing in the data affects the evaluation results significantly. We would like to the config file tells us what preprocessing is done without we need to dig into the code. Thus we recommend loading datasets using a generic class such as <code>HFGenerationDataset</code> or <code>JsonlGenerationDataset</code> and specifying a preprocessing using their parameters or Jinja2 templates in the configuration file.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>Most evaluations can be done with the <code>flexeval_lm</code> command. With <code>--eval_setup</code> option, you can specify the task to evaluate.</p>"},{"location":"getting_started/#generation-tasks","title":"Generation Tasks","text":"<p>The following minimal example evaluates the hugging face model <code>sbintuitions/tiny-lm</code> with the <code>commonsense_qa</code> task.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa\" \\\n  --save_dir \"results/commonsense_qa\"\n</code></pre> <p>The results are saved in the directory specified by <code>--save_dir</code>.</p> <p>You can find three files: <code>config.json</code>, <code>metrics.json</code> and <code>outputs.jsonl</code>.</p>"},{"location":"getting_started/#configjson","title":"<code>config.json</code>","text":"<p>The <code>config.json</code> file contains the configuration of the evaluation, as well as metadata useful for replicating the evaluation.</p> <pre><code>{\n    \"eval_setup\": {\n        \"class_path\": \"flexeval.scripts.flexeval_lm.Generation\",\n        \"init_args\": {\n          \"eval_dataset\": ...,\n          \"prompt_template\": ...,\n          \"gen_kwargs\": ...,\n          \"metrics\": ...,\n          \"batch_size\": ...,\n        },\n    },\n    \"language_model\": {\n      \"class_path\": \"flexeval.core.language_model.HuggingFaceLM\",\n      \"init_args\": {\n        \"model\": \"sbintuitions/tiny-lm\",\n        ...\n      }\n    },\n    \"save_dir\": \"results/commonsense_qa\",\n    \"metadata\": ...\n}\n</code></pre> <p>Tip</p> <p>You can replicate the evaluation by specifying the saved config in <code>flexeval_lm</code>:</p> <pre><code>flexeval_lm --config \"results/commonsense_qa/config.json\" --save_dir \"results/commonsense_qa_replicated\"\n</code></pre>"},{"location":"getting_started/#metricsjson","title":"<code>metrics.json</code>","text":"<p>The <code>metrics.json</code> file contains the evaluation metrics.</p> <pre><code>{\n    \"exact_match\": 0.004914004914004914,\n}\n</code></pre>"},{"location":"getting_started/#outputsjsonl","title":"<code>outputs.jsonl</code>","text":"<p>The <code>outputs.jsonl</code> file contains the outputs of the language model with the following fields:</p> <ul> <li><code>lm_prompt</code>: The prompt used to generate the output.</li> <li><code>lm_output</code>: The output generated by the language model.</li> <li><code>extra_info</code>: The inputs of the task and some extra informations.</li> <li><code>references</code>: The references of the task.</li> <li>instance-level metrics (e.g., <code>exact_match</code>): The metrics computed for each instance.</li> </ul>"},{"location":"getting_started/#multiple-choice-tasks","title":"Multiple Choice Tasks","text":"<p>Some tasks are implemented as multiple choice tasks. The following example evaluates the model with the <code>commonsense_qa_mc</code> setup, which solves CommonsenseQA by choosing the answer with the highest probability.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa_mc\" \\\n  --save_dir \"results/commonsense_qa_mc\"\n</code></pre> <p>The results are basically the same as the generation tasks, but the <code>outputs.jsonl</code> file has a different format:</p> <ul> <li><code>prefix</code>: The prefix text before the choices.</li> <li><code>choices</code>: The choices of the task.</li> <li><code>answer_index</code>: The index of the correct choice.</li> <li><code>log_probs</code>: The log probabilities of each choice computed by the language model.</li> <li><code>prediction</code>: The index of the choice with the highest probability.</li> <li><code>byte_norm_log_probs</code>: The byte-normalized log probabilities of each choice.</li> <li><code>byte_norm_prediction</code>: The index of the choice with the highest byte-normalized probability.</li> </ul> <p>Whether to use <code>log_probs</code> or <code>byte_norm_log_probs</code> depends on the task, so both are provided.</p>"},{"location":"getting_started/#chat-models","title":"Chat Models","text":"<p>The examples so far are intended to evaluate pretrained language models in zero/few-shot settings. Evaluating chat models may require a different setup.</p> <pre><code>export OPENAI_API_KEY=\"YOUR_API_KEY\"\n\nflexeval_lm \\\n  --language_model OpenAIChatAPI \\\n  --language_model.model \"gpt-4o-mini\" \\\n  --eval_setup \"mt-en\" \\\n  --save_dir \"results/mt-en/gpt-4o-mini\"\n</code></pre> <p>Note</p> <p>You can also specify <code>HuggingFaceLM</code> for <code>--language_model</code> but the model should have a proper chat template.</p> <p><code>outputs.jsonl</code> contains the following fields:</p> <ul> <li><code>lm_output</code>: The response generated by the language model.</li> <li><code>extra_info</code>: The inputs of the task and some extra informations.</li> <li><code>messages</code>: The chat history except for the last turn.</li> <li><code>references</code>: The references of the task, if any.</li> <li>instance-level metrics (e.g., <code>output_length</code>): The metrics computed for each instance.</li> </ul> <p>Usually, the model outputs are evaluated by human evaluation or another LLM. The preset config only defines simple metrics such as length statistics.</p> <p>To run automatic evaluation with LLMs, you can use <code>outputs.jsonl</code> from the previous command and run the following command:</p> <pre><code>flexeval_file \\\n  --eval_file \"results/mt-en/gpt-4o-mini/outputs.jsonl\" \\\n  --metrics \"assistant_eval_en_single_turn\" \\\n  --save_dir \"results/mt-en/gpt-4o-mini/eval_by_gpt\"\n</code></pre> <p>In the results, you can see the evaluation result like <code>{\"llm_score\": 7.795}</code>. You can also check the entire output of the judge LLM including the rationale of the evaluation in <code>llm_score_output</code> in <code>outputs.jsonl</code>.</p> <p>For further details and pairwise evaluation, see Evaluate with LLM Judges.</p>"},{"location":"getting_started/#perplexity","title":"Perplexity","text":"<p>You can also compute perplexity of text with the following command:</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"tiny_shakespeare\" \\\n  --save_dir \"results/tiny_shakespeare\"\n</code></pre> <p>When evaluating perplexity, there is no <code>outputs.jsonl</code> file. The <code>metrics.json</code> file contains the perplexity values normalized by the number of tokens.</p> <pre><code>{\n    \"perplexity_per_byte\": 9.080868808532346,\n    \"perplexity_per_character\": 9.080868808532346\n}\n</code></pre> <p>Tip</p> <p>You can get <code>perplexity_per_token</code> by specifying the <code>--tokenizer</code> option.  By default, the command only computes tokenizer-agnostic metrics.</p>"},{"location":"getting_started/#whats-next","title":"What's Next?","text":"<ul> <li>Run <code>flexeval_presets</code> to check the list of off-the-shelf presets. You can find the details in the Preset Configs section.</li> <li><code>flexeval</code> allows you to evaluate any language model with any task, any prompt, and any metric. To understand how to configure the evaluation, proceed to Configuration Guide.</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p><code>flexeval</code> is tested on Python 3.8+.</p>"},{"location":"installation/#install-from-pip","title":"Install from pip","text":"<pre><code>pip install flexeval\n</code></pre> <p>Extras dependencies can be installed via pip install -e \".[NAME]\".</p> Name Description vllm To load language models using vLLM."},{"location":"installation/#install-from-source","title":"Install from source","text":"<pre><code>git clone https://github.com/sbintuitions/flexeval\ncd flexeval\npip install -e .\n</code></pre>"},{"location":"installation/#install-with-docker","title":"Install with Docker","text":"<pre><code>git clone https://github.com/sbintuitions/flexeval\ncd flexeval\ndocker build -t flexeval .\n</code></pre>"},{"location":"api_reference/","title":"API Reference","text":"<ul> <li>ChatDataset</li> <li>EvalSetup</li> <li>FewShotGenerator</li> <li>FunctionToolCall</li> <li>GenerationDataset</li> <li>HFMultipleChoiceDataset</li> <li>HFRewardBenchDataset</li> <li>LMOutput</li> <li>MatchMaker</li> <li>Metric</li> <li>PairwiseJudge</li> <li>PairwiseScorer</li> <li>PromptTemplate</li> <li>ResultRecorder</li> <li>RewardModel</li> <li>StringProcessor</li> <li>TextDataset</li> <li>Tokenizer</li> <li>utils</li> </ul>"},{"location":"api_reference/ChatDataset/","title":"ChatDataset","text":""},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatDataset","title":"ChatDataset","text":"<p>A dataset holding <code>ChatInstance</code>.</p> Source code in <code>flexeval/core/chat_dataset/base.py</code> <pre><code>class ChatDataset(Sequence[ChatInstance], ABC):\n    \"\"\"A dataset holding `ChatInstance`.\"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the number of chat instances in the dataset.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __getitem__(self, i: int) -&gt; ChatInstance:\n        \"\"\"\n        Returns the i-th chat instance.\n        \"\"\"\n        raise NotImplementedError\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatDataset.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of chat instances in the dataset.</p> Source code in <code>flexeval/core/chat_dataset/base.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"\n    Returns the number of chat instances in the dataset.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatDataset.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(i: int) -&gt; ChatInstance\n</code></pre> <p>Returns the i-th chat instance.</p> Source code in <code>flexeval/core/chat_dataset/base.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, i: int) -&gt; ChatInstance:\n    \"\"\"\n    Returns the i-th chat instance.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/chat_dataset/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance","title":"ChatInstance  <code>dataclass</code>","text":"<p>A dataclass representing a single chat that will be fed to a chat language model.</p> Source code in <code>flexeval/core/chat_dataset/base.py</code> <pre><code>@dataclass\nclass ChatInstance:\n    \"\"\"\n    A dataclass representing a single chat that will be fed to a chat language model.\n    \"\"\"\n\n    messages: list[dict[str, Any]]\n    \"\"\"\n    A list of messages in the chat.\n    The format of messages typically follows [OpenAI's Chat Completions API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api).\n    ```json\n    [\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Hello! How can I help you today?\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"I'd like to book a flight to Paris.\"\n        }\n    ]\n    ```\n\n    Tool-Calling message must follow the same format as the OpenAI ChatCompletion API.\n    https://platform.openai.com/docs/guides/function-calling?api-mode=chat#defining-functions\n    ```json\n    {\n        \"role\": \"assistant\",\n        \"content\": \"content\", # `None` is also allowed if `tool_calls` exists.\n        \"tool_calls\": [\n            {\n                \"id\": \"dummy1\",\n                \"function\": {\n                    \"name\": \"search_web\",\n                    \"arguments\": \"{\\\"query\\\": \\\"flexeval developer\\\"}\"  # Note that this is a json string, not a dictionary.\n                }\n            }\n        ]\n    }\n    ```\n\n    The results from tools should be represented as messages with the role \"tool\":\n    ```\n    {\n        \"role\": \"tool\",\n        \"tool_call_id\": \"dummy1\", # Optional, models on OpenAI APIs requires this field.\n        \"name\": \"search_web\", # Optional, Some HuggingFace models require this field.\n        \"content\": \"[{\\\"title\\\": \\\"sbintuitions/flexeval: Flexible evaluation tool...\\\", \\\"description\\\": \\\"...\\\"}]\",\n    }\n    \"\"\"  # noqa: E501\n    tools: list[dict[str, Any]] | None = None\n    \"\"\"\n    A list of definitions of tools in the chat.\n    The format of tools typically follows [OpenAI's Chat Completion API](https://platform.openai.com/docs/guides/function-calling#function-calling-steps)\n    Currently, only function calling (tools with type=\"function\") is supported.\n    \"\"\"\n    references: list[str] = field(default_factory=list)\n    \"\"\"\n    A list of reference responses to the user's last message.\n    The model's response will be evaluated against these references.\n    \"\"\"\n    extra_info: dict[str, Any] = field(default_factory=dict)\n    \"\"\"\n    Extra information that can be used by passing to `Metric`.\n    \"\"\"\n\n    def __post_init__(self) -&gt; None:\n        if \"messages\" in self.extra_info:\n            msg = (\n                \"'extra_info' in ChatInstance cannot contain a key named 'messages', \"\n                \"as it will conflict with the 'messages' attribute. \"\n                \"The key 'messages' will be removed.\"\n            )\n            warnings.warn(msg, stacklevel=2)\n            self.extra_info.pop(\"messages\")\n\n    @property\n    def inputs(self) -&gt; list[dict[str, str]]:\n        \"\"\"\n        Alias for `messages`.\n        This is used in `FewShotGenerator` so that it can access the inputs with the same attribute name as\n        `GenerationInstance` and `MultipleChoiceInstance`.\n        \"\"\"\n        return self.messages\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance.messages","title":"messages  <code>instance-attribute</code>","text":"<pre><code>messages: list[dict[str, Any]]\n</code></pre> <p>A list of messages in the chat. The format of messages typically follows OpenAI's Chat Completions API. <pre><code>[\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Hello! How can I help you today?\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"I'd like to book a flight to Paris.\"\n    }\n]\n</code></pre></p> <p>Tool-Calling message must follow the same format as the OpenAI ChatCompletion API. https://platform.openai.com/docs/guides/function-calling?api-mode=chat#defining-functions <pre><code>{\n    \"role\": \"assistant\",\n    \"content\": \"content\", # `None` is also allowed if `tool_calls` exists.\n    \"tool_calls\": [\n        {\n            \"id\": \"dummy1\",\n            \"function\": {\n                \"name\": \"search_web\",\n                \"arguments\": \"{\"query\": \"flexeval developer\"}\"  # Note that this is a json string, not a dictionary.\n            }\n        }\n    ]\n}\n</code></pre></p> <p>The results from tools should be represented as messages with the role \"tool\": ``` {     \"role\": \"tool\",     \"tool_call_id\": \"dummy1\", # Optional, models on OpenAI APIs requires this field.     \"name\": \"search_web\", # Optional, Some HuggingFace models require this field.     \"content\": \"[{\"title\": \"sbintuitions/flexeval: Flexible evaluation tool...\", \"description\": \"...\"}]\", }</p>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance.tools","title":"tools  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tools: list[dict[str, Any]] | None = None\n</code></pre> <p>A list of definitions of tools in the chat. The format of tools typically follows OpenAI's Chat Completion API Currently, only function calling (tools with type=\"function\") is supported.</p>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance.references","title":"references  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>references: list[str] = field(default_factory=list)\n</code></pre> <p>A list of reference responses to the user's last message. The model's response will be evaluated against these references.</p>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance.extra_info","title":"extra_info  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra_info: dict[str, Any] = field(default_factory=dict)\n</code></pre> <p>Extra information that can be used by passing to <code>Metric</code>.</p>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance.inputs","title":"inputs  <code>property</code>","text":"<pre><code>inputs: list[dict[str, str]]\n</code></pre> <p>Alias for <code>messages</code>. This is used in <code>FewShotGenerator</code> so that it can access the inputs with the same attribute name as <code>GenerationInstance</code> and <code>MultipleChoiceInstance</code>.</p>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance.__init__","title":"__init__","text":"<pre><code>__init__(\n    messages: list[dict[str, Any]],\n    tools: list[dict[str, Any]] | None = None,\n    references: list[str] = list(),\n    extra_info: dict[str, Any] = dict(),\n) -&gt; None\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.base.ChatInstance.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/chat_dataset/base.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if \"messages\" in self.extra_info:\n        msg = (\n            \"'extra_info' in ChatInstance cannot contain a key named 'messages', \"\n            \"as it will conflict with the 'messages' attribute. \"\n            \"The key 'messages' will be removed.\"\n        )\n        warnings.warn(msg, stacklevel=2)\n        self.extra_info.pop(\"messages\")\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.HFChatDataset","title":"HFChatDataset","text":"<p>Load ChatInstances from a Hugging Face dataset.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the Hugging Face dataset.</p> </li> <li> <code>split</code>               (<code>str</code>)           \u2013            <p>The split of the dataset.</p> </li> <li> <code>input_template</code>               (<code>str</code>)           \u2013            <p>A Jinja2 template for the user input.</p> </li> <li> <code>subset</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The subset of the dataset.</p> </li> <li> <code>dataset_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>The keyword arguments to pass to the Hugging Face dataset.</p> </li> </ul> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>class HFChatDataset(TemplateChatDataset):\n    \"\"\"\n    Load ChatInstances from a Hugging Face dataset.\n\n    Args:\n        path: The path to the Hugging Face dataset.\n        split: The split of the dataset.\n        input_template: A Jinja2 template for the user input.\n        subset: The subset of the dataset.\n        dataset_kwargs: The keyword arguments to pass to the Hugging Face dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        split: str,\n        input_template: str,\n        subset: str | None = None,\n        dataset_kwargs: dict[str, Any] | None = None,\n        reference_template: str | None = None,\n        reference_list_template: str | None = None,\n        extra_info_templates: dict[str, str] | None = None,\n        system_message_template: str | None = None,\n        tools: list[dict[str, Any]] | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        dataset_kwargs = dataset_kwargs or {}\n        dataset = datasets.load_dataset(path, name=subset, split=split, **dataset_kwargs)\n        items = [dict(item) for item in dataset]\n\n        super().__init__(\n            items=items,\n            input_template=input_template,\n            reference_template=reference_template,\n            reference_list_template=reference_list_template,\n            extra_info_templates=extra_info_templates,\n            system_message_template=system_message_template,\n            tools=tools,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.HFChatDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    path: str,\n    split: str,\n    input_template: str,\n    subset: str | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    extra_info_templates: dict[str, str] | None = None,\n    system_message_template: str | None = None,\n    tools: list[dict[str, Any]] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    split: str,\n    input_template: str,\n    subset: str | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    extra_info_templates: dict[str, str] | None = None,\n    system_message_template: str | None = None,\n    tools: list[dict[str, Any]] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    dataset_kwargs = dataset_kwargs or {}\n    dataset = datasets.load_dataset(path, name=subset, split=split, **dataset_kwargs)\n    items = [dict(item) for item in dataset]\n\n    super().__init__(\n        items=items,\n        input_template=input_template,\n        reference_template=reference_template,\n        reference_list_template=reference_list_template,\n        extra_info_templates=extra_info_templates,\n        system_message_template=system_message_template,\n        tools=tools,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.JsonlChatDataset","title":"JsonlChatDataset","text":"<p>Load ChatInstances from a JSONL file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the JSONL file.</p> </li> </ul> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>class JsonlChatDataset(TemplateChatDataset):\n    \"\"\"\n    Load ChatInstances from a JSONL file.\n\n    Args:\n        path: The path to the JSONL file.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        input_template: str,\n        reference_template: str | None = None,\n        reference_list_template: str | None = None,\n        extra_info_templates: dict[str, str] | None = None,\n        system_message_template: str | None = None,\n        tools: list[dict[str, Any]] | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        with open(path) as f:\n            items = [json.loads(line) for line in f]\n\n        super().__init__(\n            items=items,\n            input_template=input_template,\n            reference_template=reference_template,\n            reference_list_template=reference_list_template,\n            extra_info_templates=extra_info_templates,\n            system_message_template=system_message_template,\n            tools=tools,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.JsonlChatDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    path: str,\n    input_template: str,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    extra_info_templates: dict[str, str] | None = None,\n    system_message_template: str | None = None,\n    tools: list[dict[str, Any]] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    input_template: str,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    extra_info_templates: dict[str, str] | None = None,\n    system_message_template: str | None = None,\n    tools: list[dict[str, Any]] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    with open(path) as f:\n        items = [json.loads(line) for line in f]\n\n    super().__init__(\n        items=items,\n        input_template=input_template,\n        reference_template=reference_template,\n        reference_list_template=reference_list_template,\n        extra_info_templates=extra_info_templates,\n        system_message_template=system_message_template,\n        tools=tools,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset","title":"TemplateChatDataset","text":"<p>This class only supports single-turn chat.</p> <p>Parameters:</p> <ul> <li> <code>items</code>               (<code>list[dict[str, Any]]</code>)           \u2013            <p>A list of items in a dict format. The \"tools\" key for each item can contain the list of function definitions. They should be in JSON Schema format as in the OpenAI Chat Completion API. https://platform.openai.com/docs/guides/function-calling?api-mode=chat#defining-functions</p> </li> <li> <code>input_template</code>               (<code>str</code>)           \u2013            <p>A Jinja2 template for the user input.</p> </li> <li> <code>reference_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Specify the Jinja2 template to render the reference string if the dataset has a single reference.</p> </li> <li> <code>reference_list_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Specify the Jinja2 template to render a list of reference strings if the dataset has multiple references.</p> </li> <li> <code>extra_info_templates</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of Jinja2 templates for extra information.</p> </li> <li> <code>system_message_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A Jinja2 template for the system message.</p> </li> <li> <code>tools</code>               (<code>list[dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default tools to use for all chat instances. Individual items can override this by including their own \"tools\" key. Typically in JSON Schema format as in the OpenAI Chat Completion API for function calling.</p> </li> <li> <code>data_range</code>               (<code>tuple[int, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The range of data to use.</p> </li> <li> <code>keep_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to filter certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.</p> </li> <li> <code>remove_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to remove certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.</p> </li> </ul> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>class TemplateChatDataset(ChatDataset):\n    \"\"\"\n    This class only supports single-turn chat.\n\n    Args:\n        items: A list of items in a dict format.\n            The \"tools\" key for each item can contain the list of function definitions.\n            They should be in JSON Schema format as in the OpenAI Chat Completion API.\n            https://platform.openai.com/docs/guides/function-calling?api-mode=chat#defining-functions\n        input_template: A Jinja2 template for the user input.\n        reference_template: Specify the Jinja2 template to render the reference string\n            if the dataset has a single reference.\n        reference_list_template: Specify the Jinja2 template to render a list of reference strings\n            if the dataset has multiple references.\n        extra_info_templates: A dictionary of Jinja2 templates for extra information.\n        system_message_template: A Jinja2 template for the system message.\n        tools: Default tools to use for all chat instances. Individual items can override this\n            by including their own \"tools\" key. Typically in JSON Schema format as in the\n            OpenAI Chat Completion API for function calling.\n        data_range: The range of data to use.\n        keep_conditions: A dictionary to indicate the condition to filter certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.\n        remove_conditions: A dictionary to indicate the condition to remove certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.\n    \"\"\"\n\n    def __init__(\n        self,\n        items: list[dict[str, Any]],\n        input_template: str,\n        reference_template: str | None = None,\n        reference_list_template: str | None = None,\n        extra_info_templates: dict[str, str] | None = None,\n        system_message_template: str | None = None,\n        tools: list[dict[str, Any]] | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        if reference_template and reference_list_template:\n            msg = \"Only one of reference_template and reference_list_template can be set.\"\n            raise ValueError(msg)\n\n        if data_range:\n            start, end = data_range\n            items = items[start:end]\n\n        keep_conditions = keep_conditions or {}\n        for template_str, value_to_keep in keep_conditions.items():\n            key_template = JINJA2_ENV.from_string(template_str)\n            items = [item for item in items if key_template.render(**item) == value_to_keep]\n        remove_conditions = remove_conditions or {}\n        for template_str, value_to_remove in remove_conditions.items():\n            key_template = JINJA2_ENV.from_string(template_str)\n            items = [item for item in items if key_template.render(**item) != value_to_remove]\n\n        self.items = items\n        self.tools = tools\n\n        self.input_template = JINJA2_ENV.from_string(input_template)\n        self.reference_template = JINJA2_ENV.from_string(reference_template) if reference_template else None\n        self.reference_list_template = (\n            JINJA2_ENV.from_string(reference_list_template) if reference_list_template else None\n        )\n\n        extra_info_templates = extra_info_templates or {}\n        self._extra_info_templates: dict[str, Template] = {\n            key: JINJA2_ENV.from_string(template) for key, template in extra_info_templates.items()\n        }\n\n        self._system_message_template: Template | None = (\n            JINJA2_ENV.from_string(system_message_template) if system_message_template else None\n        )\n\n    def __len__(self) -&gt; int:\n        return len(self.items)\n\n    def __getitem__(self, i: int) -&gt; ChatInstance:\n        item = self.items[i]\n        input_utterance = self.input_template.render(**item)\n        messages = [{\"role\": \"user\", \"content\": input_utterance}]\n\n        if self._system_message_template:\n            system_message = self._system_message_template.render(**item)\n            messages.insert(0, {\"role\": \"system\", \"content\": system_message})\n\n        reference_list: list[str] = []\n        if self.reference_template:\n            reference_string = self.reference_template.render(**item)\n            reference_list.append(reference_string)\n        if self.reference_list_template:\n            reference_list_string = self.reference_list_template.render(**item)\n            if not (reference_list_string.startswith(\"[\") and reference_list_string.endswith(\"]\")):\n                msg = (\n                    f\"The reference_list_template should render a list of strings \"\n                    f\"but we got `{reference_list_string}`.\"\n                )\n                raise ValueError(msg)\n            reference_list.extend([str(ref) for ref in literal_eval(reference_list_string)])\n\n        extra_info = dict(item.items())\n        extra_info_from_templates = {\n            key: template.render(**item) for key, template in self._extra_info_templates.items()\n        }\n        extra_info.update(extra_info_from_templates)\n\n        return ChatInstance(\n            messages=messages,\n            tools=item.get(\"tools\") or self.tools,\n            references=reference_list,\n            extra_info=extra_info,\n        )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.items","title":"items  <code>instance-attribute</code>","text":"<pre><code>items = items\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools = tools\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.input_template","title":"input_template  <code>instance-attribute</code>","text":"<pre><code>input_template = from_string(input_template)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.reference_template","title":"reference_template  <code>instance-attribute</code>","text":"<pre><code>reference_template = (\n    from_string(reference_template)\n    if reference_template\n    else None\n)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.reference_list_template","title":"reference_list_template  <code>instance-attribute</code>","text":"<pre><code>reference_list_template = (\n    from_string(reference_list_template)\n    if reference_list_template\n    else None\n)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    items: list[dict[str, Any]],\n    input_template: str,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    extra_info_templates: dict[str, str] | None = None,\n    system_message_template: str | None = None,\n    tools: list[dict[str, Any]] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    items: list[dict[str, Any]],\n    input_template: str,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    extra_info_templates: dict[str, str] | None = None,\n    system_message_template: str | None = None,\n    tools: list[dict[str, Any]] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    if reference_template and reference_list_template:\n        msg = \"Only one of reference_template and reference_list_template can be set.\"\n        raise ValueError(msg)\n\n    if data_range:\n        start, end = data_range\n        items = items[start:end]\n\n    keep_conditions = keep_conditions or {}\n    for template_str, value_to_keep in keep_conditions.items():\n        key_template = JINJA2_ENV.from_string(template_str)\n        items = [item for item in items if key_template.render(**item) == value_to_keep]\n    remove_conditions = remove_conditions or {}\n    for template_str, value_to_remove in remove_conditions.items():\n        key_template = JINJA2_ENV.from_string(template_str)\n        items = [item for item in items if key_template.render(**item) != value_to_remove]\n\n    self.items = items\n    self.tools = tools\n\n    self.input_template = JINJA2_ENV.from_string(input_template)\n    self.reference_template = JINJA2_ENV.from_string(reference_template) if reference_template else None\n    self.reference_list_template = (\n        JINJA2_ENV.from_string(reference_list_template) if reference_list_template else None\n    )\n\n    extra_info_templates = extra_info_templates or {}\n    self._extra_info_templates: dict[str, Template] = {\n        key: JINJA2_ENV.from_string(template) for key, template in extra_info_templates.items()\n    }\n\n    self._system_message_template: Template | None = (\n        JINJA2_ENV.from_string(system_message_template) if system_message_template else None\n    )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.items)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.template_based.TemplateChatDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; ChatInstance\n</code></pre> Source code in <code>flexeval/core/chat_dataset/template_based.py</code> <pre><code>def __getitem__(self, i: int) -&gt; ChatInstance:\n    item = self.items[i]\n    input_utterance = self.input_template.render(**item)\n    messages = [{\"role\": \"user\", \"content\": input_utterance}]\n\n    if self._system_message_template:\n        system_message = self._system_message_template.render(**item)\n        messages.insert(0, {\"role\": \"system\", \"content\": system_message})\n\n    reference_list: list[str] = []\n    if self.reference_template:\n        reference_string = self.reference_template.render(**item)\n        reference_list.append(reference_string)\n    if self.reference_list_template:\n        reference_list_string = self.reference_list_template.render(**item)\n        if not (reference_list_string.startswith(\"[\") and reference_list_string.endswith(\"]\")):\n            msg = (\n                f\"The reference_list_template should render a list of strings \"\n                f\"but we got `{reference_list_string}`.\"\n            )\n            raise ValueError(msg)\n        reference_list.extend([str(ref) for ref in literal_eval(reference_list_string)])\n\n    extra_info = dict(item.items())\n    extra_info_from_templates = {\n        key: template.render(**item) for key, template in self._extra_info_templates.items()\n    }\n    extra_info.update(extra_info_from_templates)\n\n    return ChatInstance(\n        messages=messages,\n        tools=item.get(\"tools\") or self.tools,\n        references=reference_list,\n        extra_info=extra_info,\n    )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.chatbot_bench.ChatbotBench","title":"ChatbotBench","text":"<p>This class loads data with the jsonl format used in chat evaluation benchmarks such as MT-Bench (Multi-turn Benchmark) or Vicuna QA Benchmark.</p> Example of a line from a jsonl file <p>{   \"question_id\": 00,   \"category\": \"writing\",   \"turns\": [     \"Compose an engaging travel blog post about a recent trip to Hawaii.\",     \"Rewrite your previous response. Start every sentence with the letter A.\"   ]   # 'tools' key is optional.   # It should be in the same format as FunctionCalling in the OpenAI ChatCompletion API.   # https://platform.openai.com/docs/guides/function-calling?api-mode=chat#defining-functions   \"tools\": [     {       \"type\": \"function\",       \"function\": {         \"name\": \"get_weather\",         \"description\": \"Get current temperature for a given location.\",         \"parameters\": {           \"type\": \"object\",           \"properties\": {             \"location\": {\"type\": \"string\", \"description\": \"City and country e.g. Bogot\u00e1, Colombia\"},           },           \"required\": [\"location\"],           \"additionalProperties\": False},         \"strict\": True       },     },   ],   # 'system_message' key is optional.   # If set, it will be inserted in the first turn as a system prompt   \"system_message\": \"You are a helpful assistant.\" }</p> Source code in <code>flexeval/core/chat_dataset/chatbot_bench.py</code> <pre><code>class ChatbotBench(ChatDataset):\n    \"\"\"This class loads data with the jsonl format used in chat evaluation benchmarks such as\n    MT-Bench (Multi-turn Benchmark) or Vicuna QA Benchmark.\n\n    Example of a line from a jsonl file:\n        {\n          \"question_id\": 00,\n          \"category\": \"writing\",\n          \"turns\": [\n            \"Compose an engaging travel blog post about a recent trip to Hawaii.\",\n            \"Rewrite your previous response. Start every sentence with the letter A.\"\n          ]\n          # 'tools' key is optional.\n          # It should be in the same format as FunctionCalling in the OpenAI ChatCompletion API.\n          # https://platform.openai.com/docs/guides/function-calling?api-mode=chat#defining-functions\n          \"tools\": [\n            {\n              \"type\": \"function\",\n              \"function\": {\n                \"name\": \"get_weather\",\n                \"description\": \"Get current temperature for a given location.\",\n                \"parameters\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"location\": {\"type\": \"string\", \"description\": \"City and country e.g. Bogot\u00e1, Colombia\"},\n                  },\n                  \"required\": [\"location\"],\n                  \"additionalProperties\": False},\n                \"strict\": True\n              },\n            },\n          ],\n          # 'system_message' key is optional.\n          # If set, it will be inserted in the first turn as a system prompt\n          \"system_message\": \"You are a helpful assistant.\"\n        }\n    \"\"\"\n\n    def __init__(\n        self,\n        path_or_name: str,\n        ref_path_or_name: str | None = None,\n        need_ref_categories: list[str] | None = None,\n        load_only_first_n: int | None = None,\n    ) -&gt; None:\n        file_path = resolve_path_or_name(path_or_name)\n\n        self._id_to_question_id: list[int | str] = []\n        self._id_to_category: list[str] = []\n        self._messages_dict: dict[int | str, list[dict[str, str]]] = {}\n        self._tools_dict: dict[int | str, list[dict[str, Any] | None]] = {}\n        with open(file_path) as f:\n            for line in f:\n                item = json.loads(line)\n                self._id_to_question_id.append(item[\"question_id\"])\n                self._id_to_category.append(item[\"category\"])\n                input_messages = [{\"role\": \"user\", \"content\": turn} for turn in item[\"turns\"]]\n                if item.get(\"system_message\"):\n                    input_messages = [{\"role\": \"system\", \"content\": item[\"system_message\"]}, *input_messages]\n                if load_only_first_n is not None:\n                    input_messages = input_messages[:load_only_first_n]\n                self._messages_dict[item[\"question_id\"]] = input_messages\n                self._tools_dict[item[\"question_id\"]] = item.get(\"tools\")\n\n        self._references_dict: dict[int | str, list[str]] = {}\n        if ref_path_or_name is not None:\n            ref_file_path = resolve_path_or_name(ref_path_or_name)\n            with open(ref_file_path) as f:\n                for line in f:\n                    item = json.loads(line)\n                    self._references_dict[item[\"question_id\"]] = item[\"choices\"][0][\"turns\"]\n\n        self.need_ref_categories = need_ref_categories or [\n            \"math\",\n            \"coding\",\n            \"reasoning\",\n        ]\n\n    def __len__(self) -&gt; int:\n        return len(self._id_to_question_id)\n\n    def __getitem__(self, i: int) -&gt; ChatInstance:\n        question_id = self._id_to_question_id[i]\n        category = self._id_to_category[i]\n        references: list[str] = []\n        if category in self.need_ref_categories:\n            references = self._references_dict.get(question_id, [])\n        return ChatInstance(\n            self._messages_dict[question_id],\n            tools=self._tools_dict[question_id],\n            references=references,\n            extra_info={\"category\": category},\n        )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.chatbot_bench.ChatbotBench.need_ref_categories","title":"need_ref_categories  <code>instance-attribute</code>","text":"<pre><code>need_ref_categories = need_ref_categories or [\n    \"math\",\n    \"coding\",\n    \"reasoning\",\n]\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.chatbot_bench.ChatbotBench.__init__","title":"__init__","text":"<pre><code>__init__(\n    path_or_name: str,\n    ref_path_or_name: str | None = None,\n    need_ref_categories: list[str] | None = None,\n    load_only_first_n: int | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/chat_dataset/chatbot_bench.py</code> <pre><code>def __init__(\n    self,\n    path_or_name: str,\n    ref_path_or_name: str | None = None,\n    need_ref_categories: list[str] | None = None,\n    load_only_first_n: int | None = None,\n) -&gt; None:\n    file_path = resolve_path_or_name(path_or_name)\n\n    self._id_to_question_id: list[int | str] = []\n    self._id_to_category: list[str] = []\n    self._messages_dict: dict[int | str, list[dict[str, str]]] = {}\n    self._tools_dict: dict[int | str, list[dict[str, Any] | None]] = {}\n    with open(file_path) as f:\n        for line in f:\n            item = json.loads(line)\n            self._id_to_question_id.append(item[\"question_id\"])\n            self._id_to_category.append(item[\"category\"])\n            input_messages = [{\"role\": \"user\", \"content\": turn} for turn in item[\"turns\"]]\n            if item.get(\"system_message\"):\n                input_messages = [{\"role\": \"system\", \"content\": item[\"system_message\"]}, *input_messages]\n            if load_only_first_n is not None:\n                input_messages = input_messages[:load_only_first_n]\n            self._messages_dict[item[\"question_id\"]] = input_messages\n            self._tools_dict[item[\"question_id\"]] = item.get(\"tools\")\n\n    self._references_dict: dict[int | str, list[str]] = {}\n    if ref_path_or_name is not None:\n        ref_file_path = resolve_path_or_name(ref_path_or_name)\n        with open(ref_file_path) as f:\n            for line in f:\n                item = json.loads(line)\n                self._references_dict[item[\"question_id\"]] = item[\"choices\"][0][\"turns\"]\n\n    self.need_ref_categories = need_ref_categories or [\n        \"math\",\n        \"coding\",\n        \"reasoning\",\n    ]\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.chatbot_bench.ChatbotBench.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/chat_dataset/chatbot_bench.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._id_to_question_id)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.chatbot_bench.ChatbotBench.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; ChatInstance\n</code></pre> Source code in <code>flexeval/core/chat_dataset/chatbot_bench.py</code> <pre><code>def __getitem__(self, i: int) -&gt; ChatInstance:\n    question_id = self._id_to_question_id[i]\n    category = self._id_to_category[i]\n    references: list[str] = []\n    if category in self.need_ref_categories:\n        references = self._references_dict.get(question_id, [])\n    return ChatInstance(\n        self._messages_dict[question_id],\n        tools=self._tools_dict[question_id],\n        references=references,\n        extra_info={\"category\": category},\n    )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.openai_messages.OpenAIMessagesDataset","title":"OpenAIMessagesDataset","text":"<p>This class loads data with OpenAI-like format in jsonl file. The difference lies in that this class has 'tool_definition' field, in which available tools are listed.</p> <p>Tool-Calling (Function-Calling) is supported in this class. It must follow the same format as the OpenAI ChatCompletion API. https://platform.openai.com/docs/guides/function-calling?api-mode=chat#defining-functions</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>               (<code>str | list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Path or list of paths to <code>.jsonl</code> file(s).</p> </li> <li> <code>message_key</code>               (<code>str</code>, default:                   <code>'messages'</code> )           \u2013            <p>Key used to extract the list of messages from each JSON object.</p> </li> <li> <code>tool_definitions_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Key used to extract the list of tool definitions from each JSON object. Set to <code>None</code> (default) for data without tool_calls.</p> </li> <li> <code>drop_if_last_from_assistant</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If true, when the last utterance is given by assistant, drop it.</p> </li> </ul> <p>In Jsonl, each line must have a following structure: <pre><code>{\n  '&lt;message_key&gt;': [\n    {\n      'role': 'user',\n      'content': '\u3053\u3093\u306b\u3061\u306f\u3002\u5143\u6c17\u304c\u51fa\u308b\u8a00\u8449\u3092\u6559\u3048\u3066\u4e0b\u3055\u3044\u3002'\n    },\n    {\n      'role': 'assistant',\n      'content': '\u3053\u3093\u306a\u306e\u306f\u3069\u3046\u3067\u3057\u3087\u3046\u3002\u3069\u3093\u3069\u3093\u3084\u3063\u3066\u304f\u3060\u3055\u3044\uff01'\n    },\n  ],\n}\n</code></pre></p> <p>Example with tool-calling: <pre><code>{\n  '&lt;message_key&gt;': [\n    {\n      'role': 'user',\n      'content': '\u3053\u3093\u306b\u3061\u306f\u3002\u5143\u6c17\u304c\u51fa\u308b\u5049\u4eba\u306e\u8a00\u8449\u3092\u6559\u3048\u3066\u4e0b\u3055\u3044\u3002'\n    },\n    {\n      'role': 'assistant',\n      'content': '\u8abf\u3079\u3066\u307f\u307e\u3059\u306d\u3002',\n      'tool_calls': [\n        {\n          'id': 'dummy1',\n          'function': {\n            'name': 'web_search',\n            'arguments': '{\"query\": \"\u5143\u6c17\u304c\u51fa\u308b\u8a00\u8449 \u5049\u4eba\"}',\n          }\n        }\n      ]\n    }\n  ],\n  '&lt;tool_definitions_key&gt;': [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"web_search\",\n        ...\n      }\n    }\n  ]\n}\n</code></pre></p> Source code in <code>flexeval/core/chat_dataset/openai_messages.py</code> <pre><code>class OpenAIMessagesDataset(ChatDataset):\n    \"\"\"This class loads data with OpenAI-like format in jsonl file.\n    The difference lies in that this class has 'tool_definition' field, in which\n    available tools are listed.\n\n    Tool-Calling (Function-Calling) is supported in this class.\n    It must follow the same format as the OpenAI ChatCompletion API.\n    https://platform.openai.com/docs/guides/function-calling?api-mode=chat#defining-functions\n\n    Parameters:\n        file_path (str | list[str] | None): Path or list of paths to `.jsonl` file(s).\n        message_key (str): Key used to extract the list of messages from each JSON object.\n        tool_definitions_key (str | None): Key used to extract the list of tool definitions from each JSON object.\n            Set to `None` (default) for data without tool_calls.\n        drop_if_last_from_assistant (bool): If true, when the last utterance is given by assistant, drop it.\n\n    In Jsonl, each line must have a following structure:\n    ```json\n    {\n      '&lt;message_key&gt;': [\n        {\n          'role': 'user',\n          'content': '\u3053\u3093\u306b\u3061\u306f\u3002\u5143\u6c17\u304c\u51fa\u308b\u8a00\u8449\u3092\u6559\u3048\u3066\u4e0b\u3055\u3044\u3002'\n        },\n        {\n          'role': 'assistant',\n          'content': '\u3053\u3093\u306a\u306e\u306f\u3069\u3046\u3067\u3057\u3087\u3046\u3002\u3069\u3093\u3069\u3093\u3084\u3063\u3066\u304f\u3060\u3055\u3044\uff01'\n        },\n      ],\n    }\n    ```\n\n    Example with tool-calling:\n    ```json\n    {\n      '&lt;message_key&gt;': [\n        {\n          'role': 'user',\n          'content': '\u3053\u3093\u306b\u3061\u306f\u3002\u5143\u6c17\u304c\u51fa\u308b\u5049\u4eba\u306e\u8a00\u8449\u3092\u6559\u3048\u3066\u4e0b\u3055\u3044\u3002'\n        },\n        {\n          'role': 'assistant',\n          'content': '\u8abf\u3079\u3066\u307f\u307e\u3059\u306d\u3002',\n          'tool_calls': [\n            {\n              'id': 'dummy1',\n              'function': {\n                'name': 'web_search',\n                'arguments': '{\"query\": \"\u5143\u6c17\u304c\u51fa\u308b\u8a00\u8449 \u5049\u4eba\"}',\n              }\n            }\n          ]\n        }\n      ],\n      '&lt;tool_definitions_key&gt;': [\n        {\n          \"type\": \"function\",\n          \"function\": {\n            \"name\": \"web_search\",\n            ...\n          }\n        }\n      ]\n    }\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        file_path: str | None = None,\n        message_key: str = \"messages\",\n        tool_definitions_key: str | None = None,\n        drop_if_last_from_assistant: bool = False,\n    ) -&gt; None:\n        self.conversations: list[ChatInstance] = []\n        with open(file_path) as f:\n            dataset = [json.loads(line) for line in f]\n        for sample in dataset:\n            tool_dicts = None\n            if tool_definitions_key is not None:\n                tool_dicts = sample.get(tool_definitions_key, None)\n\n            messages: list[dict[str, Any]] = sample.pop(message_key)\n            if drop_if_last_from_assistant and messages[-1][\"role\"] == \"assistant\":\n                messages = messages[:-1]\n            self.conversations.append(ChatInstance(messages=messages, tools=tool_dicts, extra_info=sample))\n\n    def __len__(self) -&gt; int:\n        return len(self.conversations)\n\n    def __getitem__(self, idx: int) -&gt; ChatInstance:\n        return self.conversations[idx]\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.openai_messages.OpenAIMessagesDataset.conversations","title":"conversations  <code>instance-attribute</code>","text":"<pre><code>conversations: list[ChatInstance] = []\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.openai_messages.OpenAIMessagesDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    file_path: str | None = None,\n    message_key: str = \"messages\",\n    tool_definitions_key: str | None = None,\n    drop_if_last_from_assistant: bool = False,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/chat_dataset/openai_messages.py</code> <pre><code>def __init__(\n    self,\n    file_path: str | None = None,\n    message_key: str = \"messages\",\n    tool_definitions_key: str | None = None,\n    drop_if_last_from_assistant: bool = False,\n) -&gt; None:\n    self.conversations: list[ChatInstance] = []\n    with open(file_path) as f:\n        dataset = [json.loads(line) for line in f]\n    for sample in dataset:\n        tool_dicts = None\n        if tool_definitions_key is not None:\n            tool_dicts = sample.get(tool_definitions_key, None)\n\n        messages: list[dict[str, Any]] = sample.pop(message_key)\n        if drop_if_last_from_assistant and messages[-1][\"role\"] == \"assistant\":\n            messages = messages[:-1]\n        self.conversations.append(ChatInstance(messages=messages, tools=tool_dicts, extra_info=sample))\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.openai_messages.OpenAIMessagesDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/chat_dataset/openai_messages.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.conversations)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.openai_messages.OpenAIMessagesDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx: int) -&gt; ChatInstance\n</code></pre> Source code in <code>flexeval/core/chat_dataset/openai_messages.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; ChatInstance:\n    return self.conversations[idx]\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.sacrebleu_dataset.SacreBleuChatDataset","title":"SacreBleuChatDataset","text":"<p>Load datasets from the sacrebleu library. The available datasets are defined in sacrebleu.DATASETS.</p> Source code in <code>flexeval/core/chat_dataset/sacrebleu_dataset.py</code> <pre><code>class SacreBleuChatDataset(ChatDataset):\n    \"\"\"Load datasets from the [sacrebleu](https://github.com/mjpost/sacrebleu) library.\n    The available datasets are defined in sacrebleu.DATASETS.\n    \"\"\"\n\n    def __init__(self, name: str, langpair: str) -&gt; None:\n        self._source_list: list[str] = list(sacrebleu.DATASETS[name].source(langpair))\n        self._references_list: list[list[str]] = [\n            [r.strip() for r in refs] for refs in sacrebleu.DATASETS[name].references(langpair)\n        ]\n\n        if len(self._source_list) != len(self._references_list):\n            msg = \"The number of source and reference pairs should be the same.\"\n            raise ValueError(msg)\n\n    def __len__(self) -&gt; int:\n        return len(self._source_list)\n\n    def __getitem__(self, i: int) -&gt; ChatInstance:\n        return ChatInstance(\n            messages=[{\"role\": \"user\", \"content\": self._source_list[i]}],\n            references=self._references_list[i],\n            extra_info={},\n        )\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.sacrebleu_dataset.SacreBleuChatDataset.__init__","title":"__init__","text":"<pre><code>__init__(name: str, langpair: str) -&gt; None\n</code></pre> Source code in <code>flexeval/core/chat_dataset/sacrebleu_dataset.py</code> <pre><code>def __init__(self, name: str, langpair: str) -&gt; None:\n    self._source_list: list[str] = list(sacrebleu.DATASETS[name].source(langpair))\n    self._references_list: list[list[str]] = [\n        [r.strip() for r in refs] for refs in sacrebleu.DATASETS[name].references(langpair)\n    ]\n\n    if len(self._source_list) != len(self._references_list):\n        msg = \"The number of source and reference pairs should be the same.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.sacrebleu_dataset.SacreBleuChatDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/chat_dataset/sacrebleu_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._source_list)\n</code></pre>"},{"location":"api_reference/ChatDataset/#flexeval.core.chat_dataset.sacrebleu_dataset.SacreBleuChatDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; ChatInstance\n</code></pre> Source code in <code>flexeval/core/chat_dataset/sacrebleu_dataset.py</code> <pre><code>def __getitem__(self, i: int) -&gt; ChatInstance:\n    return ChatInstance(\n        messages=[{\"role\": \"user\", \"content\": self._source_list[i]}],\n        references=self._references_list[i],\n        extra_info={},\n    )\n</code></pre>"},{"location":"api_reference/EvalSetup/","title":"EvalSetup","text":""},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.EvalSetup","title":"EvalSetup","text":"<p>Abstract class to give evaluation functions a common interface.</p> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>class EvalSetup(ABC):\n    \"\"\"Abstract class to give evaluation functions a common interface.\"\"\"\n\n    @abstractmethod\n    def evaluate_lm(\n        self,\n        language_model: LanguageModel,\n    ) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n        pass\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.EvalSetup.evaluate_lm","title":"evaluate_lm  <code>abstractmethod</code>","text":"<pre><code>evaluate_lm(\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]\n</code></pre> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>@abstractmethod\ndef evaluate_lm(\n    self,\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n    pass\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse","title":"ChatResponse  <code>dataclass</code>","text":"<p>Evaluation setup for chat response generation. In this setup, the model receives context in a chat format like: <pre><code>[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"}\n]\n</code></pre></p> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>@dataclass\nclass ChatResponse(EvalSetup):\n    \"\"\"\n    Evaluation setup for chat response generation.\n    In this setup, the model receives context in a chat format like:\n    ```json\n    [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n        {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"}\n    ]\n    ```\n    \"\"\"\n\n    eval_dataset: ChatDataset\n    gen_kwargs: dict[str, Any] = field(default_factory=dict)\n    few_shot_generator: FewShotGenerator | None = None\n    metrics: list[Metric] | Metric | None = None\n    batch_size: int = 4\n    max_instances: int | None = None\n\n    def evaluate_lm(\n        self,\n        language_model: LanguageModel,\n    ) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n        metrics = self.metrics or []\n        if isinstance(metrics, Metric):\n            metrics = [metrics]\n        metrics += [FinishReasonCount(), OutputLengthStats()]\n\n        return evaluate_chat_response(\n            language_model=language_model,\n            gen_kwargs=self.gen_kwargs,\n            eval_dataset=self.eval_dataset,\n            metrics=metrics,\n            batch_size=self.batch_size,\n            max_instances=self.max_instances,\n            few_shot_generator=self.few_shot_generator,\n        )\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.eval_dataset","title":"eval_dataset  <code>instance-attribute</code>","text":"<pre><code>eval_dataset: ChatDataset\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.gen_kwargs","title":"gen_kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gen_kwargs: dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.few_shot_generator","title":"few_shot_generator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>few_shot_generator: FewShotGenerator | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.metrics","title":"metrics  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metrics: list[Metric] | Metric | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 4\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.max_instances","title":"max_instances  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_instances: int | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.__init__","title":"__init__","text":"<pre><code>__init__(\n    eval_dataset: ChatDataset,\n    gen_kwargs: dict[str, Any] = dict(),\n    few_shot_generator: FewShotGenerator | None = None,\n    metrics: list[Metric] | Metric | None = None,\n    batch_size: int = 4,\n    max_instances: int | None = None,\n) -&gt; None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.ChatResponse.evaluate_lm","title":"evaluate_lm","text":"<pre><code>evaluate_lm(\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]\n</code></pre> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>def evaluate_lm(\n    self,\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n    metrics = self.metrics or []\n    if isinstance(metrics, Metric):\n        metrics = [metrics]\n    metrics += [FinishReasonCount(), OutputLengthStats()]\n\n    return evaluate_chat_response(\n        language_model=language_model,\n        gen_kwargs=self.gen_kwargs,\n        eval_dataset=self.eval_dataset,\n        metrics=metrics,\n        batch_size=self.batch_size,\n        max_instances=self.max_instances,\n        few_shot_generator=self.few_shot_generator,\n    )\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation","title":"Generation  <code>dataclass</code>","text":"<p>Evaluation setup for text generation. The model receives a prompt in a plain text format and generates its continuation.</p> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>@dataclass\nclass Generation(EvalSetup):\n    \"\"\"\n    Evaluation setup for text generation.\n    The model receives a prompt in a plain text format and generates its continuation.\n    \"\"\"\n\n    eval_dataset: GenerationDataset\n    prompt_template: PromptTemplate | str\n    gen_kwargs: dict[str, Any] = field(default_factory=dict)\n    few_shot_generator: FewShotGenerator | None = None\n    metrics: list[Metric] | Metric | None = None\n    batch_size: int = 4\n    max_instances: int | None = None\n\n    def __post_init__(self) -&gt; None:\n        if isinstance(self.prompt_template, str):\n            self.prompt_template = instantiate_prompt_template_from_string(self.prompt_template)\n\n    def evaluate_lm(\n        self,\n        language_model: LanguageModel,\n    ) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n        metrics = self.metrics or []\n        if isinstance(metrics, Metric):\n            metrics = [metrics]\n        metrics += [FinishReasonCount(), OutputLengthStats()]\n\n        return evaluate_generation(\n            language_model=language_model,\n            gen_kwargs=self.gen_kwargs,\n            eval_dataset=self.eval_dataset,\n            prompt_template=self.prompt_template,\n            few_shot_generator=self.few_shot_generator,\n            metrics=metrics,\n            batch_size=self.batch_size,\n            max_instances=self.max_instances,\n        )\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.eval_dataset","title":"eval_dataset  <code>instance-attribute</code>","text":"<pre><code>eval_dataset: GenerationDataset\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template: PromptTemplate | str\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.gen_kwargs","title":"gen_kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gen_kwargs: dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.few_shot_generator","title":"few_shot_generator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>few_shot_generator: FewShotGenerator | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.metrics","title":"metrics  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metrics: list[Metric] | Metric | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 4\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.max_instances","title":"max_instances  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_instances: int | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.__init__","title":"__init__","text":"<pre><code>__init__(\n    eval_dataset: GenerationDataset,\n    prompt_template: PromptTemplate | str,\n    gen_kwargs: dict[str, Any] = dict(),\n    few_shot_generator: FewShotGenerator | None = None,\n    metrics: list[Metric] | Metric | None = None,\n    batch_size: int = 4,\n    max_instances: int | None = None,\n) -&gt; None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if isinstance(self.prompt_template, str):\n        self.prompt_template = instantiate_prompt_template_from_string(self.prompt_template)\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Generation.evaluate_lm","title":"evaluate_lm","text":"<pre><code>evaluate_lm(\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]\n</code></pre> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>def evaluate_lm(\n    self,\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n    metrics = self.metrics or []\n    if isinstance(metrics, Metric):\n        metrics = [metrics]\n    metrics += [FinishReasonCount(), OutputLengthStats()]\n\n    return evaluate_generation(\n        language_model=language_model,\n        gen_kwargs=self.gen_kwargs,\n        eval_dataset=self.eval_dataset,\n        prompt_template=self.prompt_template,\n        few_shot_generator=self.few_shot_generator,\n        metrics=metrics,\n        batch_size=self.batch_size,\n        max_instances=self.max_instances,\n    )\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice","title":"MultipleChoice  <code>dataclass</code>","text":"<p>Evaluation setup for multiple choice questions. The model receives a prompt and a list of choices and selects the answer with the highest probability.</p> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>@dataclass\nclass MultipleChoice(EvalSetup):\n    \"\"\"\n    Evaluation setup for multiple choice questions.\n    The model receives a prompt and a list of choices and selects the answer with the highest probability.\n    \"\"\"\n\n    eval_dataset: MultipleChoiceDataset\n    prompt_template: PromptTemplate | str\n    few_shot_generator: FewShotGenerator | None = None\n    batch_size: int = 4\n    max_instances: int | None = None\n\n    def __post_init__(self) -&gt; None:\n        if isinstance(self.prompt_template, str):\n            self.prompt_template = instantiate_prompt_template_from_string(self.prompt_template)\n\n    def evaluate_lm(\n        self,\n        language_model: LanguageModel,\n    ) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n        return evaluate_multiple_choice(\n            language_model=language_model,\n            eval_dataset=self.eval_dataset,\n            prompt_template=self.prompt_template,\n            few_shot_generator=self.few_shot_generator,\n            batch_size=self.batch_size,\n            max_instances=self.max_instances,\n        )\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.eval_dataset","title":"eval_dataset  <code>instance-attribute</code>","text":"<pre><code>eval_dataset: MultipleChoiceDataset\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template: PromptTemplate | str\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.few_shot_generator","title":"few_shot_generator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>few_shot_generator: FewShotGenerator | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 4\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.max_instances","title":"max_instances  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_instances: int | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.__init__","title":"__init__","text":"<pre><code>__init__(\n    eval_dataset: MultipleChoiceDataset,\n    prompt_template: PromptTemplate | str,\n    few_shot_generator: FewShotGenerator | None = None,\n    batch_size: int = 4,\n    max_instances: int | None = None,\n) -&gt; None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if isinstance(self.prompt_template, str):\n        self.prompt_template = instantiate_prompt_template_from_string(self.prompt_template)\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.MultipleChoice.evaluate_lm","title":"evaluate_lm","text":"<pre><code>evaluate_lm(\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]\n</code></pre> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>def evaluate_lm(\n    self,\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n    return evaluate_multiple_choice(\n        language_model=language_model,\n        eval_dataset=self.eval_dataset,\n        prompt_template=self.prompt_template,\n        few_shot_generator=self.few_shot_generator,\n        batch_size=self.batch_size,\n        max_instances=self.max_instances,\n    )\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity","title":"Perplexity  <code>dataclass</code>","text":"<p>Evaluation setup for perplexity. The model receives plain text and computes the perplexity of the text.</p> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>@dataclass\nclass Perplexity(EvalSetup):\n    \"\"\"\n    Evaluation setup for perplexity.\n    The model receives plain text and computes the perplexity of the text.\n    \"\"\"\n\n    eval_dataset: TextDataset\n    batch_size: int = 4\n    tokenizer: Tokenizer | None = None\n    max_instances: int | None = None\n\n    def evaluate_lm(\n        self,\n        language_model: LanguageModel,\n    ) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n        metrics = evaluate_perplexity(\n            language_model=language_model,\n            eval_dataset=self.eval_dataset,\n            batch_size=self.batch_size,\n            tokenizer=self.tokenizer,\n            max_instances=self.max_instances,\n        )\n        return metrics, None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity.eval_dataset","title":"eval_dataset  <code>instance-attribute</code>","text":"<pre><code>eval_dataset: TextDataset\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: int = 4\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity.tokenizer","title":"tokenizer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tokenizer: Tokenizer | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity.max_instances","title":"max_instances  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_instances: int | None = None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity.__init__","title":"__init__","text":"<pre><code>__init__(\n    eval_dataset: TextDataset,\n    batch_size: int = 4,\n    tokenizer: Tokenizer | None = None,\n    max_instances: int | None = None,\n) -&gt; None\n</code></pre>"},{"location":"api_reference/EvalSetup/#flexeval.core.eval_setups.Perplexity.evaluate_lm","title":"evaluate_lm","text":"<pre><code>evaluate_lm(\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]\n</code></pre> Source code in <code>flexeval/core/eval_setups.py</code> <pre><code>def evaluate_lm(\n    self,\n    language_model: LanguageModel,\n) -&gt; tuple[dict[str, float], list[dict[str, Any]] | None]:\n    metrics = evaluate_perplexity(\n        language_model=language_model,\n        eval_dataset=self.eval_dataset,\n        batch_size=self.batch_size,\n        tokenizer=self.tokenizer,\n        max_instances=self.max_instances,\n    )\n    return metrics, None\n</code></pre>"},{"location":"api_reference/FewShotGenerator/","title":"FewShotGenerator","text":""},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.base.FewShotGenerator","title":"FewShotGenerator","text":"Source code in <code>flexeval/core/few_shot_generator/base.py</code> <pre><code>class FewShotGenerator(ABC):\n    def __init__(self, num_trials_to_avoid_leak: int) -&gt; None:\n        self._num_trials_to_avoid_leak = num_trials_to_avoid_leak\n\n    @abstractmethod\n    def _sample_instances(self, eval_inputs: list[dict[str, Any]] | dict[str, Any] | None = None) -&gt; list[Instance]:\n        \"\"\"\n        Sample instances for few-shot learning.\n        This method should be implemented in the derived class.\n        \"\"\"\n        raise NotImplementedError\n\n    def __call__(self, eval_inputs: list[dict[str, Any]] | dict[str, Any] | None = None) -&gt; list[Instance]:\n        \"\"\"\n        Sample instances for few-shot learning.\n        This method calls `_sample_instances` and\n        checks if the sampled instances have the same inputs as the evaluation instance.\n\n        Args:\n            eval_inputs: The inputs of the evaluation instance.\n                This is used to avoid data leakage\n                by checking if the sampled instances have the same inputs as the evaluation instance.\n\n        Returns:\n            A list of instances for few-shot learning.\n        \"\"\"\n        sampled_instances = self._sample_instances(eval_inputs=eval_inputs)\n\n        # check if the sampled instances are the same as the eval_instance\n        if self._num_trials_to_avoid_leak and eval_inputs is not None:\n            for _ in range(self._num_trials_to_avoid_leak):\n                if all(sampled.inputs != eval_inputs for sampled in sampled_instances):\n                    return sampled_instances\n                # retry sampling\n                sampled_instances = self._sample_instances(eval_inputs=eval_inputs)\n\n            msg = (\n                f\"Few-shot instance has the same inputs as the evaluation instance, \"\n                f\"which indicates a data leak. \"\n                f\"Failed to sample a different instance after {self._num_trials_to_avoid_leak} trials.\"\n            )\n            raise ValueError(msg)\n\n        return sampled_instances\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.base.FewShotGenerator.__init__","title":"__init__","text":"<pre><code>__init__(num_trials_to_avoid_leak: int) -&gt; None\n</code></pre> Source code in <code>flexeval/core/few_shot_generator/base.py</code> <pre><code>def __init__(self, num_trials_to_avoid_leak: int) -&gt; None:\n    self._num_trials_to_avoid_leak = num_trials_to_avoid_leak\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.base.FewShotGenerator.__call__","title":"__call__","text":"<pre><code>__call__(\n    eval_inputs: list[dict[str, Any]]\n    | dict[str, Any]\n    | None = None,\n) -&gt; list[Instance]\n</code></pre> <p>Sample instances for few-shot learning. This method calls <code>_sample_instances</code> and checks if the sampled instances have the same inputs as the evaluation instance.</p> <p>Parameters:</p> <ul> <li> <code>eval_inputs</code>               (<code>list[dict[str, Any]] | dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>The inputs of the evaluation instance. This is used to avoid data leakage by checking if the sampled instances have the same inputs as the evaluation instance.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Instance]</code>           \u2013            <p>A list of instances for few-shot learning.</p> </li> </ul> Source code in <code>flexeval/core/few_shot_generator/base.py</code> <pre><code>def __call__(self, eval_inputs: list[dict[str, Any]] | dict[str, Any] | None = None) -&gt; list[Instance]:\n    \"\"\"\n    Sample instances for few-shot learning.\n    This method calls `_sample_instances` and\n    checks if the sampled instances have the same inputs as the evaluation instance.\n\n    Args:\n        eval_inputs: The inputs of the evaluation instance.\n            This is used to avoid data leakage\n            by checking if the sampled instances have the same inputs as the evaluation instance.\n\n    Returns:\n        A list of instances for few-shot learning.\n    \"\"\"\n    sampled_instances = self._sample_instances(eval_inputs=eval_inputs)\n\n    # check if the sampled instances are the same as the eval_instance\n    if self._num_trials_to_avoid_leak and eval_inputs is not None:\n        for _ in range(self._num_trials_to_avoid_leak):\n            if all(sampled.inputs != eval_inputs for sampled in sampled_instances):\n                return sampled_instances\n            # retry sampling\n            sampled_instances = self._sample_instances(eval_inputs=eval_inputs)\n\n        msg = (\n            f\"Few-shot instance has the same inputs as the evaluation instance, \"\n            f\"which indicates a data leak. \"\n            f\"Failed to sample a different instance after {self._num_trials_to_avoid_leak} trials.\"\n        )\n        raise ValueError(msg)\n\n    return sampled_instances\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.balanced.BalancedFewShotGenerator","title":"BalancedFewShotGenerator","text":"Source code in <code>flexeval/core/few_shot_generator/balanced.py</code> <pre><code>class BalancedFewShotGenerator(FewShotGenerator):\n    def __init__(\n        self,\n        dataset: GenerationDataset,\n        num_shots: int,\n        seed: int = 42,\n        num_trials_to_avoid_leak: int = 3,\n    ) -&gt; None:\n        super().__init__(num_trials_to_avoid_leak=num_trials_to_avoid_leak)\n        if not isinstance(dataset, GenerationDataset):\n            msg = \"BalancedFewShotGenerator only supports GenerationDataset\"\n            raise TypeError(msg)\n\n        if num_shots &gt; len(dataset):\n            msg = (\n                f\"`num_shots` should be less than or equal to the number of instances in `dataset`. \"\n                f\"num_shots: {num_shots}, len(dataset): {len(dataset)}\"\n            )\n            raise ValueError(msg)\n\n        self.dataset = dataset\n        self.num_shots = num_shots\n        self._rnd = random.Random(seed)\n\n        # Separate instances by label\n        # Here we assume that the label is the first element of references of the instance.\n        label_to_ids: dict[str, list[int]] = defaultdict(list)\n        for i, instance in enumerate(dataset):\n            label_to_ids[instance.references[0]].append(i)\n        self._label_to_ids = label_to_ids\n\n    def _sample_instances(\n        self,\n        eval_inputs: list[dict[str, Any]] | dict[str, Any] | None = None,\n    ) -&gt; list[GenerationInstance]:\n        # Shuffle labels\n        labels = list(self._label_to_ids.keys())\n        self._rnd.shuffle(labels)\n\n        # Evenly distribute num_samples to each label\n        num_samples_list = [self.num_shots // len(labels)] * len(labels)\n        remaining_samples = self.num_shots % len(labels)\n        for i in range(remaining_samples):\n            num_samples_list[i] += 1\n\n        # Sample instances from each label\n        sampled_indices: list[int] = []\n        for label, num_samples_for_the_label in zip(labels, num_samples_list):\n            sampled_indices += self._rnd.sample(\n                self._label_to_ids[label],\n                num_samples_for_the_label,\n            )\n        self._rnd.shuffle(sampled_indices)\n\n        return [self.dataset[i] for i in sampled_indices]\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(dataset={self.dataset}, num_shots={self.num_shots})\"\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.balanced.BalancedFewShotGenerator.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset = dataset\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.balanced.BalancedFewShotGenerator.num_shots","title":"num_shots  <code>instance-attribute</code>","text":"<pre><code>num_shots = num_shots\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.balanced.BalancedFewShotGenerator.__init__","title":"__init__","text":"<pre><code>__init__(\n    dataset: GenerationDataset,\n    num_shots: int,\n    seed: int = 42,\n    num_trials_to_avoid_leak: int = 3,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/few_shot_generator/balanced.py</code> <pre><code>def __init__(\n    self,\n    dataset: GenerationDataset,\n    num_shots: int,\n    seed: int = 42,\n    num_trials_to_avoid_leak: int = 3,\n) -&gt; None:\n    super().__init__(num_trials_to_avoid_leak=num_trials_to_avoid_leak)\n    if not isinstance(dataset, GenerationDataset):\n        msg = \"BalancedFewShotGenerator only supports GenerationDataset\"\n        raise TypeError(msg)\n\n    if num_shots &gt; len(dataset):\n        msg = (\n            f\"`num_shots` should be less than or equal to the number of instances in `dataset`. \"\n            f\"num_shots: {num_shots}, len(dataset): {len(dataset)}\"\n        )\n        raise ValueError(msg)\n\n    self.dataset = dataset\n    self.num_shots = num_shots\n    self._rnd = random.Random(seed)\n\n    # Separate instances by label\n    # Here we assume that the label is the first element of references of the instance.\n    label_to_ids: dict[str, list[int]] = defaultdict(list)\n    for i, instance in enumerate(dataset):\n        label_to_ids[instance.references[0]].append(i)\n    self._label_to_ids = label_to_ids\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.balanced.BalancedFewShotGenerator.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/few_shot_generator/balanced.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(dataset={self.dataset}, num_shots={self.num_shots})\"\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.fixed.FixedFewShotGenerator","title":"FixedFewShotGenerator","text":"Source code in <code>flexeval/core/few_shot_generator/fixed.py</code> <pre><code>class FixedFewShotGenerator(FewShotGenerator):\n    def __init__(self, instance_class: str, instance_params: list[dict[str, Any]]) -&gt; None:\n        super().__init__(num_trials_to_avoid_leak=0)\n\n        if instance_class == \"GenerationInstance\":\n            instance_init = GenerationInstance\n        elif instance_class == \"MultipleChoiceInstance\":\n            instance_init = MultipleChoiceInstance\n        elif instance_class == \"ChatInstance\":\n            instance_init = ChatInstance\n        else:\n            msg = f\"Unknown instance class: {instance_class}\"\n            raise ValueError(msg)\n\n        self.instances = [instance_init(**params) for params in instance_params]\n\n    def _sample_instances(self, eval_inputs: list[dict[str, Any]] | dict[str, Any] | None = None) -&gt; list[Instance]:\n        return self.instances\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(instances={self.instances})\"\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.fixed.FixedFewShotGenerator.instances","title":"instances  <code>instance-attribute</code>","text":"<pre><code>instances = [\n    instance_init(**params) for params in instance_params\n]\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.fixed.FixedFewShotGenerator.__init__","title":"__init__","text":"<pre><code>__init__(\n    instance_class: str,\n    instance_params: list[dict[str, Any]],\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/few_shot_generator/fixed.py</code> <pre><code>def __init__(self, instance_class: str, instance_params: list[dict[str, Any]]) -&gt; None:\n    super().__init__(num_trials_to_avoid_leak=0)\n\n    if instance_class == \"GenerationInstance\":\n        instance_init = GenerationInstance\n    elif instance_class == \"MultipleChoiceInstance\":\n        instance_init = MultipleChoiceInstance\n    elif instance_class == \"ChatInstance\":\n        instance_init = ChatInstance\n    else:\n        msg = f\"Unknown instance class: {instance_class}\"\n        raise ValueError(msg)\n\n    self.instances = [instance_init(**params) for params in instance_params]\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.fixed.FixedFewShotGenerator.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/few_shot_generator/fixed.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(instances={self.instances})\"\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.rand.RandomFewShotGenerator","title":"RandomFewShotGenerator","text":"Source code in <code>flexeval/core/few_shot_generator/rand.py</code> <pre><code>class RandomFewShotGenerator(FewShotGenerator):\n    def __init__(\n        self,\n        dataset: Dataset,\n        num_shots: int,\n        seed: int = 42,\n        num_trials_to_avoid_leak: int = 3,\n    ) -&gt; None:\n        super().__init__(num_trials_to_avoid_leak=num_trials_to_avoid_leak)\n\n        if num_shots &gt; len(dataset):\n            msg = (\n                f\"`num_shots` should be less than or equal to the number of instances in `dataset`. \"\n                f\"num_shots: {num_shots}, len(dataset): {len(dataset)}\"\n            )\n            raise ValueError(msg)\n\n        self.dataset = dataset\n        self.num_shots = num_shots\n        self._rnd = random.Random(seed)\n\n    def _sample_instances(self, eval_inputs: list[dict[str, Any]] | dict[str, Any] | None = None) -&gt; list[Instance]:\n        sampled_indices = self._rnd.sample(range(len(self.dataset)), self.num_shots)\n        return [self.dataset[i] for i in sampled_indices]\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(dataset={self.dataset}, num_shots={self.num_shots})\"\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.rand.RandomFewShotGenerator.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset = dataset\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.rand.RandomFewShotGenerator.num_shots","title":"num_shots  <code>instance-attribute</code>","text":"<pre><code>num_shots = num_shots\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.rand.RandomFewShotGenerator.__init__","title":"__init__","text":"<pre><code>__init__(\n    dataset: Dataset,\n    num_shots: int,\n    seed: int = 42,\n    num_trials_to_avoid_leak: int = 3,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/few_shot_generator/rand.py</code> <pre><code>def __init__(\n    self,\n    dataset: Dataset,\n    num_shots: int,\n    seed: int = 42,\n    num_trials_to_avoid_leak: int = 3,\n) -&gt; None:\n    super().__init__(num_trials_to_avoid_leak=num_trials_to_avoid_leak)\n\n    if num_shots &gt; len(dataset):\n        msg = (\n            f\"`num_shots` should be less than or equal to the number of instances in `dataset`. \"\n            f\"num_shots: {num_shots}, len(dataset): {len(dataset)}\"\n        )\n        raise ValueError(msg)\n\n    self.dataset = dataset\n    self.num_shots = num_shots\n    self._rnd = random.Random(seed)\n</code></pre>"},{"location":"api_reference/FewShotGenerator/#flexeval.core.few_shot_generator.rand.RandomFewShotGenerator.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/few_shot_generator/rand.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(dataset={self.dataset}, num_shots={self.num_shots})\"\n</code></pre>"},{"location":"api_reference/FunctionToolCall/","title":"FunctionToolCall","text":""},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.FunctionToolCall","title":"FunctionToolCall  <code>dataclass</code>","text":"<p>Represents a function tool call with its name, arguments, and optional ID.</p> <p>Attributes:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the function to call.</p> </li> <li> <code>arguments</code>               (<code>str</code>)           \u2013            <p>Arguments to pass to the function. This is must be a JSON formatted string</p> </li> <li> <code>id</code>               (<code>int | None</code>)           \u2013            <p>An optional identifier for the tool call.</p> </li> </ul> Source code in <code>flexeval/core/tool_parser/base.py</code> <pre><code>@dataclass\nclass FunctionToolCall(ToolCall):\n    \"\"\"\n    Represents a function tool call with its name, arguments, and optional ID.\n\n    Attributes:\n        name: The name of the function to call.\n        arguments: Arguments to pass to the function.\n            This is must be a JSON formatted string\n        id: An optional identifier for the tool call.\n    \"\"\"\n\n    name: str\n    arguments: str\n    id: int | None = None\n\n    def __post_init__(self) -&gt; None:\n        if self.id is None:\n            self.id = generate_tool_call_id()\n        try:\n            json.loads(self.arguments)\n        except json.JSONDecodeError as exc:\n            msg = \"'arguments' must be a JSON formatted string\"\n            raise ValueError(msg) from exc\n\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Return a dictionary representation of the function tool call.\n\n        Returns:\n            A dictionary with the structure required for function tool calls.\n        \"\"\"\n        return {\"id\": self.id, \"type\": \"function\", \"function\": {\"name\": self.name, \"arguments\": self.arguments}}\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.FunctionToolCall.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.FunctionToolCall.arguments","title":"arguments  <code>instance-attribute</code>","text":"<pre><code>arguments: str\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.FunctionToolCall.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: int | None = None\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.FunctionToolCall.__init__","title":"__init__","text":"<pre><code>__init__(\n    name: str, arguments: str, id: int | None = None\n) -&gt; None\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.FunctionToolCall.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/tool_parser/base.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.id is None:\n        self.id = generate_tool_call_id()\n    try:\n        json.loads(self.arguments)\n    except json.JSONDecodeError as exc:\n        msg = \"'arguments' must be a JSON formatted string\"\n        raise ValueError(msg) from exc\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.FunctionToolCall.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return a dictionary representation of the function tool call.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>A dictionary with the structure required for function tool calls.</p> </li> </ul> Source code in <code>flexeval/core/tool_parser/base.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Return a dictionary representation of the function tool call.\n\n    Returns:\n        A dictionary with the structure required for function tool calls.\n    \"\"\"\n    return {\"id\": self.id, \"type\": \"function\", \"function\": {\"name\": self.name, \"arguments\": self.arguments}}\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.ToolCall","title":"ToolCall  <code>dataclass</code>","text":"<p>An abstract base class representing a generic tool call. Subclasses should implement the <code>to_dict</code> method to return a dictionary representation of the tool call.</p> Source code in <code>flexeval/core/tool_parser/base.py</code> <pre><code>@dataclass\nclass ToolCall(ABC):\n    \"\"\"\n    An abstract base class representing a generic tool call.\n    Subclasses should implement the `to_dict` method to return a dictionary representation of the tool call.\n    \"\"\"\n\n    @abstractmethod\n    def to_dict(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Return a dictionary representation of the tool call.\n\n        Returns:\n            A dictionary describing the tool call.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.ToolCall.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.ToolCall.to_dict","title":"to_dict  <code>abstractmethod</code>","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Return a dictionary representation of the tool call.</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>A dictionary describing the tool call.</p> </li> </ul> Source code in <code>flexeval/core/tool_parser/base.py</code> <pre><code>@abstractmethod\ndef to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Return a dictionary representation of the tool call.\n\n    Returns:\n        A dictionary describing the tool call.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.ToolCallingMessage","title":"ToolCallingMessage  <code>dataclass</code>","text":"<p>Represents the parsed result of a model output that may contain tool calls.</p> <p>Attributes:</p> <ul> <li> <code>validation_result</code>               (<code>Literal['CompleteToolCall', 'InCompleteToolCall', 'TextOnly']</code>)           \u2013            <p>The validation result of the parsing (e.g., 'CompleteToolCall', 'InCompleteToolCall', or 'TextOnly').</p> </li> <li> <code>text</code>               (<code>str</code>)           \u2013            <p>The text remaining after extracting the tool-calling part.</p> </li> <li> <code>raw_text</code>               (<code>str</code>)           \u2013            <p>The raw, unprocessed text.</p> </li> <li> <code>tool_calls</code>               (<code>list[ToolCall]</code>)           \u2013            <p>A list of ToolCall objects extracted from the text.</p> </li> </ul> Source code in <code>flexeval/core/tool_parser/base.py</code> <pre><code>@dataclass\nclass ToolCallingMessage:\n    \"\"\"\n    Represents the parsed result of a model output that may contain tool calls.\n\n    Attributes:\n        validation_result: The validation result of the parsing\n            (e.g., 'CompleteToolCall', 'InCompleteToolCall', or 'TextOnly').\n        text: The text remaining after extracting the tool-calling part.\n        raw_text: The raw, unprocessed text.\n        tool_calls: A list of ToolCall objects extracted from the text.\n    \"\"\"\n\n    validation_result: Literal[\n        \"CompleteToolCall\",\n        \"InCompleteToolCall\",\n        \"TextOnly\",\n    ]\n    text: str = None\n    raw_text: str = None\n    tool_calls: list[ToolCall] = field(default_factory=list)\n\n    def __post_init__(self) -&gt; None:\n        self.tool_call_dicts = [tool_call.to_dict() for tool_call in self.tool_calls]\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.ToolCallingMessage.validation_result","title":"validation_result  <code>instance-attribute</code>","text":"<pre><code>validation_result: Literal[\n    \"CompleteToolCall\", \"InCompleteToolCall\", \"TextOnly\"\n]\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.ToolCallingMessage.text","title":"text  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>text: str = None\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.ToolCallingMessage.raw_text","title":"raw_text  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_text: str = None\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.ToolCallingMessage.tool_calls","title":"tool_calls  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_calls: list[ToolCall] = field(default_factory=list)\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.ToolCallingMessage.__init__","title":"__init__","text":"<pre><code>__init__(\n    validation_result: Literal[\n        \"CompleteToolCall\", \"InCompleteToolCall\", \"TextOnly\"\n    ],\n    text: str = None,\n    raw_text: str = None,\n    tool_calls: list[ToolCall] = list(),\n) -&gt; None\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.ToolCallingMessage.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/tool_parser/base.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    self.tool_call_dicts = [tool_call.to_dict() for tool_call in self.tool_calls]\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.ToolParser","title":"ToolParser","text":"<p>An interface class used to extract tool calls from the model's output.</p> Source code in <code>flexeval/core/tool_parser/base.py</code> <pre><code>class ToolParser(ABC):\n    \"\"\"\n    An interface class used to extract tool calls from the model's output.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, text: str) -&gt; ToolCallingMessage:\n        \"\"\"\n        Extract tool_calls from the input text.\n\n        Args:\n            text: The text to process.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/FunctionToolCall/#flexeval.core.tool_parser.base.ToolParser.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(text: str) -&gt; ToolCallingMessage\n</code></pre> <p>Extract tool_calls from the input text.</p> <p>Parameters:</p> <ul> <li> <code>text</code>               (<code>str</code>)           \u2013            <p>The text to process.</p> </li> </ul> Source code in <code>flexeval/core/tool_parser/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, text: str) -&gt; ToolCallingMessage:\n    \"\"\"\n    Extract tool_calls from the input text.\n\n    Args:\n        text: The text to process.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/GenerationDataset/","title":"GenerationDataset","text":""},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationDataset","title":"GenerationDataset","text":"<p>A dataset holding <code>GenerationInstance</code>.</p> Source code in <code>flexeval/core/generation_dataset/base.py</code> <pre><code>class GenerationDataset(Sequence[GenerationInstance], ABC):\n    \"\"\"A dataset holding `GenerationInstance`.\"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the number of instances in the dataset.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __getitem__(self, i: int) -&gt; GenerationInstance:\n        \"\"\"\n        Returns the i-th instance.\n        \"\"\"\n        raise NotImplementedError\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationDataset.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of instances in the dataset.</p> Source code in <code>flexeval/core/generation_dataset/base.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"\n    Returns the number of instances in the dataset.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationDataset.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(i: int) -&gt; GenerationInstance\n</code></pre> <p>Returns the i-th instance.</p> Source code in <code>flexeval/core/generation_dataset/base.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, i: int) -&gt; GenerationInstance:\n    \"\"\"\n    Returns the i-th instance.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/generation_dataset/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationInstance","title":"GenerationInstance  <code>dataclass</code>","text":"<p>A dataclass representing a single input-output pair of a generation task.</p> Source code in <code>flexeval/core/generation_dataset/base.py</code> <pre><code>@dataclass\nclass GenerationInstance:\n    \"\"\"\n    A dataclass representing a single input-output pair of a generation task.\n    \"\"\"\n\n    inputs: dict[str, Any]\n    \"\"\"\n    Inputs of the generation task.\n    This will be embedded into the prompt for the language model in `PromptTemplate`.\n    \"\"\"\n    references: list[str] = field(default_factory=list)\n    \"\"\"\n    Reference outputs for the generation task.\n    The model's output will be evaluated against these references in `Metric`.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationInstance.inputs","title":"inputs  <code>instance-attribute</code>","text":"<pre><code>inputs: dict[str, Any]\n</code></pre> <p>Inputs of the generation task. This will be embedded into the prompt for the language model in <code>PromptTemplate</code>.</p>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationInstance.references","title":"references  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>references: list[str] = field(default_factory=list)\n</code></pre> <p>Reference outputs for the generation task. The model's output will be evaluated against these references in <code>Metric</code>.</p>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.base.GenerationInstance.__init__","title":"__init__","text":"<pre><code>__init__(\n    inputs: dict[str, Any], references: list[str] = list()\n) -&gt; None\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.HFGenerationDataset","title":"HFGenerationDataset","text":"<p>Load GenerationInstances from a huggingface dataset.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the Hugging Face dataset.</p> </li> <li> <code>split</code>               (<code>str</code>)           \u2013            <p>The split of the dataset.</p> </li> <li> <code>subset</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The subset of the dataset.</p> </li> <li> <code>dataset_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>The additional keyword arguments for loading the dataset.</p> </li> </ul> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>class HFGenerationDataset(TemplateGenerationDataset):\n    \"\"\"\n    Load GenerationInstances from a huggingface dataset.\n\n    Args:\n        path: The path to the Hugging Face dataset.\n        split: The split of the dataset.\n        subset: The subset of the dataset.\n        dataset_kwargs: The additional keyword arguments for loading the dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        split: str,\n        subset: str | None = None,\n        dataset_kwargs: dict[str, Any] | None = None,\n        reference_template: str | None = None,\n        reference_list_template: str | None = None,\n        input_templates: dict[str, str] | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        dataset_kwargs = dataset_kwargs or {}\n        dataset = datasets.load_dataset(path, name=subset, split=split, **dataset_kwargs)\n        items = [dict(item) for item in dataset]\n\n        super().__init__(\n            items=items,\n            reference_template=reference_template,\n            reference_list_template=reference_list_template,\n            input_templates=input_templates,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.HFGenerationDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    path: str,\n    split: str,\n    subset: str | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    input_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    split: str,\n    subset: str | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    input_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    dataset_kwargs = dataset_kwargs or {}\n    dataset = datasets.load_dataset(path, name=subset, split=split, **dataset_kwargs)\n    items = [dict(item) for item in dataset]\n\n    super().__init__(\n        items=items,\n        reference_template=reference_template,\n        reference_list_template=reference_list_template,\n        input_templates=input_templates,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.JsonlGenerationDataset","title":"JsonlGenerationDataset","text":"<p>Load GenerationInstances from a JSONL file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the JSONL file.</p> </li> </ul> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>class JsonlGenerationDataset(TemplateGenerationDataset):\n    \"\"\"\n    Load GenerationInstances from a JSONL file.\n\n    Args:\n        path: The path to the JSONL file.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        reference_template: str | None = None,\n        reference_list_template: str | None = None,\n        input_templates: dict[str, str] | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        with open(path) as f:\n            items = [json.loads(line) for line in f]\n\n        super().__init__(\n            items=items,\n            reference_template=reference_template,\n            reference_list_template=reference_list_template,\n            input_templates=input_templates,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.JsonlGenerationDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    path: str,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    input_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    input_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    with open(path) as f:\n        items = [json.loads(line) for line in f]\n\n    super().__init__(\n        items=items,\n        reference_template=reference_template,\n        reference_list_template=reference_list_template,\n        input_templates=input_templates,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset","title":"TemplateGenerationDataset","text":"<p>Load GenerationInstances from a JSONL file.</p> <p>Parameters:</p> <ul> <li> <code>items</code>               (<code>list[dict[str, Any]]</code>)           \u2013            <p>A list of dict items.</p> </li> <li> <code>reference_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Specify the Jinja2 template to render the reference string if the dataset has a single reference.</p> </li> <li> <code>reference_list_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Specify the Jinja2 template to render a list of reference strings if the dataset has multiple references.</p> </li> <li> <code>input_templates</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of Jinja2 templates for the inputs.</p> </li> <li> <code>data_range</code>               (<code>tuple[int, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The range of data to use.</p> </li> <li> <code>keep_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to filter certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.</p> </li> <li> <code>remove_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to remove certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.</p> </li> </ul> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>class TemplateGenerationDataset(GenerationDataset):\n    \"\"\"\n    Load GenerationInstances from a JSONL file.\n\n    Args:\n        items: A list of dict items.\n        reference_template: Specify the Jinja2 template to render the reference string\n            if the dataset has a single reference.\n        reference_list_template: Specify the Jinja2 template to render a list of reference strings\n            if the dataset has multiple references.\n        input_templates: A dictionary of Jinja2 templates for the inputs.\n        data_range: The range of data to use.\n        keep_conditions: A dictionary to indicate the condition to filter certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.\n        remove_conditions: A dictionary to indicate the condition to remove certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.\n    \"\"\"\n\n    def __init__(\n        self,\n        items: list[dict[str, Any]],\n        reference_template: str | None = None,\n        reference_list_template: str | None = None,\n        input_templates: dict[str, str] | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        if reference_template and reference_list_template:\n            msg = \"Only one of reference_template and reference_list_template can be set.\"\n            raise ValueError(msg)\n\n        if data_range:\n            start, end = data_range\n            items = items[start:end]\n\n        keep_conditions = keep_conditions or {}\n        for template_str, value_to_keep in keep_conditions.items():\n            key_template = JINJA2_ENV.from_string(template_str)\n            items = [item for item in items if key_template.render(**item) == value_to_keep]\n        remove_conditions = remove_conditions or {}\n        for template_str, value_to_remove in remove_conditions.items():\n            key_template = JINJA2_ENV.from_string(template_str)\n            items = [item for item in items if key_template.render(**item) != value_to_remove]\n\n        self.items = items\n        input_templates = input_templates or {}\n        self.input_templates: dict[str, Template] = {k: JINJA2_ENV.from_string(v) for k, v in input_templates.items()}\n        self.reference_template = JINJA2_ENV.from_string(reference_template) if reference_template else None\n        self.reference_list_template = (\n            JINJA2_ENV.from_string(reference_list_template) if reference_list_template else None\n        )\n\n    def __len__(self) -&gt; int:\n        return len(self.items)\n\n    def __getitem__(self, i: int) -&gt; GenerationInstance:\n        item = self.items[i]\n        inputs = dict(item.items())\n        inputs.update({k: v.render(**item) for k, v in self.input_templates.items()})\n\n        reference_list: list[str] = []\n        if self.reference_template:\n            reference_string = self.reference_template.render(**item)\n            reference_list.append(reference_string)\n        if self.reference_list_template:\n            reference_list_string = self.reference_list_template.render(**item)\n            if not (reference_list_string.startswith(\"[\") and reference_list_string.endswith(\"]\")):\n                msg = (\n                    f\"The reference_list_template should render a list of strings \"\n                    f\"but we got `{reference_list_string}`.\"\n                )\n                raise ValueError(msg)\n            reference_list.extend([str(ref) for ref in literal_eval(reference_list_string)])\n        return GenerationInstance(inputs=inputs, references=reference_list)\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.items","title":"items  <code>instance-attribute</code>","text":"<pre><code>items = items\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.input_templates","title":"input_templates  <code>instance-attribute</code>","text":"<pre><code>input_templates: dict[str, Template] = {\n    k: from_string(v) for (k, v) in items()\n}\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.reference_template","title":"reference_template  <code>instance-attribute</code>","text":"<pre><code>reference_template = (\n    from_string(reference_template)\n    if reference_template\n    else None\n)\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.reference_list_template","title":"reference_list_template  <code>instance-attribute</code>","text":"<pre><code>reference_list_template = (\n    from_string(reference_list_template)\n    if reference_list_template\n    else None\n)\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    items: list[dict[str, Any]],\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    input_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    items: list[dict[str, Any]],\n    reference_template: str | None = None,\n    reference_list_template: str | None = None,\n    input_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    if reference_template and reference_list_template:\n        msg = \"Only one of reference_template and reference_list_template can be set.\"\n        raise ValueError(msg)\n\n    if data_range:\n        start, end = data_range\n        items = items[start:end]\n\n    keep_conditions = keep_conditions or {}\n    for template_str, value_to_keep in keep_conditions.items():\n        key_template = JINJA2_ENV.from_string(template_str)\n        items = [item for item in items if key_template.render(**item) == value_to_keep]\n    remove_conditions = remove_conditions or {}\n    for template_str, value_to_remove in remove_conditions.items():\n        key_template = JINJA2_ENV.from_string(template_str)\n        items = [item for item in items if key_template.render(**item) != value_to_remove]\n\n    self.items = items\n    input_templates = input_templates or {}\n    self.input_templates: dict[str, Template] = {k: JINJA2_ENV.from_string(v) for k, v in input_templates.items()}\n    self.reference_template = JINJA2_ENV.from_string(reference_template) if reference_template else None\n    self.reference_list_template = (\n        JINJA2_ENV.from_string(reference_list_template) if reference_list_template else None\n    )\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.items)\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.template_based.TemplateGenerationDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; GenerationInstance\n</code></pre> Source code in <code>flexeval/core/generation_dataset/template_based.py</code> <pre><code>def __getitem__(self, i: int) -&gt; GenerationInstance:\n    item = self.items[i]\n    inputs = dict(item.items())\n    inputs.update({k: v.render(**item) for k, v in self.input_templates.items()})\n\n    reference_list: list[str] = []\n    if self.reference_template:\n        reference_string = self.reference_template.render(**item)\n        reference_list.append(reference_string)\n    if self.reference_list_template:\n        reference_list_string = self.reference_list_template.render(**item)\n        if not (reference_list_string.startswith(\"[\") and reference_list_string.endswith(\"]\")):\n            msg = (\n                f\"The reference_list_template should render a list of strings \"\n                f\"but we got `{reference_list_string}`.\"\n            )\n            raise ValueError(msg)\n        reference_list.extend([str(ref) for ref in literal_eval(reference_list_string)])\n    return GenerationInstance(inputs=inputs, references=reference_list)\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.sacrebleu_dataset.SacreBleuDataset","title":"SacreBleuDataset","text":"<p>Load datasets from the sacrebleu library. The available datasets are defined in sacrebleu.DATASETS.</p> Source code in <code>flexeval/core/generation_dataset/sacrebleu_dataset.py</code> <pre><code>class SacreBleuDataset(GenerationDataset):\n    \"\"\"Load datasets from the [sacrebleu](https://github.com/mjpost/sacrebleu) library.\n    The available datasets are defined in sacrebleu.DATASETS.\n    \"\"\"\n\n    def __init__(self, name: str, langpair: str) -&gt; None:\n        self._source_list: list[str] = list(sacrebleu.DATASETS[name].source(langpair))\n        self._references_list: list[list[str]] = [\n            [r.strip() for r in refs] for refs in sacrebleu.DATASETS[name].references(langpair)\n        ]\n\n        if len(self._source_list) != len(self._references_list):\n            msg = \"The number of source and reference pairs should be the same.\"\n            raise ValueError(msg)\n\n    def __len__(self) -&gt; int:\n        return len(self._source_list)\n\n    def __getitem__(self, i: int) -&gt; GenerationInstance:\n        return GenerationInstance(\n            inputs={\"source\": self._source_list[i]},\n            references=self._references_list[i],\n        )\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.sacrebleu_dataset.SacreBleuDataset.__init__","title":"__init__","text":"<pre><code>__init__(name: str, langpair: str) -&gt; None\n</code></pre> Source code in <code>flexeval/core/generation_dataset/sacrebleu_dataset.py</code> <pre><code>def __init__(self, name: str, langpair: str) -&gt; None:\n    self._source_list: list[str] = list(sacrebleu.DATASETS[name].source(langpair))\n    self._references_list: list[list[str]] = [\n        [r.strip() for r in refs] for refs in sacrebleu.DATASETS[name].references(langpair)\n    ]\n\n    if len(self._source_list) != len(self._references_list):\n        msg = \"The number of source and reference pairs should be the same.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.sacrebleu_dataset.SacreBleuDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/generation_dataset/sacrebleu_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._source_list)\n</code></pre>"},{"location":"api_reference/GenerationDataset/#flexeval.core.generation_dataset.sacrebleu_dataset.SacreBleuDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; GenerationInstance\n</code></pre> Source code in <code>flexeval/core/generation_dataset/sacrebleu_dataset.py</code> <pre><code>def __getitem__(self, i: int) -&gt; GenerationInstance:\n    return GenerationInstance(\n        inputs={\"source\": self._source_list[i]},\n        references=self._references_list[i],\n    )\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/","title":"HFMultipleChoiceDataset","text":""},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceDataset","title":"MultipleChoiceDataset","text":"Source code in <code>flexeval/core/multiple_choice_dataset/base.py</code> <pre><code>class MultipleChoiceDataset(Sequence[MultipleChoiceInstance], ABC):\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the number of instances in the dataset.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __getitem__(self, i: int) -&gt; MultipleChoiceInstance:\n        \"\"\"\n        Returns the i-th instance.\n        \"\"\"\n        raise NotImplementedError\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceDataset.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of instances in the dataset.</p> Source code in <code>flexeval/core/multiple_choice_dataset/base.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"\n    Returns the number of instances in the dataset.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceDataset.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(i: int) -&gt; MultipleChoiceInstance\n</code></pre> <p>Returns the i-th instance.</p> Source code in <code>flexeval/core/multiple_choice_dataset/base.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, i: int) -&gt; MultipleChoiceInstance:\n    \"\"\"\n    Returns the i-th instance.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/multiple_choice_dataset/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceInstance","title":"MultipleChoiceInstance  <code>dataclass</code>","text":"<p>A dataclass representing a single input-output pair of a multiple-choice task.</p> Source code in <code>flexeval/core/multiple_choice_dataset/base.py</code> <pre><code>@dataclass\nclass MultipleChoiceInstance:\n    \"\"\"\n    A dataclass representing a single input-output pair of a multiple-choice task.\n    \"\"\"\n\n    inputs: dict[str, str]\n    \"\"\"\n    Inputs of the multiple-choice task.\n    This will be embedded into the prompt for the language model in `PromptTemplate`.\n    \"\"\"\n    choices: list[str]\n    \"\"\"\n    Choices for the multiple-choice task.\n    `LanguageModel` will choose the answer based on the log-probabilities of these choices.\n    \"\"\"\n    answer_index: int\n    \"\"\"\n    Index of the correct answer in `choices`.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceInstance.inputs","title":"inputs  <code>instance-attribute</code>","text":"<pre><code>inputs: dict[str, str]\n</code></pre> <p>Inputs of the multiple-choice task. This will be embedded into the prompt for the language model in <code>PromptTemplate</code>.</p>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceInstance.choices","title":"choices  <code>instance-attribute</code>","text":"<pre><code>choices: list[str]\n</code></pre> <p>Choices for the multiple-choice task. <code>LanguageModel</code> will choose the answer based on the log-probabilities of these choices.</p>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceInstance.answer_index","title":"answer_index  <code>instance-attribute</code>","text":"<pre><code>answer_index: int\n</code></pre> <p>Index of the correct answer in <code>choices</code>.</p>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.base.MultipleChoiceInstance.__init__","title":"__init__","text":"<pre><code>__init__(\n    inputs: dict[str, str],\n    choices: list[str],\n    answer_index: int,\n) -&gt; None\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.HFMultipleChoiceDataset","title":"HFMultipleChoiceDataset","text":"<p>Load MultipleChoiceInstance from a huggingface dataset.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The name or path of the huggingface dataset.</p> </li> <li> <code>split</code>               (<code>str</code>)           \u2013            <p>The split of the dataset to use.</p> </li> <li> <code>subset</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The subset of the dataset to use.</p> </li> <li> <code>dataset_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>The keyword arguments for loading the dataset.</p> </li> </ul> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>class HFMultipleChoiceDataset(TemplateMultipleChoiceDataset):\n    \"\"\"\n    Load MultipleChoiceInstance from a huggingface dataset.\n\n    Args:\n        path: The name or path of the huggingface dataset.\n        split: The split of the dataset to use.\n        subset: The subset of the dataset to use.\n        dataset_kwargs: The keyword arguments for loading the dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        split: str,\n        choices_templates: list[str],\n        answer_index_template: str,\n        input_templates: dict[str, str] | None = None,\n        subset: str | None = None,\n        dataset_kwargs: dict[str, Any] | None = None,\n        whitespace_before_choices: bool = False,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        dataset_kwargs = dataset_kwargs or {}\n        items = datasets.load_dataset(path, split=split, name=subset, **dataset_kwargs)\n        items = [dict(item) for item in items]\n\n        super().__init__(\n            items=items,\n            choices_templates=choices_templates,\n            answer_index_template=answer_index_template,\n            input_templates=input_templates,\n            whitespace_before_choices=whitespace_before_choices,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.HFMultipleChoiceDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    path: str,\n    split: str,\n    choices_templates: list[str],\n    answer_index_template: str,\n    input_templates: dict[str, str] | None = None,\n    subset: str | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n    whitespace_before_choices: bool = False,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    split: str,\n    choices_templates: list[str],\n    answer_index_template: str,\n    input_templates: dict[str, str] | None = None,\n    subset: str | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n    whitespace_before_choices: bool = False,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    dataset_kwargs = dataset_kwargs or {}\n    items = datasets.load_dataset(path, split=split, name=subset, **dataset_kwargs)\n    items = [dict(item) for item in items]\n\n    super().__init__(\n        items=items,\n        choices_templates=choices_templates,\n        answer_index_template=answer_index_template,\n        input_templates=input_templates,\n        whitespace_before_choices=whitespace_before_choices,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.JsonlMultipleChoiceDataset","title":"JsonlMultipleChoiceDataset","text":"<p>Load MultipleChoiceInstance from a JSONL file.</p> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>class JsonlMultipleChoiceDataset(TemplateMultipleChoiceDataset):\n    \"\"\"\n    Load MultipleChoiceInstance from a JSONL file.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        choices_templates: list[str],\n        answer_index_template: str,\n        input_templates: dict[str, str] | None = None,\n        whitespace_before_choices: bool = False,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        with open(path) as f:\n            items = [json.loads(line) for line in f]\n\n        super().__init__(\n            items=items,\n            choices_templates=choices_templates,\n            answer_index_template=answer_index_template,\n            input_templates=input_templates,\n            whitespace_before_choices=whitespace_before_choices,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.JsonlMultipleChoiceDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    path: str,\n    choices_templates: list[str],\n    answer_index_template: str,\n    input_templates: dict[str, str] | None = None,\n    whitespace_before_choices: bool = False,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    choices_templates: list[str],\n    answer_index_template: str,\n    input_templates: dict[str, str] | None = None,\n    whitespace_before_choices: bool = False,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    with open(path) as f:\n        items = [json.loads(line) for line in f]\n\n    super().__init__(\n        items=items,\n        choices_templates=choices_templates,\n        answer_index_template=answer_index_template,\n        input_templates=input_templates,\n        whitespace_before_choices=whitespace_before_choices,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset","title":"TemplateMultipleChoiceDataset","text":"<p>An abstract dataset class for multiple-choice tasks. This class generates multiple-choice instances from a dict item and Jinja2 templates.</p> <p>Parameters:</p> <ul> <li> <code>items</code>               (<code>list[dict[str, Any]]</code>)           \u2013            <p>A list of dict items.</p> </li> <li> <code>choices_templates</code>               (<code>list[str]</code>)           \u2013            <p>A list of Jinja2 templates for the choices.</p> </li> <li> <code>answer_index_template</code>               (<code>str</code>)           \u2013            <p>A Jinja2 template for the index of the correct answer.</p> </li> <li> <code>input_templates</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of Jinja2 templates for the inputs.</p> </li> <li> <code>whitespace_before_choices</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add a whitespace before each choice. Maybe necessary for language with whitespaces.</p> </li> <li> <code>data_range</code>               (<code>tuple[int, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>The range of data to use.</p> </li> <li> <code>keep_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to filter certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.</p> </li> <li> <code>remove_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to remove certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.</p> </li> </ul> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>class TemplateMultipleChoiceDataset(MultipleChoiceDataset):\n    \"\"\"\n    An abstract dataset class for multiple-choice tasks.\n    This class generates multiple-choice instances from a dict item and Jinja2 templates.\n\n    Args:\n        items: A list of dict items.\n        choices_templates: A list of Jinja2 templates for the choices.\n        answer_index_template: A Jinja2 template for the index of the correct answer.\n        input_templates: A dictionary of Jinja2 templates for the inputs.\n        whitespace_before_choices: Whether to add a whitespace before each choice.\n            Maybe necessary for language with whitespaces.\n        data_range: The range of data to use.\n        keep_conditions: A dictionary to indicate the condition to filter certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.\n        remove_conditions: A dictionary to indicate the condition to remove certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.\n    \"\"\"\n\n    def __init__(\n        self,\n        items: list[dict[str, Any]],\n        choices_templates: list[str],\n        answer_index_template: str,\n        input_templates: dict[str, str] | None = None,\n        whitespace_before_choices: bool = False,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        if data_range:\n            start, end = data_range\n            items = items[start:end]\n\n        keep_conditions = keep_conditions or {}\n        for template_str, value_to_keep in keep_conditions.items():\n            key_template = JINJA2_ENV.from_string(template_str)\n            items = [item for item in items if key_template.render(**item) == value_to_keep]\n        remove_conditions = remove_conditions or {}\n        for template_str, value_to_remove in remove_conditions.items():\n            key_template = JINJA2_ENV.from_string(template_str)\n            items = [item for item in items if key_template.render(**item) != value_to_remove]\n        self.items = items\n\n        input_templates = input_templates or {}\n        self.input_templates: dict[str, Template] = {k: JINJA2_ENV.from_string(v) for k, v in input_templates.items()}\n        self.choices_templates = [JINJA2_ENV.from_string(t) for t in choices_templates]\n        self.answer_index_template = JINJA2_ENV.from_string(\n            answer_index_template,\n        )\n        self.whitespace_before_choices = whitespace_before_choices\n\n    def __len__(self) -&gt; int:\n        return len(self.items)\n\n    def __getitem__(self, i: int) -&gt; MultipleChoiceInstance:\n        item = self.items[i]\n        inputs = dict(item.items())\n        inputs.update({k: v.render(**item) for k, v in self.input_templates.items()})\n\n        choices = [t.render(**item) for t in self.choices_templates]\n        choices = list(filter(lambda x: len(x) &gt; 0, choices))\n        if self.whitespace_before_choices:\n            choices = [\" \" + c for c in choices]\n\n        answer_index = int(self.answer_index_template.render(**item))\n        if not (answer_index &gt;= 0 and answer_index &lt; len(choices)):\n            msg = f\"at least {answer_index+1} choices required, but got {choices}\"\n            raise ValueError(msg)\n\n        return MultipleChoiceInstance(\n            inputs=inputs,\n            choices=choices,\n            answer_index=answer_index,\n        )\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.items","title":"items  <code>instance-attribute</code>","text":"<pre><code>items = items\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.input_templates","title":"input_templates  <code>instance-attribute</code>","text":"<pre><code>input_templates: dict[str, Template] = {\n    k: from_string(v) for (k, v) in items()\n}\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.choices_templates","title":"choices_templates  <code>instance-attribute</code>","text":"<pre><code>choices_templates = [\n    from_string(t) for t in choices_templates\n]\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.answer_index_template","title":"answer_index_template  <code>instance-attribute</code>","text":"<pre><code>answer_index_template = from_string(answer_index_template)\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.whitespace_before_choices","title":"whitespace_before_choices  <code>instance-attribute</code>","text":"<pre><code>whitespace_before_choices = whitespace_before_choices\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    items: list[dict[str, Any]],\n    choices_templates: list[str],\n    answer_index_template: str,\n    input_templates: dict[str, str] | None = None,\n    whitespace_before_choices: bool = False,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    items: list[dict[str, Any]],\n    choices_templates: list[str],\n    answer_index_template: str,\n    input_templates: dict[str, str] | None = None,\n    whitespace_before_choices: bool = False,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    if data_range:\n        start, end = data_range\n        items = items[start:end]\n\n    keep_conditions = keep_conditions or {}\n    for template_str, value_to_keep in keep_conditions.items():\n        key_template = JINJA2_ENV.from_string(template_str)\n        items = [item for item in items if key_template.render(**item) == value_to_keep]\n    remove_conditions = remove_conditions or {}\n    for template_str, value_to_remove in remove_conditions.items():\n        key_template = JINJA2_ENV.from_string(template_str)\n        items = [item for item in items if key_template.render(**item) != value_to_remove]\n    self.items = items\n\n    input_templates = input_templates or {}\n    self.input_templates: dict[str, Template] = {k: JINJA2_ENV.from_string(v) for k, v in input_templates.items()}\n    self.choices_templates = [JINJA2_ENV.from_string(t) for t in choices_templates]\n    self.answer_index_template = JINJA2_ENV.from_string(\n        answer_index_template,\n    )\n    self.whitespace_before_choices = whitespace_before_choices\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.items)\n</code></pre>"},{"location":"api_reference/HFMultipleChoiceDataset/#flexeval.core.multiple_choice_dataset.template_based.TemplateMultipleChoiceDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; MultipleChoiceInstance\n</code></pre> Source code in <code>flexeval/core/multiple_choice_dataset/template_based.py</code> <pre><code>def __getitem__(self, i: int) -&gt; MultipleChoiceInstance:\n    item = self.items[i]\n    inputs = dict(item.items())\n    inputs.update({k: v.render(**item) for k, v in self.input_templates.items()})\n\n    choices = [t.render(**item) for t in self.choices_templates]\n    choices = list(filter(lambda x: len(x) &gt; 0, choices))\n    if self.whitespace_before_choices:\n        choices = [\" \" + c for c in choices]\n\n    answer_index = int(self.answer_index_template.render(**item))\n    if not (answer_index &gt;= 0 and answer_index &lt; len(choices)):\n        msg = f\"at least {answer_index+1} choices required, but got {choices}\"\n        raise ValueError(msg)\n\n    return MultipleChoiceInstance(\n        inputs=inputs,\n        choices=choices,\n        answer_index=answer_index,\n    )\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/","title":"HFRewardBenchDataset","text":""},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchDataset","title":"RewardBenchDataset","text":"Source code in <code>flexeval/core/reward_bench_dataset/base.py</code> <pre><code>class RewardBenchDataset(Sequence[RewardBenchInstance], ABC):\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of instances in the dataset.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __getitem__(self, i: int) -&gt; RewardBenchInstance:\n        \"\"\"Returns the i-th instance.\"\"\"\n        raise NotImplementedError\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchDataset.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Returns the number of instances in the dataset.</p> Source code in <code>flexeval/core/reward_bench_dataset/base.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Returns the number of instances in the dataset.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchDataset.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(i: int) -&gt; RewardBenchInstance\n</code></pre> <p>Returns the i-th instance.</p> Source code in <code>flexeval/core/reward_bench_dataset/base.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, i: int) -&gt; RewardBenchInstance:\n    \"\"\"Returns the i-th instance.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/reward_bench_dataset/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchInstance","title":"RewardBenchInstance  <code>dataclass</code>","text":"<p>A dataclass representing a triplet (prompt, chosen, rejected) of a reward bench task.</p> Source code in <code>flexeval/core/reward_bench_dataset/base.py</code> <pre><code>@dataclass\nclass RewardBenchInstance:\n    \"\"\"A dataclass representing a triplet (prompt, chosen, rejected) of a\n    reward bench task.\"\"\"\n\n    prompt: list[dict[str, str]]\n    \"\"\"\n    The prompt for chosen/rejected responses.\n    The format is a list of dictionaries, where each dictionary represents an OpenAI-format chat message,\n    such as `{\"role\": \"user\", \"content\": \"Hello!\"}`.\n    \"\"\"\n    chosen: list[dict[str, str]]\n    \"\"\"\n    The chosen response to the prompt.\n    The format is the same as `prompt`.\n    \"\"\"\n    rejected: list[dict[str, str]]\n    \"\"\"\n    The rejected response to the prompt.\n    The format is the same as `prompt`.\n    \"\"\"\n    category_key: str | None = None\n    \"\"\"\n    A key to compute category-wise average accuracies.\n    \"\"\"\n    extra_info: dict[str, Any] = field(default_factory=dict)\n    \"\"\"\n    Extra information that can be used by passing to `Metric`.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchInstance.prompt","title":"prompt  <code>instance-attribute</code>","text":"<pre><code>prompt: list[dict[str, str]]\n</code></pre> <p>The prompt for chosen/rejected responses. The format is a list of dictionaries, where each dictionary represents an OpenAI-format chat message, such as <code>{\"role\": \"user\", \"content\": \"Hello!\"}</code>.</p>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchInstance.chosen","title":"chosen  <code>instance-attribute</code>","text":"<pre><code>chosen: list[dict[str, str]]\n</code></pre> <p>The chosen response to the prompt. The format is the same as <code>prompt</code>.</p>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchInstance.rejected","title":"rejected  <code>instance-attribute</code>","text":"<pre><code>rejected: list[dict[str, str]]\n</code></pre> <p>The rejected response to the prompt. The format is the same as <code>prompt</code>.</p>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchInstance.category_key","title":"category_key  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>category_key: str | None = None\n</code></pre> <p>A key to compute category-wise average accuracies.</p>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchInstance.extra_info","title":"extra_info  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>extra_info: dict[str, Any] = field(default_factory=dict)\n</code></pre> <p>Extra information that can be used by passing to <code>Metric</code>.</p>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.base.RewardBenchInstance.__init__","title":"__init__","text":"<pre><code>__init__(\n    prompt: list[dict[str, str]],\n    chosen: list[dict[str, str]],\n    rejected: list[dict[str, str]],\n    category_key: str | None = None,\n    extra_info: dict[str, Any] = dict(),\n) -&gt; None\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.template_based.HFRewardBenchDataset","title":"HFRewardBenchDataset","text":"<p>Load RewardBenchInstances from a Hugging Face dataset.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the Hugging Face dataset.</p> </li> <li> <code>split</code>               (<code>str</code>)           \u2013            <p>The split of the dataset.</p> </li> <li> <code>subset</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The subset of the dataset.</p> </li> <li> <code>dataset_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>The keyword arguments to pass to the Hugging Face dataset.</p> </li> </ul> Source code in <code>flexeval/core/reward_bench_dataset/template_based.py</code> <pre><code>class HFRewardBenchDataset(TemplateRewardBenchDataset):\n    \"\"\"\n    Load RewardBenchInstances from a Hugging Face dataset.\n\n    Args:\n        path: The path to the Hugging Face dataset.\n        split: The split of the dataset.\n        subset: The subset of the dataset.\n        dataset_kwargs: The keyword arguments to pass to the Hugging Face dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        split: str,\n        subset: str | None = None,\n        dataset_kwargs: dict[str, Any] | None = None,\n        prompt_template: str = \"{{ prompt }}\",\n        chosen_template: str = \"{{ chosen }}\",\n        rejected_template: str = \"{{ rejected }}\",\n        category_template: str | None = None,\n        extra_info_templates: dict[str, str] | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        dataset_kwargs = dataset_kwargs or {}\n        dataset = datasets.load_dataset(path, name=subset, split=split, **dataset_kwargs)\n        items = [dict(item) for item in dataset]\n\n        super().__init__(\n            items=items,\n            prompt_template=prompt_template,\n            chosen_template=chosen_template,\n            rejected_template=rejected_template,\n            category_template=category_template,\n            extra_info_templates=extra_info_templates,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.template_based.HFRewardBenchDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    path: str,\n    split: str,\n    subset: str | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n    prompt_template: str = \"{{ prompt }}\",\n    chosen_template: str = \"{{ chosen }}\",\n    rejected_template: str = \"{{ rejected }}\",\n    category_template: str | None = None,\n    extra_info_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/reward_bench_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    split: str,\n    subset: str | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n    prompt_template: str = \"{{ prompt }}\",\n    chosen_template: str = \"{{ chosen }}\",\n    rejected_template: str = \"{{ rejected }}\",\n    category_template: str | None = None,\n    extra_info_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    dataset_kwargs = dataset_kwargs or {}\n    dataset = datasets.load_dataset(path, name=subset, split=split, **dataset_kwargs)\n    items = [dict(item) for item in dataset]\n\n    super().__init__(\n        items=items,\n        prompt_template=prompt_template,\n        chosen_template=chosen_template,\n        rejected_template=rejected_template,\n        category_template=category_template,\n        extra_info_templates=extra_info_templates,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.template_based.JsonlRewardBenchDataset","title":"JsonlRewardBenchDataset","text":"<p>Load RewardBenchInstances from a JSONL file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The path to the JSONL file.</p> </li> </ul> Source code in <code>flexeval/core/reward_bench_dataset/template_based.py</code> <pre><code>class JsonlRewardBenchDataset(TemplateRewardBenchDataset):\n    \"\"\"\n    Load RewardBenchInstances from a JSONL file.\n\n    Args:\n        path: The path to the JSONL file.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        prompt_template: str = \"{{ prompt }}\",\n        chosen_template: str = \"{{ chosen }}\",\n        rejected_template: str = \"{{ rejected }}\",\n        category_template: str | None = None,\n        extra_info_templates: dict[str, str] | None = None,\n        data_range: tuple[int, int] | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n    ) -&gt; None:\n        with open(path) as f:\n            items = [json.loads(line) for line in f]\n\n        super().__init__(\n            items=items,\n            prompt_template=prompt_template,\n            chosen_template=chosen_template,\n            rejected_template=rejected_template,\n            category_template=category_template,\n            extra_info_templates=extra_info_templates,\n            data_range=data_range,\n            keep_conditions=keep_conditions,\n            remove_conditions=remove_conditions,\n        )\n</code></pre>"},{"location":"api_reference/HFRewardBenchDataset/#flexeval.core.reward_bench_dataset.template_based.JsonlRewardBenchDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    path: str,\n    prompt_template: str = \"{{ prompt }}\",\n    chosen_template: str = \"{{ chosen }}\",\n    rejected_template: str = \"{{ rejected }}\",\n    category_template: str | None = None,\n    extra_info_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/reward_bench_dataset/template_based.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    prompt_template: str = \"{{ prompt }}\",\n    chosen_template: str = \"{{ chosen }}\",\n    rejected_template: str = \"{{ rejected }}\",\n    category_template: str | None = None,\n    extra_info_templates: dict[str, str] | None = None,\n    data_range: tuple[int, int] | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n) -&gt; None:\n    with open(path) as f:\n        items = [json.loads(line) for line in f]\n\n    super().__init__(\n        items=items,\n        prompt_template=prompt_template,\n        chosen_template=chosen_template,\n        rejected_template=rejected_template,\n        category_template=category_template,\n        extra_info_templates=extra_info_templates,\n        data_range=data_range,\n        keep_conditions=keep_conditions,\n        remove_conditions=remove_conditions,\n    )\n</code></pre>"},{"location":"api_reference/LMOutput/","title":"LMOutput","text":""},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LMOutput","title":"LMOutput  <code>dataclass</code>","text":"Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>@dataclass\nclass LMOutput:\n    text: str | None\n    \"\"\"\n    The output text of the language model.\n    None is allowed only if tool_calls is set.\n    \"\"\"\n    raw_text: str | None = None\n    \"\"\"\n    The raw output text of the language model before post-processing.\n    \"\"\"\n    finish_reason: str | None = None\n    \"\"\"\n    The reason why the generation is finished.\n    Typically,\n    - 'stop': A stop sequence is generated.\n    - 'length': The maximum length is reached.\n    \"\"\"\n    tool_calls: list[dict[str, Any]] | None = None\n    \"\"\"\n    the tools called by the language model\n    \"\"\"\n    tool_call_validation_result: str | None = None\n    \"\"\"\n    validation results of parsing for tool_calls\n    \"\"\"\n\n    def __post_init__(self) -&gt; None:\n        if self.text is None:\n            self.text = \"\"\n            if self.tool_calls is None:\n                msg = \"Both `text` and `tool_calls` are empty.\"\n                logger.warning(msg)\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LMOutput.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str | None\n</code></pre> <p>The output text of the language model. None is allowed only if tool_calls is set.</p>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LMOutput.raw_text","title":"raw_text  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_text: str | None = None\n</code></pre> <p>The raw output text of the language model before post-processing.</p>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LMOutput.finish_reason","title":"finish_reason  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>finish_reason: str | None = None\n</code></pre> <p>The reason why the generation is finished. Typically, - 'stop': A stop sequence is generated. - 'length': The maximum length is reached.</p>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LMOutput.tool_calls","title":"tool_calls  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_calls: list[dict[str, Any]] | None = None\n</code></pre> <p>the tools called by the language model</p>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LMOutput.tool_call_validation_result","title":"tool_call_validation_result  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_call_validation_result: str | None = None\n</code></pre> <p>validation results of parsing for tool_calls</p>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LMOutput.__init__","title":"__init__","text":"<pre><code>__init__(\n    text: str | None,\n    raw_text: str | None = None,\n    finish_reason: str | None = None,\n    tool_calls: list[dict[str, Any]] | None = None,\n    tool_call_validation_result: str | None = None,\n) -&gt; None\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LMOutput.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.text is None:\n        self.text = \"\"\n        if self.tool_calls is None:\n            msg = \"Both `text` and `tool_calls` are empty.\"\n            logger.warning(msg)\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LanguageModel","title":"LanguageModel","text":"<p>LanguageModel is what you want to evaluate with this library.</p> <p>It can generate text based on the input text, response to chat messages, and compute log probabilities.</p> <p>Parameters:</p> <ul> <li> <code>string_processors</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>A single or a list of StringProcessor objects to process the model's output.</p> </li> </ul> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>class LanguageModel:\n    \"\"\"LanguageModel is what you want to evaluate with this library.\n\n    It can generate text based on the input text, response to chat messages, and compute log probabilities.\n\n    Args:\n        string_processors: A single or a list of StringProcessor objects to process the model's output.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        string_processors: StringProcessor | list[StringProcessor] | None = None,\n    ) -&gt; None:\n        if string_processors is None:\n            string_processors = []\n        elif isinstance(string_processors, StringProcessor):\n            string_processors = [string_processors]\n\n        self.string_processors = string_processors\n\n    def _batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        \"\"\"\n        Generate text based on the input text list.\n\n        Args:\n            text_list: A list of input texts.\n            stop_sequences: A string or a list of strings that will stop the generation when they are generated.\n                This argument exists to give a common interface to various models that have different names for it.\n            max_new_tokens: The maximum number of tokens to generate for each text.\n                This argument exists to give a common interface to various models that have different names for it.\n            **kwargs: Additional keyword arguments for text generation.\n                The acceptable keys depend on the specific implementation of the model.\n                These arguments override corresponding values in the model's default_gen_kwargs.\n                Special cases:\n                - 'stop_sequences' or any similar model-specific kwargs:\n                    Merged with default_gen_kwargs instead of overriding.\n        \"\"\"\n        msg = f\"{self.__class__.__name__} cannot generate text.\"\n        raise NotImplementedError(msg)\n\n    def _batch_generate_chat_response(\n        self,\n        chat_messages_list: list[list[dict[str, Any]]],\n        tools_list: list[list[dict[str, Any]] | None] | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        \"\"\"Generate chat responses based on the chat messages in the list.\n        This method is used for chatbot models.\n\n        Args:\n            chat_messages_list: A list of chat messages.\n            tools_list: A list of tool definitions.\n                Each function definition should be a dict that conforms to the OpenAI Chat Completion API format.\n                https://platform.openai.com/docs/guides/function-calling?api-mode=chat#defining-functions\n        \"\"\"\n        msg = f\"{self.__class__.__name__} cannot generate chat responses.\"\n        raise NotImplementedError(msg)\n\n    def _batch_compute_log_probs(\n        self,\n        text_list: list[str],\n        prefix_list: list[str] | None = None,\n        stride: int | None = None,\n    ) -&gt; list[float]:\n        \"\"\"\n        Compute log probabilities of the text list.\n        Used for compute perplexity of text, or solving multiple choice questions.\n\n        Args:\n            text_list: A list of texts to compute log probabilities.\n            prefix_list: A list of prefixes for each text.\n            stride: The stride for computing log probabilities.\n        \"\"\"\n        msg = f\"{self.__class__.__name__} cannot compute perplexity.\"\n        raise NotImplementedError(msg)\n\n    def _batch_compute_chat_log_probs(\n        self, prompt_list: list[list[dict[str, Any]]], response_list: list[dict[str, Any]]\n    ) -&gt; list[float]:\n        \"\"\"\n        Compute log probabilities of the chat responses given the chat history.\n\n        Args:\n            prompt_list: A list of chat histories.\n            response_list: A list of chat responses.\n        \"\"\"\n        msg = f\"{self.__class__.__name__} cannot compute chat log probabilities.\"\n        raise NotImplementedError(msg)\n\n    @final\n    def complete_text(\n        self,\n        text: str | list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; LMOutput | list[LMOutput]:\n        \"\"\"\n        A wrapper for `batch_complete_text` that accepts a single text or a list of texts.\n        This is a convenient method for end-users.\n        To implement generation logic, you should override `batch_complete_text` method.\n        \"\"\"\n\n        # Normalize the input text\n        text_list = text\n        if isinstance(text, str):\n            text_list = [text]\n\n        lm_outputs = self._batch_complete_text(\n            text_list, stop_sequences=stop_sequences, max_new_tokens=max_new_tokens, **kwargs\n        )\n\n        # Post-process the generated text\n        if self.string_processors:\n            for lm_output in lm_outputs:\n                lm_output.raw_text = lm_output.text\n                for string_processor in self.string_processors:\n                    lm_output.text = string_processor(lm_output.text)\n\n        # Return the result\n        if isinstance(text, str):\n            return lm_outputs[0]\n        return lm_outputs\n\n    @final\n    def generate_chat_response(\n        self,\n        chat_messages: list[dict[str, Any]] | list[list[dict[str, Any]]],\n        tools: list[dict[str, Any]] | list[list[dict[str, Any]]] | None = None,\n        **kwargs,\n    ) -&gt; LMOutput | list[LMOutput]:\n        \"\"\"\n        A wrapper for `batch_generate_chat_response` that accepts a single chat message or a list of chat messages.\n        This is a convenient method for end-users.\n        To implement generation logic, you should override `batch_generate_chat_response` method.\n        \"\"\"\n\n        chat_messages_list = chat_messages\n        tools_list = tools\n\n        if isinstance(chat_messages[0], dict):\n            chat_messages_list = [chat_messages]\n        if tools and isinstance(tools[0], dict):\n            tools_list = [tools]\n\n        if tools_list and len(tools_list) != len(chat_messages_list):\n            msg = \"tools_list must be either None or a list of the same length as chat_messages_list.\"\n            raise ValueError(msg)\n\n        lm_outputs = self._batch_generate_chat_response(chat_messages_list, tools_list=tools_list, **kwargs)\n\n        # Post-process the generatessd text\n        if self.string_processors:\n            for lm_output in lm_outputs:\n                lm_output.raw_text = lm_output.text\n                for string_processor in self.string_processors:\n                    lm_output.text = string_processor(lm_output.text)\n\n        # Return the result\n        if isinstance(chat_messages[0], dict):\n            return lm_outputs[0]\n        return lm_outputs\n\n    @final\n    def compute_log_probs(\n        self,\n        text_list: str | list[str],\n        prefix_list: list[str] | None = None,\n        stride: int | None = None,\n    ) -&gt; float | list[float]:\n        \"\"\"\n        A wrapper for `batch_compute_log_probs` that accepts a single text or a list of texts.\n        This is a convenient method for end-users.\n        To implement computation logic, you should override `batch_compute_log_probs` method.\n        \"\"\"\n\n        if isinstance(text_list, str):\n            return self._batch_compute_log_probs([text_list], prefix_list=prefix_list, stride=stride)[0]\n        return self._batch_compute_log_probs(text_list, prefix_list=prefix_list, stride=stride)\n\n    @final\n    def compute_chat_log_probs(\n        self, prompt: list[dict[str, Any]] | list[list[dict[str, Any]]], response: dict[str, Any] | list[dict[str, Any]]\n    ) -&gt; float | list[float]:\n        \"\"\"\n        A wrapper for `batch_compute_chat_log_probs` that accepts a single chat prompt or a list of chat prompts.\n        This is a convenient method for end-users.\n        To implement computation logic, you should override `batch_compute_chat_log_probs` method.\n        \"\"\"\n\n        if isinstance(prompt[0], dict):\n            return self._batch_compute_chat_log_probs([prompt], [response])[0]\n        return self._batch_compute_chat_log_probs(prompt, response)\n\n    def cleanup_resources(self) -&gt; None:\n        \"\"\"\n        Clean up resources if necessary.\n        This method is called when the language model is no longer needed.\n        \"\"\"\n\n    def __del__(self) -&gt; None:\n        self.cleanup_resources()\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LanguageModel.string_processors","title":"string_processors  <code>instance-attribute</code>","text":"<pre><code>string_processors = string_processors\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LanguageModel.__init__","title":"__init__","text":"<pre><code>__init__(\n    string_processors: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>def __init__(\n    self,\n    string_processors: StringProcessor | list[StringProcessor] | None = None,\n) -&gt; None:\n    if string_processors is None:\n        string_processors = []\n    elif isinstance(string_processors, StringProcessor):\n        string_processors = [string_processors]\n\n    self.string_processors = string_processors\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LanguageModel.complete_text","title":"complete_text","text":"<pre><code>complete_text(\n    text: str | list[str],\n    stop_sequences: str | list[str] | None = None,\n    max_new_tokens: int | None = None,\n    **kwargs,\n) -&gt; LMOutput | list[LMOutput]\n</code></pre> <p>A wrapper for <code>batch_complete_text</code> that accepts a single text or a list of texts. This is a convenient method for end-users. To implement generation logic, you should override <code>batch_complete_text</code> method.</p> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>@final\ndef complete_text(\n    self,\n    text: str | list[str],\n    stop_sequences: str | list[str] | None = None,\n    max_new_tokens: int | None = None,\n    **kwargs,\n) -&gt; LMOutput | list[LMOutput]:\n    \"\"\"\n    A wrapper for `batch_complete_text` that accepts a single text or a list of texts.\n    This is a convenient method for end-users.\n    To implement generation logic, you should override `batch_complete_text` method.\n    \"\"\"\n\n    # Normalize the input text\n    text_list = text\n    if isinstance(text, str):\n        text_list = [text]\n\n    lm_outputs = self._batch_complete_text(\n        text_list, stop_sequences=stop_sequences, max_new_tokens=max_new_tokens, **kwargs\n    )\n\n    # Post-process the generated text\n    if self.string_processors:\n        for lm_output in lm_outputs:\n            lm_output.raw_text = lm_output.text\n            for string_processor in self.string_processors:\n                lm_output.text = string_processor(lm_output.text)\n\n    # Return the result\n    if isinstance(text, str):\n        return lm_outputs[0]\n    return lm_outputs\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LanguageModel.generate_chat_response","title":"generate_chat_response","text":"<pre><code>generate_chat_response(\n    chat_messages: list[dict[str, Any]]\n    | list[list[dict[str, Any]]],\n    tools: list[dict[str, Any]]\n    | list[list[dict[str, Any]]]\n    | None = None,\n    **kwargs,\n) -&gt; LMOutput | list[LMOutput]\n</code></pre> <p>A wrapper for <code>batch_generate_chat_response</code> that accepts a single chat message or a list of chat messages. This is a convenient method for end-users. To implement generation logic, you should override <code>batch_generate_chat_response</code> method.</p> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>@final\ndef generate_chat_response(\n    self,\n    chat_messages: list[dict[str, Any]] | list[list[dict[str, Any]]],\n    tools: list[dict[str, Any]] | list[list[dict[str, Any]]] | None = None,\n    **kwargs,\n) -&gt; LMOutput | list[LMOutput]:\n    \"\"\"\n    A wrapper for `batch_generate_chat_response` that accepts a single chat message or a list of chat messages.\n    This is a convenient method for end-users.\n    To implement generation logic, you should override `batch_generate_chat_response` method.\n    \"\"\"\n\n    chat_messages_list = chat_messages\n    tools_list = tools\n\n    if isinstance(chat_messages[0], dict):\n        chat_messages_list = [chat_messages]\n    if tools and isinstance(tools[0], dict):\n        tools_list = [tools]\n\n    if tools_list and len(tools_list) != len(chat_messages_list):\n        msg = \"tools_list must be either None or a list of the same length as chat_messages_list.\"\n        raise ValueError(msg)\n\n    lm_outputs = self._batch_generate_chat_response(chat_messages_list, tools_list=tools_list, **kwargs)\n\n    # Post-process the generatessd text\n    if self.string_processors:\n        for lm_output in lm_outputs:\n            lm_output.raw_text = lm_output.text\n            for string_processor in self.string_processors:\n                lm_output.text = string_processor(lm_output.text)\n\n    # Return the result\n    if isinstance(chat_messages[0], dict):\n        return lm_outputs[0]\n    return lm_outputs\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LanguageModel.compute_log_probs","title":"compute_log_probs","text":"<pre><code>compute_log_probs(\n    text_list: str | list[str],\n    prefix_list: list[str] | None = None,\n    stride: int | None = None,\n) -&gt; float | list[float]\n</code></pre> <p>A wrapper for <code>batch_compute_log_probs</code> that accepts a single text or a list of texts. This is a convenient method for end-users. To implement computation logic, you should override <code>batch_compute_log_probs</code> method.</p> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>@final\ndef compute_log_probs(\n    self,\n    text_list: str | list[str],\n    prefix_list: list[str] | None = None,\n    stride: int | None = None,\n) -&gt; float | list[float]:\n    \"\"\"\n    A wrapper for `batch_compute_log_probs` that accepts a single text or a list of texts.\n    This is a convenient method for end-users.\n    To implement computation logic, you should override `batch_compute_log_probs` method.\n    \"\"\"\n\n    if isinstance(text_list, str):\n        return self._batch_compute_log_probs([text_list], prefix_list=prefix_list, stride=stride)[0]\n    return self._batch_compute_log_probs(text_list, prefix_list=prefix_list, stride=stride)\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LanguageModel.compute_chat_log_probs","title":"compute_chat_log_probs","text":"<pre><code>compute_chat_log_probs(\n    prompt: list[dict[str, Any]]\n    | list[list[dict[str, Any]]],\n    response: dict[str, Any] | list[dict[str, Any]],\n) -&gt; float | list[float]\n</code></pre> <p>A wrapper for <code>batch_compute_chat_log_probs</code> that accepts a single chat prompt or a list of chat prompts. This is a convenient method for end-users. To implement computation logic, you should override <code>batch_compute_chat_log_probs</code> method.</p> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>@final\ndef compute_chat_log_probs(\n    self, prompt: list[dict[str, Any]] | list[list[dict[str, Any]]], response: dict[str, Any] | list[dict[str, Any]]\n) -&gt; float | list[float]:\n    \"\"\"\n    A wrapper for `batch_compute_chat_log_probs` that accepts a single chat prompt or a list of chat prompts.\n    This is a convenient method for end-users.\n    To implement computation logic, you should override `batch_compute_chat_log_probs` method.\n    \"\"\"\n\n    if isinstance(prompt[0], dict):\n        return self._batch_compute_chat_log_probs([prompt], [response])[0]\n    return self._batch_compute_chat_log_probs(prompt, response)\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LanguageModel.cleanup_resources","title":"cleanup_resources","text":"<pre><code>cleanup_resources() -&gt; None\n</code></pre> <p>Clean up resources if necessary. This method is called when the language model is no longer needed.</p> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>def cleanup_resources(self) -&gt; None:\n    \"\"\"\n    Clean up resources if necessary.\n    This method is called when the language model is no longer needed.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.base.LanguageModel.__del__","title":"__del__","text":"<pre><code>__del__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/base.py</code> <pre><code>def __del__(self) -&gt; None:\n    self.cleanup_resources()\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM","title":"HuggingFaceLM","text":"<p>LanguageModel implementation using Hugging Face Transformers.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The model name or path of the Hugging Face model.</p> </li> <li> <code>model_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments for the model instantiation by <code>from_pretrained()</code>.</p> </li> <li> <code>tokenizer</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The tokenizer name or path of the Hugging Face tokenizer.</p> </li> <li> <code>tokenizer_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments for the tokenizer instantiation by `from_pretrained().</p> </li> <li> <code>add_special_tokens</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add special tokens to the input. Note that whether BOS or EOS tokens are added depends on the tokenizer.</p> </li> <li> <code>amp_dtype</code>               (<code>Literal['float16', 'bfloat16'] | None</code>, default:                   <code>None</code> )           \u2013            <p>The dtype for automatic mixed precision.</p> </li> <li> <code>random_seed</code>               (<code>int</code>, default:                   <code>42</code> )           \u2013            <p>Random seed for the model.</p> </li> <li> <code>load_peft</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Should be set to True when loading the model from PEFT weights.</p> </li> <li> <code>custom_chat_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A custom chat template for chatbot models. If specified, this overrides the default chat template of the tokenizer.</p> </li> <li> <code>system_message</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>System messages to be prepended to given messages. It applies only for chat response.</p> </li> <li> <code>default_gen_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default generation kwargs to use when calling the API.</p> </li> <li> <code>string_processors</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>A single or a list of StringProcessor objects to process the model's output.</p> </li> <li> <code>model_limit_tokens</code>               (<code>int | None | Literal['default']</code>, default:                   <code>'default'</code> )           \u2013            <p>An upper limit on the number of tokens (input + output) the model can handle. If <code>max_new_tokens</code> exceeds this limit in <code>generate_chat_response()</code>, it will be capped to this value. If this value is set to less than or equal to the model's capacity and the input exceeds it, an empty string is returned instead of raising an error. If set to \u201cdefault\u201d, the value will be automatically determined when possible.</p> </li> <li> <code>tool_parser</code>               (<code>ToolParser | None</code>, default:                   <code>None</code> )           \u2013            <p>A ToolParser object to extract the tool_calls from the model's output.</p> </li> <li> <code>tools</code>               (<code>list[dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default tools to use in chat responses when no tools are explicitly provided.</p> </li> </ul> Source code in <code>flexeval/core/language_model/hf_lm.py</code> <pre><code>class HuggingFaceLM(LanguageModel):\n    \"\"\"\n    LanguageModel implementation using Hugging Face Transformers.\n\n    Args:\n        model: The model name or path of the Hugging Face model.\n        model_kwargs: Keyword arguments for the model instantiation by `from_pretrained()`.\n        tokenizer: The tokenizer name or path of the Hugging Face tokenizer.\n        tokenizer_kwargs: Keyword arguments for the tokenizer instantiation by `from_pretrained().\n        add_special_tokens: Whether to add special tokens to the input.\n            Note that whether BOS or EOS tokens are added depends on the tokenizer.\n        amp_dtype: The dtype for automatic mixed precision.\n        random_seed: Random seed for the model.\n        load_peft: Should be set to True when loading the model from PEFT weights.\n        custom_chat_template: A custom chat template for chatbot models.\n            If specified, this overrides the default chat template of the tokenizer.\n        system_message: System messages to be prepended to given messages. It applies only for\n            chat response.\n        default_gen_kwargs: Default generation kwargs to use when calling the API.\n        string_processors: A single or a list of StringProcessor objects to process the model's output.\n        model_limit_tokens: An upper limit on the number of tokens (input + output) the model can handle.\n            If `max_new_tokens` exceeds this limit in `generate_chat_response()`, it will be capped to this value.\n            If this value is set to less than or equal to the model's capacity and the input exceeds it,\n            an empty string is returned instead of raising an error.\n            If set to \u201cdefault\u201d, the value will be automatically determined when possible.\n        tool_parser: A ToolParser object to extract the tool_calls from the model's output.\n        tools: Default tools to use in chat responses when no tools are explicitly provided.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        model_kwargs: dict[str, Any] | None = None,\n        tokenizer: str | None = None,\n        tokenizer_kwargs: dict[str, Any] | None = None,\n        add_special_tokens: bool = False,\n        amp_dtype: Literal[\"float16\", \"bfloat16\"] | None = None,\n        random_seed: int = 42,\n        load_peft: bool = False,\n        custom_chat_template: str | None = None,\n        chat_template_kwargs: dict[str, Any] | None = None,\n        system_message: str | None = None,\n        default_gen_kwargs: dict[str, Any] | None = None,\n        string_processors: StringProcessor | list[StringProcessor] | None = None,\n        model_limit_tokens: int | None | Literal[\"default\"] = \"default\",\n        tool_parser: ToolParser | None = None,\n        tools: list[dict[str, Any]] | None = None,\n    ) -&gt; None:\n        super().__init__(string_processors=string_processors)\n        self._model_name_or_path = model\n        tokenizer = tokenizer if tokenizer else model\n        tokenizer_kwargs = tokenizer_kwargs or {}\n        self.tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(tokenizer, **tokenizer_kwargs)\n        self.custom_chat_template = custom_chat_template\n        self.system_message = system_message\n        self.chat_template_kwargs = chat_template_kwargs or {}\n        self.add_special_tokens = add_special_tokens\n        self.default_gen_kwargs = default_gen_kwargs or {}\n        # `self.model` is initialized lazily to avoid unnecessary memory usage.\n        self.model: PreTrainedModel | None = None\n        self.model_kwargs = get_default_model_kwargs(model_kwargs)\n        self.load_peft = load_peft\n        self.amp_dtype = amp_dtype\n        self.model_limit_tokens = model_limit_tokens\n        self.tool_parser = tool_parser\n        self.tools = tools\n        logger.info(f\"amp_dtype: {amp_dtype}\")\n        logger.info(f\"random seed: {random_seed}\")\n        transformers.set_seed(random_seed)\n\n    @staticmethod\n    def load_model(method: Callable) -&gt; Callable:\n        \"\"\"Decorator to load the model lazily.\"\"\"\n\n        def wrapper(self: HuggingFaceLM, *args: tuple, **kwargs: dict) -&gt; Callable:\n            if self.model is None:\n                if not self.load_peft:\n                    self.model = AutoModelForCausalLM.from_pretrained(\n                        self._model_name_or_path,\n                        **self.model_kwargs,\n                    )\n                else:\n                    from peft import AutoPeftModelForCausalLM\n\n                    self.model = AutoPeftModelForCausalLM.from_pretrained(\n                        self._model_name_or_path,\n                        **self.model_kwargs,\n                    )\n\n                self.model.eval()\n\n                if self.model_limit_tokens == \"default\":\n                    hf_config = self.model.config.to_dict()\n                    if \"n_positions\" in hf_config:\n                        self.model_limit_tokens = hf_config[\"n_positions\"]\n                    elif \"max_position_embeddings\" in hf_config:\n                        self.model_limit_tokens = hf_config[\"max_position_embeddings\"]\n                    else:\n                        msg = (\n                            \"`model_limit_tokens` was set to \u201cdefault\u201d, but the default max_position_embedeings \"\n                            \"could not be found in the config. Set it to `None`.\"\n                        )\n                        logger.warning(msg)\n\n                logger.info(f\"model device: {self.model.device}\")\n                logger.info(f\"model dtype: {self.model.dtype}\")\n            return method(self, *args, **kwargs)\n\n        return wrapper\n\n    def _get_amp_context(self) -&gt; contextlib.AbstractContextManager:\n        if self.amp_dtype is None:\n            return contextlib.nullcontext()\n        if self.amp_dtype == \"float16\":\n            return torch.amp.autocast(\n                device_type=self.model.device.type,\n                dtype=torch.float16,\n            )\n        if self.amp_dtype == \"bfloat16\":\n            return torch.amp.autocast(\n                device_type=self.model.device.type,\n                dtype=torch.bfloat16,\n            )\n\n        msg = f\"Invalid amp_dtype: {self.amp_dtype}\"\n        raise ValueError(msg)\n\n    def _get_stop_token_ids(self, stop_sequences: list[str]) -&gt; list[int]:\n        stop_token_ids: list[int] = []\n        for stop_seq in stop_sequences:\n            # Try to convert string to id using `convert_tokens_to_ids`\n            # We do not use the `encode` method\n            # because in the case of sentencepiece-based tokenizers,\n            # calling the encode method adds a redundant space at the beginning of the string,\n            stop_token_id = self.tokenizer.convert_tokens_to_ids(stop_seq)\n\n            # NeoXTokenizer returns Unk when calling convert_tokens_ids\n            # because each token is stored in a peculiar way\n            # Ex. \"\u300d\" -&gt; \"\u00e3\u0122\u012f\"\n            if stop_token_id == self.tokenizer.unk_token_id:\n                # In such a case, we try to get the ID by calling the encode method.\n                stop_seq_tokens = self.tokenizer.encode(stop_seq, add_special_tokens=False)\n                if stop_seq_tokens:\n                    stop_token_id = stop_seq_tokens[-1]\n            # If the token does not match the specified string itself, we do not include it as a stop token id\n            if self.tokenizer.decode(stop_token_id) != stop_seq:\n                continue\n\n            stop_token_ids.append(stop_token_id)\n        return stop_token_ids\n\n    @torch.inference_mode()\n    @load_model\n    def _batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        ignore_eos: bool = False,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        gen_kwargs = self.default_gen_kwargs.copy()\n        gen_kwargs.update(kwargs)\n        if max_new_tokens is not None:\n            gen_kwargs[\"max_new_tokens\"] = max_new_tokens\n\n        model_inputs = tokenize_text_for_lm_prefix(\n            text_list,\n            self.tokenizer,\n            add_special_tokens=self.add_special_tokens,\n        ).to(self.model.device)\n        input_token_length = model_inputs[\"input_ids\"].shape[1]\n\n        if self.model_limit_tokens:\n            model_limit_new_tokens = self.model_limit_tokens - input_token_length\n            if model_limit_new_tokens &lt;= 0:\n                msg = (\n                    f\"Received input that is longer than `model_limit_tokens = {self.model_limit_tokens}`. \"\n                    f\"This batch returns empty strings.\"\n                )\n                logger.warning(msg)\n                return [LMOutput(text=\"\", finish_reason=\"input_length_limit\") for _ in text_list]\n\n            if \"max_new_tokens\" not in gen_kwargs or model_limit_new_tokens &lt; gen_kwargs[\"max_new_tokens\"]:\n                gen_kwargs[\"max_new_tokens\"] = model_limit_new_tokens\n\n        # set the stop sequences\n        stop_sequences = normalize_stop_sequences(\n            stop_sequences_list=[\n                stop_sequences,\n                gen_kwargs.pop(\"stop_strings\", None),  # This is used in the transformers `generate` function\n                gen_kwargs.pop(\"stop_sequences\", None),  # This is a common variable name used in flexeval\n            ],\n            bos_token=self.tokenizer.bos_token,\n            eos_token=self.tokenizer.eos_token,\n            ignore_eos=ignore_eos,\n        )\n        stop_token_ids = self._get_stop_token_ids(stop_sequences)\n        gen_kwargs.update(\n            {\n                \"eos_token_id\": stop_token_ids,\n                \"pad_token_id\": self.tokenizer.pad_token_id,\n            },\n        )\n\n        with self._get_amp_context():\n            generated_tokens = self.model.generate(**model_inputs, **gen_kwargs)\n\n        # We strip the input text and stop sequences from the output text.\n        lm_outputs: list[LMOutput] = []\n        for generated_tensor in generated_tokens:\n            input_tensor = generated_tensor[:input_token_length]\n            output_tensor = generated_tensor[input_token_length:]\n\n            input_tokens = [t for t in input_tensor.tolist() if t != self.tokenizer.pad_token_id]\n            output_tokens = [t for t in output_tensor.tolist() if t != self.tokenizer.pad_token_id]\n            decoded_text = decode_for_lm_continuation(output_tokens, input_tokens, self.tokenizer)\n\n            finish_reason = \"length\"\n            for stop_seq in stop_sequences:\n                idx = decoded_text.find(stop_seq)\n                if idx != -1:\n                    decoded_text = decoded_text[:idx]\n                    finish_reason = \"stop\"\n            lm_outputs.append(LMOutput(text=decoded_text, finish_reason=finish_reason))\n        return lm_outputs\n\n    @load_model\n    def _batch_generate_chat_response(\n        self,\n        chat_messages_list: list[list[dict[str, Any]]],\n        tools_list: list[list[dict[str, Any]] | None] | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        if tools_list is None:\n            tools_list = [self.tools] * len(chat_messages_list)\n        if self.system_message is not None:\n            for chat_messages in chat_messages_list:\n                chat_messages.insert(0, {\"role\": \"system\", \"content\": self.system_message})\n        chat_messages_as_string = [\n            self.tokenizer.apply_chat_template(\n                deserialize_tool_calls_in_messages(chat_messages),\n                tools=tools,\n                tokenize=False,\n                add_generation_prompt=True,\n                chat_template=self.custom_chat_template,\n                **self.chat_template_kwargs,\n            )\n            for chat_messages, tools in zip(chat_messages_list, tools_list)\n        ]\n        lm_outputs = self._batch_complete_text(chat_messages_as_string, **kwargs)\n        if self.tool_parser:\n            for lm_output, tools in zip(lm_outputs, tools_list):\n                if tools is None:\n                    continue\n                lm_output: LMOutput\n                parsed_tool_calling_message = self.tool_parser(lm_output.text)\n                lm_output.tool_calls = parsed_tool_calling_message.tool_call_dicts\n                lm_output.raw_text = parsed_tool_calling_message.raw_text\n                lm_output.text = parsed_tool_calling_message.text\n                lm_output.tool_call_validation_result = parsed_tool_calling_message.validation_result\n\n        return lm_outputs\n\n    @torch.inference_mode()\n    @load_model\n    def _batch_compute_log_probs(\n        self,\n        text_list: list[str],\n        prefix_list: list[str] | None = None,\n        stride: int | None = None,\n    ) -&gt; list[float]:\n        batch_size = len(text_list)\n\n        # prepare prefix encoding\n        prefix_list = prefix_list if prefix_list else [\"\" for _ in range(batch_size)]\n        # If the prefix is an empty string, replace it with the bos token regardless of the model being trained with it.\n        # This is needed to correctly calculate the log probabilities of the first token.\n        for i in range(batch_size):\n            if prefix_list[i] == \"\":\n                prefix_list[i] = self.tokenizer.bos_token\n\n        prefix_encoding = tokenize_text_for_lm_prefix(\n            prefix_list,\n            self.tokenizer,\n            add_special_tokens=self.add_special_tokens,\n        )\n\n        # prepare continuation encoding\n        # If the last token is a special token, it is treated as a beginning of a new sentence.\n        continuation_encoding = tokenize_text_for_lm_continuation(\n            text_list,\n            self.tokenizer,\n            as_continuation=[\n                prefix_ids[-1] not in self.tokenizer.all_special_ids for prefix_ids in prefix_encoding.input_ids\n            ],\n        )\n\n        input_data_dict: dict[str, torch.Tensor] = {}\n        for key in continuation_encoding:\n            input_data_dict[key] = torch.cat(\n                [prefix_encoding[key].long(), continuation_encoding[key].long()],\n                dim=1,\n            )\n        input_encoding = BatchEncoding(input_data_dict)\n\n        max_length = self.model.config.max_position_embeddings\n        stride = stride or max_length // 2\n        if not (0 &lt; stride &lt; max_length):\n            msg = f\"stride must be in (0, {max_length}), but got {stride}\"\n            raise ValueError(msg)\n        sequence_length = input_encoding.input_ids.size(1)\n\n        with self._get_amp_context():\n            # stores log probabilities of the next token for each input token\n            last_computed_index: int = 0\n            log_prob_of_next = torch.zeros_like(\n                input_encoding.input_ids,\n                dtype=torch.float32,\n            )\n            for chunk_start in range(0, sequence_length, stride):\n                chunk_end = min(chunk_start + max_length, sequence_length)\n\n                # Visualize the input / output processing\n                # input_encoding.input_ids: [ 0  1  2  3  4 ]\n                # chunk_input_ids:          [ 0  1  2  3    ]\n                # chunk_target_ids:         [    1  2  3  4 ]\n\n                input_start = chunk_start\n                input_end = chunk_end - 1\n\n                chunk_input_ids = input_encoding.input_ids[:, input_start:input_end].to(self.model.device)\n                chunk_input_mask = input_encoding.attention_mask[:, input_start:input_end].to(self.model.device)\n                chunk_target_ids = input_encoding.input_ids[:, chunk_start + 1 : chunk_end].to(self.model.device)\n\n                chunkmodel_inputs = self.model.prepare_inputs_for_generation(\n                    chunk_input_ids,\n                    attention_mask=chunk_input_mask,\n                )\n                lm_outputs = self.model.forward(**chunkmodel_inputs)\n\n                chunk_log_probs = F.log_softmax(lm_outputs.logits, dim=-1)\n                # shape of chunk_log_probs: (batch_size, sequence_length, vocab_size)\n                # shape of target_ids: (batch_size, sequence_length)\n                # get the log probs of the target ids\n                chunk_next_log_probs = chunk_log_probs.gather(\n                    dim=-1,\n                    index=chunk_target_ids.unsqueeze(-1),\n                ).squeeze(-1)\n\n                log_prob_of_next[:, last_computed_index:input_end] = chunk_next_log_probs[\n                    :,\n                    last_computed_index - input_start :,\n                ]\n\n                last_computed_index = input_end\n\n                if chunk_end == sequence_length:\n                    break\n\n            log_prob_mask = input_encoding.attention_mask.clone()\n            # replace the last token's log prob with 0\n            for i in range(log_prob_mask.shape[0]):\n                last_non_pad_index = log_prob_mask[i].nonzero(as_tuple=True)[0][-1].item()\n                log_prob_mask[i, last_non_pad_index] = 0\n            # mask out log probs of prefix tokens\n            prefix_length = prefix_encoding.input_ids.shape[1]\n            if prefix_length &gt; 0:\n                log_prob_mask[:, : prefix_length - 1] = 0\n            total_log_probs = (log_prob_of_next * log_prob_mask).sum(dim=-1)\n        return total_log_probs.tolist()\n\n    @load_model\n    def _batch_compute_chat_log_probs(\n        self, prompt_list: list[list[dict[str, Any]]], response_list: list[dict[str, Any]]\n    ) -&gt; list[float]:\n        prompt_as_string: list[str] = []\n        response_as_string: list[str] = []\n        for prompt, response in zip(prompt_list, response_list):\n            prompt_as_string_i, response_as_string_i = get_prefix_and_completion_from_chat(\n                prompt,\n                response,\n                self.tokenizer,\n                custom_chat_template=self.custom_chat_template,\n            )\n            prompt_as_string.append(prompt_as_string_i)\n            response_as_string.append(response_as_string_i)\n        return self._batch_compute_log_probs(response_as_string, prefix_list=prompt_as_string)\n\n    def cleanup_resources(self) -&gt; None:\n        del self.model\n        self.model = None\n        gc.collect()\n        if torch.cuda.is_available():\n            logger.info(\"Cleaning up CUDA resources...\")\n            torch.cuda.empty_cache()\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(model={self._model_name_or_path!r})\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer: PreTrainedTokenizer = from_pretrained(\n    tokenizer, **tokenizer_kwargs\n)\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.custom_chat_template","title":"custom_chat_template  <code>instance-attribute</code>","text":"<pre><code>custom_chat_template = custom_chat_template\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.system_message","title":"system_message  <code>instance-attribute</code>","text":"<pre><code>system_message = system_message\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.chat_template_kwargs","title":"chat_template_kwargs  <code>instance-attribute</code>","text":"<pre><code>chat_template_kwargs = chat_template_kwargs or {}\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.add_special_tokens","title":"add_special_tokens  <code>instance-attribute</code>","text":"<pre><code>add_special_tokens = add_special_tokens\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.default_gen_kwargs","title":"default_gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>default_gen_kwargs = default_gen_kwargs or {}\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: PreTrainedModel | None = None\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.model_kwargs","title":"model_kwargs  <code>instance-attribute</code>","text":"<pre><code>model_kwargs = get_default_model_kwargs(model_kwargs)\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.load_peft","title":"load_peft  <code>instance-attribute</code>","text":"<pre><code>load_peft = load_peft\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.amp_dtype","title":"amp_dtype  <code>instance-attribute</code>","text":"<pre><code>amp_dtype = amp_dtype\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.model_limit_tokens","title":"model_limit_tokens  <code>instance-attribute</code>","text":"<pre><code>model_limit_tokens = model_limit_tokens\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.tool_parser","title":"tool_parser  <code>instance-attribute</code>","text":"<pre><code>tool_parser = tool_parser\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools = tools\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.__init__","title":"__init__","text":"<pre><code>__init__(\n    model: str,\n    model_kwargs: dict[str, Any] | None = None,\n    tokenizer: str | None = None,\n    tokenizer_kwargs: dict[str, Any] | None = None,\n    add_special_tokens: bool = False,\n    amp_dtype: Literal[\"float16\", \"bfloat16\"] | None = None,\n    random_seed: int = 42,\n    load_peft: bool = False,\n    custom_chat_template: str | None = None,\n    chat_template_kwargs: dict[str, Any] | None = None,\n    system_message: str | None = None,\n    default_gen_kwargs: dict[str, Any] | None = None,\n    string_processors: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    model_limit_tokens: int\n    | None\n    | Literal[\"default\"] = \"default\",\n    tool_parser: ToolParser | None = None,\n    tools: list[dict[str, Any]] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/hf_lm.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    model_kwargs: dict[str, Any] | None = None,\n    tokenizer: str | None = None,\n    tokenizer_kwargs: dict[str, Any] | None = None,\n    add_special_tokens: bool = False,\n    amp_dtype: Literal[\"float16\", \"bfloat16\"] | None = None,\n    random_seed: int = 42,\n    load_peft: bool = False,\n    custom_chat_template: str | None = None,\n    chat_template_kwargs: dict[str, Any] | None = None,\n    system_message: str | None = None,\n    default_gen_kwargs: dict[str, Any] | None = None,\n    string_processors: StringProcessor | list[StringProcessor] | None = None,\n    model_limit_tokens: int | None | Literal[\"default\"] = \"default\",\n    tool_parser: ToolParser | None = None,\n    tools: list[dict[str, Any]] | None = None,\n) -&gt; None:\n    super().__init__(string_processors=string_processors)\n    self._model_name_or_path = model\n    tokenizer = tokenizer if tokenizer else model\n    tokenizer_kwargs = tokenizer_kwargs or {}\n    self.tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(tokenizer, **tokenizer_kwargs)\n    self.custom_chat_template = custom_chat_template\n    self.system_message = system_message\n    self.chat_template_kwargs = chat_template_kwargs or {}\n    self.add_special_tokens = add_special_tokens\n    self.default_gen_kwargs = default_gen_kwargs or {}\n    # `self.model` is initialized lazily to avoid unnecessary memory usage.\n    self.model: PreTrainedModel | None = None\n    self.model_kwargs = get_default_model_kwargs(model_kwargs)\n    self.load_peft = load_peft\n    self.amp_dtype = amp_dtype\n    self.model_limit_tokens = model_limit_tokens\n    self.tool_parser = tool_parser\n    self.tools = tools\n    logger.info(f\"amp_dtype: {amp_dtype}\")\n    logger.info(f\"random seed: {random_seed}\")\n    transformers.set_seed(random_seed)\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.load_model","title":"load_model  <code>staticmethod</code>","text":"<pre><code>load_model(method: Callable) -&gt; Callable\n</code></pre> <p>Decorator to load the model lazily.</p> Source code in <code>flexeval/core/language_model/hf_lm.py</code> <pre><code>@staticmethod\ndef load_model(method: Callable) -&gt; Callable:\n    \"\"\"Decorator to load the model lazily.\"\"\"\n\n    def wrapper(self: HuggingFaceLM, *args: tuple, **kwargs: dict) -&gt; Callable:\n        if self.model is None:\n            if not self.load_peft:\n                self.model = AutoModelForCausalLM.from_pretrained(\n                    self._model_name_or_path,\n                    **self.model_kwargs,\n                )\n            else:\n                from peft import AutoPeftModelForCausalLM\n\n                self.model = AutoPeftModelForCausalLM.from_pretrained(\n                    self._model_name_or_path,\n                    **self.model_kwargs,\n                )\n\n            self.model.eval()\n\n            if self.model_limit_tokens == \"default\":\n                hf_config = self.model.config.to_dict()\n                if \"n_positions\" in hf_config:\n                    self.model_limit_tokens = hf_config[\"n_positions\"]\n                elif \"max_position_embeddings\" in hf_config:\n                    self.model_limit_tokens = hf_config[\"max_position_embeddings\"]\n                else:\n                    msg = (\n                        \"`model_limit_tokens` was set to \u201cdefault\u201d, but the default max_position_embedeings \"\n                        \"could not be found in the config. Set it to `None`.\"\n                    )\n                    logger.warning(msg)\n\n            logger.info(f\"model device: {self.model.device}\")\n            logger.info(f\"model dtype: {self.model.dtype}\")\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.cleanup_resources","title":"cleanup_resources","text":"<pre><code>cleanup_resources() -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/hf_lm.py</code> <pre><code>def cleanup_resources(self) -&gt; None:\n    del self.model\n    self.model = None\n    gc.collect()\n    if torch.cuda.is_available():\n        logger.info(\"Cleaning up CUDA resources...\")\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.hf_lm.HuggingFaceLM.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/language_model/hf_lm.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(model={self._model_name_or_path!r})\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.litellm_api.LiteLLMChatAPI","title":"LiteLLMChatAPI","text":"<p>LanguageModel implementation using LiteLLM. Various APIs are available, such as OpenAI, Claude, Gemini, etc. See also: https://docs.litellm.ai/docs/providers</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>, default:                   <code>'openai/gpt-3.5-turbo'</code> )           \u2013            <p>The name of the model to use. e.g. 'openai/gpt-3.5-turbo',</p> </li> <li> <code>default_gen_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default generation kwargs to use when calling the API.</p> </li> <li> <code>developer_message</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Instructions to the model that are prioritized ahead of user messages. Previously called the system prompt.</p> </li> <li> <code>string_processors</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>A single or a list of StringProcessor objects to process the model's output.</p> </li> <li> <code>ignore_seed</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, ignore the seed specified in default_gen_kwargs. This is an option for models that do not support seed parameters such as anthropic/claude.</p> </li> <li> <code>model_limit_completion_tokens</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>An upper limit on the number of tokens the model can generate. For example, if a too-large <code>max_new_tokens</code> is given to generate_chat_response(), this value will cap it.</p> </li> <li> <code>max_parallel_requests</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of parallel requests to send to the API.</p> </li> <li> <code>tools</code>               (<code>list[dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default tools to use in chat responses when no tools are explicitly provided.</p> </li> </ul> Source code in <code>flexeval/core/language_model/litellm_api.py</code> <pre><code>class LiteLLMChatAPI(OpenAIChatAPI):\n    \"\"\"\n    LanguageModel implementation using LiteLLM.\n    Various APIs are available, such as OpenAI, Claude, Gemini, etc.\n    See also: https://docs.litellm.ai/docs/providers\n\n    Args:\n        model: The name of the model to use. e.g. 'openai/gpt-3.5-turbo',\n        default_gen_kwargs: Default generation kwargs to use when calling the API.\n        developer_message: Instructions to the model that are prioritized ahead of user messages.\n            Previously called the system prompt.\n        string_processors: A single or a list of StringProcessor objects to process the model's output.\n        ignore_seed: If True, ignore the seed specified in default_gen_kwargs.\n            This is an option for models that do not support seed parameters such as anthropic/claude.\n        model_limit_completion_tokens: An upper limit on the number of tokens the model can generate.\n            For example, if a too-large `max_new_tokens` is given to generate_chat_response(), this value will cap it.\n        max_parallel_requests: Maximum number of parallel requests to send to the API.\n        tools: Default tools to use in chat responses when no tools are explicitly provided.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"openai/gpt-3.5-turbo\",\n        default_gen_kwargs: dict[str, Any] | None = None,\n        developer_message: str | None = None,\n        string_processors: StringProcessor | list[StringProcessor] | None = None,\n        ignore_seed: bool = False,\n        model_limit_completion_tokens: int | None = None,\n        max_parallel_requests: int | None = None,\n        tools: list[dict[str, Any]] | None = None,\n    ) -&gt; None:\n        super().__init__(\n            model=model,\n            api_headers=None,\n            default_gen_kwargs=default_gen_kwargs,\n            developer_message=developer_message,\n            string_processors=string_processors,\n            model_limit_new_tokens=model_limit_completion_tokens,\n            max_parallel_requests=max_parallel_requests,\n            tools=tools,\n        )\n        self.model = model\n        self.default_gen_kwargs = default_gen_kwargs or {}\n        # convert the flexeval-specific argument name to the OpenAI-specific name\n        if \"max_new_tokens\" in self.default_gen_kwargs:\n            self.default_gen_kwargs[\"max_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n\n        self.api_call_func = completion\n        self.empty_response = convert_to_model_response_object(\n            response_object=EMPTY_RESPONSE_OPENAI.to_dict(),\n            model_response_object=ModelResponse(),\n        )\n        self.ignore_seed = ignore_seed\n\n    def _batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        if \"seed\" in kwargs and self.ignore_seed:\n            kwargs.pop(\"seed\")\n        return super()._batch_complete_text(text_list, stop_sequences, max_new_tokens, **kwargs)\n\n    def _batch_generate_chat_response(\n        self,\n        chat_messages_list: list[list[dict[str, Any]]],\n        tools_list: list[list[dict[str, Any]] | None] | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        if \"seed\" in kwargs and self.ignore_seed:\n            kwargs.pop(\"seed\")\n        return super()._batch_generate_chat_response(chat_messages_list, tools_list, **kwargs)\n\n    def _batch_compute_chat_log_probs(\n        self,\n        prompt_list: list[list[dict[str, Any]]],\n        response_list: list[dict[str, Any]],\n        temperature: float = 0,\n        seed: int = 42,\n        top_logprobs: int = 20,\n    ) -&gt; list[float | None]:\n        raise NotImplementedError\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.litellm_api.LiteLLMChatAPI.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.litellm_api.LiteLLMChatAPI.default_gen_kwargs","title":"default_gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>default_gen_kwargs = default_gen_kwargs or {}\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.litellm_api.LiteLLMChatAPI.api_call_func","title":"api_call_func  <code>instance-attribute</code>","text":"<pre><code>api_call_func = completion\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.litellm_api.LiteLLMChatAPI.empty_response","title":"empty_response  <code>instance-attribute</code>","text":"<pre><code>empty_response = convert_to_model_response_object(\n    response_object=to_dict(),\n    model_response_object=ModelResponse(),\n)\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.litellm_api.LiteLLMChatAPI.ignore_seed","title":"ignore_seed  <code>instance-attribute</code>","text":"<pre><code>ignore_seed = ignore_seed\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.litellm_api.LiteLLMChatAPI.__init__","title":"__init__","text":"<pre><code>__init__(\n    model: str = \"openai/gpt-3.5-turbo\",\n    default_gen_kwargs: dict[str, Any] | None = None,\n    developer_message: str | None = None,\n    string_processors: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    ignore_seed: bool = False,\n    model_limit_completion_tokens: int | None = None,\n    max_parallel_requests: int | None = None,\n    tools: list[dict[str, Any]] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/litellm_api.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"openai/gpt-3.5-turbo\",\n    default_gen_kwargs: dict[str, Any] | None = None,\n    developer_message: str | None = None,\n    string_processors: StringProcessor | list[StringProcessor] | None = None,\n    ignore_seed: bool = False,\n    model_limit_completion_tokens: int | None = None,\n    max_parallel_requests: int | None = None,\n    tools: list[dict[str, Any]] | None = None,\n) -&gt; None:\n    super().__init__(\n        model=model,\n        api_headers=None,\n        default_gen_kwargs=default_gen_kwargs,\n        developer_message=developer_message,\n        string_processors=string_processors,\n        model_limit_new_tokens=model_limit_completion_tokens,\n        max_parallel_requests=max_parallel_requests,\n        tools=tools,\n    )\n    self.model = model\n    self.default_gen_kwargs = default_gen_kwargs or {}\n    # convert the flexeval-specific argument name to the OpenAI-specific name\n    if \"max_new_tokens\" in self.default_gen_kwargs:\n        self.default_gen_kwargs[\"max_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n\n    self.api_call_func = completion\n    self.empty_response = convert_to_model_response_object(\n        response_object=EMPTY_RESPONSE_OPENAI.to_dict(),\n        model_response_object=ModelResponse(),\n    )\n    self.ignore_seed = ignore_seed\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.litellm_api.LiteLLMChatAPI.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/language_model/litellm_api.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAIChatAPI","title":"OpenAIChatAPI","text":"<p>LanguageModel implementation using OpenAI's ChatGPT API. Note that this class is inherited by litellm_api.LiteLLMChatAPI, so be careful when making any modifications.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>, default:                   <code>'gpt-3.5-turbo'</code> )           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_headers</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of headers to use when making requests to the OpenAI API.</p> </li> <li> <code>default_gen_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default generation kwargs to use when calling the API.</p> </li> <li> <code>developer_message</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Instructions to the model that are prioritized ahead of user messages. Previously called the system prompt.</p> </li> <li> <code>string_processors</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>A single or a list of StringProcessor objects to process the model's output.</p> </li> <li> <code>model_limit_new_tokens</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>An upper limit on the number of tokens the model can generate. For example, if a too-large <code>max_new_tokens</code> is given to generate_chat_response(), this value will cap it.</p> </li> <li> <code>max_parallel_requests</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of parallel requests to send to the OpenAI API.</p> </li> <li> <code>tools</code>               (<code>list[dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default tools to use in chat responses when no tools are explicitly provided.</p> </li> </ul> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>class OpenAIChatAPI(LanguageModel):\n    \"\"\"\n    LanguageModel implementation using OpenAI's ChatGPT API.\n    Note that this class is inherited by litellm_api.LiteLLMChatAPI, so be careful when making any modifications.\n\n    Args:\n        model: The name of the model to use.\n        api_headers: A dictionary of headers to use when making requests to the OpenAI API.\n        default_gen_kwargs: Default generation kwargs to use when calling the API.\n        developer_message: Instructions to the model that are prioritized ahead of user messages.\n            Previously called the system prompt.\n        string_processors: A single or a list of StringProcessor objects to process the model's output.\n        model_limit_new_tokens: An upper limit on the number of tokens the model can generate.\n            For example, if a too-large `max_new_tokens` is given to generate_chat_response(), this value will cap it.\n        max_parallel_requests: Maximum number of parallel requests to send to the OpenAI API.\n        tools: Default tools to use in chat responses when no tools are explicitly provided.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"gpt-3.5-turbo\",\n        api_headers: dict[str, str] | None = None,\n        default_gen_kwargs: dict[str, Any] | None = None,\n        developer_message: str | None = None,\n        string_processors: StringProcessor | list[StringProcessor] | None = None,\n        model_limit_new_tokens: int | None = None,\n        max_parallel_requests: int | None = None,\n        tools: list[dict[str, Any]] | None = None,\n    ) -&gt; None:\n        super().__init__(string_processors=string_processors)\n        self.model = model\n        if api_headers is None:\n            api_headers = {}\n        client = OpenAI(**api_headers)\n        self.api_call_func = client.chat.completions.create\n        self.empty_response = EMPTY_RESPONSE\n        self.default_gen_kwargs = default_gen_kwargs or {}\n        # convert the flexeval-specific argument name to the OpenAI-specific name\n        if \"max_new_tokens\" in self.default_gen_kwargs:\n            self.default_gen_kwargs[\"max_completion_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n\n        self.developer_message = developer_message\n        self.model_limit_new_tokens = model_limit_new_tokens\n        self.max_parallel_requests = max_parallel_requests\n        self.tools = tools\n\n    def _parallel_run_chatgpt(\n        self,\n        messages_list: list[list[dict[str, Any]]],\n        tools_list: list[list[dict[str, Any]] | None] | None = None,\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[ChatCompletion]:\n        \"\"\"Send multiple chat requests to the OpenAI in parallel.\"\"\"\n        if self.developer_message is not None:\n            # Insert the developer message at the beginning of each conversation\n            messages_list = [\n                [{\"role\": \"developer\", \"content\": self.developer_message}, *messages] for messages in messages_list\n            ]\n\n        gen_kwargs = self.default_gen_kwargs.copy()\n        gen_kwargs.update(kwargs)\n        if max_new_tokens is not None:\n            if \"max_completion_tokens\" in gen_kwargs:\n                msg = (\n                    \"You specified both `max_new_tokens` and `max_completion_tokens` in generation kwargs. \"\n                    \"Note that `max_new_tokens` overrides `max_completion_tokens` by default. \"\n                    \"It is recommended to specify only one of them to avoid unexpected behavior.\"\n                )\n                logger.warning(msg)\n            gen_kwargs[\"max_completion_tokens\"] = max_new_tokens\n\n        if self.model_limit_new_tokens and (gen_kwargs.get(\"max_completion_tokens\", 0) &gt; self.model_limit_new_tokens):\n            msg = (\n                f\"The specified `max_new_tokens` ({gen_kwargs['max_completion_tokens']}) exceeds\"\n                f\"the model\u2019s capability ({self.model_limit_new_tokens} tokens). It will be reduced.\"\n            )\n            logger.warning(msg)\n            gen_kwargs[\"max_completion_tokens\"] = self.model_limit_new_tokens\n\n        stop_sequences = normalize_stop_sequences(\n            stop_sequences_list=[\n                stop_sequences,\n                gen_kwargs.pop(\"stop\", None),  # This is used in the OpenAI API\n                gen_kwargs.pop(\"stop_sequences\", None),  # This is a common variable name used in flexeval\n            ],\n        )\n\n        if tools_list is None:\n            tools_list = [self.tools] * len(messages_list)\n\n        max_workers = self.max_parallel_requests or len(messages_list)\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = [\n                executor.submit(\n                    _retry_on_error,\n                    openai_call=lambda messages=messages, tools=tools: self.api_call_func(\n                        model=self.model,\n                        messages=messages,\n                        tools=tools,\n                        stop=stop_sequences or NotGiven(),\n                        **gen_kwargs,\n                    ),\n                    empty_response=self.empty_response,\n                )\n                for messages, tools in zip(messages_list, tools_list)\n            ]\n            return [future.result() for future in futures]\n\n    def _batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        messages_list = [[{\"role\": \"user\", \"content\": text}] for text in text_list]\n        api_responses = self._parallel_run_chatgpt(\n            messages_list,\n            stop_sequences=stop_sequences,\n            max_new_tokens=max_new_tokens,\n            **kwargs,\n        )\n        outputs = [\n            LMOutput(text=res.choices[0].message.content, finish_reason=res.choices[0].finish_reason)\n            for res in api_responses\n        ]\n\n        if all(output.text == \"\" for output in outputs):\n            logger.warning(\"All generated texts are empty strings. Something may be wrong.\")\n        return outputs\n\n    def _batch_generate_chat_response(\n        self,\n        chat_messages_list: list[list[dict[str, Any]]],\n        tools_list: list[list[dict[str, Any]] | None] | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        api_responses = self._parallel_run_chatgpt(chat_messages_list, tools_list=tools_list, **kwargs)\n        outputs = [\n            LMOutput(\n                text=res.choices[0].message.content,\n                finish_reason=res.choices[0].finish_reason,\n                tool_calls=[tool_call.to_dict() for tool_call in res.choices[0].message.tool_calls]\n                if res.choices[0].message.tool_calls\n                else None,\n            )\n            for res in api_responses\n        ]\n        if all(output.text == \"\" for output in outputs):\n            logger.warning(\"All generated texts are empty strings. Something may go wrong.\")\n        return outputs\n\n    def _batch_compute_chat_log_probs(\n        self,\n        prompt_list: list[list[dict[str, Any]]],\n        response_list: list[dict[str, Any]],\n        temperature: float = 0,\n        seed: int = 42,\n        top_logprobs: int = 20,\n    ) -&gt; list[float | None]:\n        \"\"\"\n        Return logprob of one-token response only due to restriction of OpenAI API.\n        If you pass a response with two or more tokens, raise an error.\n\n        This function is mainly used for calculating weighted average of multi-choice prompts.\n        Under the design of this function, we need to pass the same prompt (the number of choice) times.\n        We only need one request for one prompt because OpenAI API returns a list of log probs.\n        So, this function removes duplicates of prompts before requesting API and\n        returns log probs by restoring the raw prompt list.\n        \"\"\"\n\n        # Check the number of tokens is 1\n        response_contents = [resp[\"content\"] for resp in response_list]\n        for response_content in response_contents:\n            num_tokens = number_of_tokens_in_openai_model(self.model, response_content)\n            if num_tokens &gt; 1:\n                err_msg = f\"OpenAIChatAPI.batch_compute_chat_log_probs is not applicable for two or more tokens of response content: '{response_content}'\"  # noqa: E501\n                raise NotImplementedError(err_msg)\n\n        # For saving cost, remove duplication from message_list for an API request.\n        unique_prompt_list = remove_duplicates_from_prompt_list(prompt_list)\n        api_responses = self._parallel_run_chatgpt(\n            unique_prompt_list,\n            max_completion_tokens=1,\n            seed=seed,\n            logprobs=True,\n            top_logprobs=top_logprobs,\n        )\n\n        log_probs = []\n        top_logprobs_list = [res.choices[0].logprobs.content[0].top_logprobs for res in api_responses]\n        for index, prompt in enumerate(prompt_list):\n            target_token = response_contents[index]\n            index_in_unique = unique_prompt_list.index(prompt)\n\n            log_prob = None  # if target token not in top_logprobs, return None for log_prob of the token\n            top_logprobs = top_logprobs_list[index_in_unique]\n            for token_logprob in top_logprobs:\n                if token_logprob.token == target_token:\n                    log_prob = token_logprob.logprob\n                    break\n            log_probs.append(log_prob)\n\n        return log_probs\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAIChatAPI.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAIChatAPI.api_call_func","title":"api_call_func  <code>instance-attribute</code>","text":"<pre><code>api_call_func = create\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAIChatAPI.empty_response","title":"empty_response  <code>instance-attribute</code>","text":"<pre><code>empty_response = EMPTY_RESPONSE\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAIChatAPI.default_gen_kwargs","title":"default_gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>default_gen_kwargs = default_gen_kwargs or {}\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAIChatAPI.developer_message","title":"developer_message  <code>instance-attribute</code>","text":"<pre><code>developer_message = developer_message\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAIChatAPI.model_limit_new_tokens","title":"model_limit_new_tokens  <code>instance-attribute</code>","text":"<pre><code>model_limit_new_tokens = model_limit_new_tokens\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAIChatAPI.max_parallel_requests","title":"max_parallel_requests  <code>instance-attribute</code>","text":"<pre><code>max_parallel_requests = max_parallel_requests\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAIChatAPI.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools = tools\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAIChatAPI.__init__","title":"__init__","text":"<pre><code>__init__(\n    model: str = \"gpt-3.5-turbo\",\n    api_headers: dict[str, str] | None = None,\n    default_gen_kwargs: dict[str, Any] | None = None,\n    developer_message: str | None = None,\n    string_processors: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    model_limit_new_tokens: int | None = None,\n    max_parallel_requests: int | None = None,\n    tools: list[dict[str, Any]] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"gpt-3.5-turbo\",\n    api_headers: dict[str, str] | None = None,\n    default_gen_kwargs: dict[str, Any] | None = None,\n    developer_message: str | None = None,\n    string_processors: StringProcessor | list[StringProcessor] | None = None,\n    model_limit_new_tokens: int | None = None,\n    max_parallel_requests: int | None = None,\n    tools: list[dict[str, Any]] | None = None,\n) -&gt; None:\n    super().__init__(string_processors=string_processors)\n    self.model = model\n    if api_headers is None:\n        api_headers = {}\n    client = OpenAI(**api_headers)\n    self.api_call_func = client.chat.completions.create\n    self.empty_response = EMPTY_RESPONSE\n    self.default_gen_kwargs = default_gen_kwargs or {}\n    # convert the flexeval-specific argument name to the OpenAI-specific name\n    if \"max_new_tokens\" in self.default_gen_kwargs:\n        self.default_gen_kwargs[\"max_completion_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n\n    self.developer_message = developer_message\n    self.model_limit_new_tokens = model_limit_new_tokens\n    self.max_parallel_requests = max_parallel_requests\n    self.tools = tools\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAIChatAPI.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAICompletionAPI","title":"OpenAICompletionAPI","text":"<p>LanguageModel implementation using OpenAI's Completion API.</p> <p>Note that Completion API is a legacy API, with only a few models (such as gpt-3.5-turbo-instruct) supported by OpenAI. This LanguageModel implementation is primarily intended for use with on-premise VLLM servers, as described in the documentation: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>, default:                   <code>'gpt-3.5-turbo-instruct'</code> )           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_headers</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of headers to use when making requests to the OpenAI API.</p> </li> <li> <code>default_gen_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default generation kwargs to use when calling the API.</p> </li> <li> <code>string_processors</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>A single or a list of StringProcessor objects to process the model's output.</p> </li> <li> <code>max_parallel_requests</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum number of parallel requests to send to the OpenAI API.</p> </li> </ul> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>class OpenAICompletionAPI(LanguageModel):\n    \"\"\"LanguageModel implementation using OpenAI's Completion API.\n\n    Note that Completion API is a legacy API, with only a few models (such as gpt-3.5-turbo-instruct)\n    supported by OpenAI. This LanguageModel implementation is primarily intended for use with on-premise\n    VLLM servers, as described in the documentation: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n\n    Args:\n        model: The name of the model to use.\n        api_headers: A dictionary of headers to use when making requests to the OpenAI API.\n        default_gen_kwargs: Default generation kwargs to use when calling the API.\n        string_processors: A single or a list of StringProcessor objects to process the model's output.\n        max_parallel_requests: Maximum number of parallel requests to send to the OpenAI API.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"gpt-3.5-turbo-instruct\",\n        api_headers: dict[str, str] | None = None,\n        default_gen_kwargs: dict[str, Any] | None = None,\n        string_processors: StringProcessor | list[StringProcessor] | None = None,\n        max_parallel_requests: int | None = None,\n    ) -&gt; None:\n        super().__init__(string_processors=string_processors)\n        self.model = model\n        if api_headers is None:\n            api_headers = {}\n        client = OpenAI(**api_headers)\n        self.api_call_func = client.completions.create\n        self.empty_response = EMPTY_RESPONSE\n        self.default_gen_kwargs = default_gen_kwargs or {}\n        self.max_parallel_requests = max_parallel_requests\n        # convert the flexeval-specific argument name to the OpenAI-specific name\n        if \"max_new_tokens\" in self.default_gen_kwargs:\n            self.default_gen_kwargs[\"max_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n\n    def _parallel_run_chatgpt(\n        self,\n        prompt_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[str]:\n        \"\"\"Send multiple completion requests to the OpenAI in parallel.\"\"\"\n\n        gen_kwargs = self.default_gen_kwargs.copy()\n        gen_kwargs.update(kwargs)\n        if max_new_tokens is not None:\n            gen_kwargs[\"max_tokens\"] = max_new_tokens\n\n        stop_sequences = normalize_stop_sequences(\n            stop_sequences_list=[\n                stop_sequences,\n                gen_kwargs.pop(\"stop\", None),  # This is used in the OpenAI API\n                gen_kwargs.pop(\"stop_sequences\", None),  # This is a common variable name used in flexeval\n            ],\n        )\n        if stop_sequences:\n            gen_kwargs[\"stop\"] = stop_sequences\n\n        max_workers = self.max_parallel_requests or len(prompt_list)\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = [\n                executor.submit(\n                    _retry_on_error,\n                    # Define an anonymous function with a lambda expression and pass it,\n                    # and call it inside the _retry_on_error function\n                    openai_call=lambda x=ms: self.api_call_func(\n                        model=self.model,\n                        prompt=x,\n                        **gen_kwargs,\n                    ),\n                    empty_response=self.empty_response,\n                )\n                for ms in prompt_list\n            ]\n        return [future.result() for future in futures]\n\n    def _batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        api_responses = self._parallel_run_chatgpt(\n            text_list,\n            stop_sequences=stop_sequences,\n            max_new_tokens=max_new_tokens,\n            **kwargs,\n        )\n\n        return [LMOutput(text=res.choices[0].text, finish_reason=res.choices[0].finish_reason) for res in api_responses]\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAICompletionAPI.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAICompletionAPI.api_call_func","title":"api_call_func  <code>instance-attribute</code>","text":"<pre><code>api_call_func = create\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAICompletionAPI.empty_response","title":"empty_response  <code>instance-attribute</code>","text":"<pre><code>empty_response = EMPTY_RESPONSE\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAICompletionAPI.default_gen_kwargs","title":"default_gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>default_gen_kwargs = default_gen_kwargs or {}\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAICompletionAPI.max_parallel_requests","title":"max_parallel_requests  <code>instance-attribute</code>","text":"<pre><code>max_parallel_requests = max_parallel_requests\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAICompletionAPI.__init__","title":"__init__","text":"<pre><code>__init__(\n    model: str = \"gpt-3.5-turbo-instruct\",\n    api_headers: dict[str, str] | None = None,\n    default_gen_kwargs: dict[str, Any] | None = None,\n    string_processors: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    max_parallel_requests: int | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"gpt-3.5-turbo-instruct\",\n    api_headers: dict[str, str] | None = None,\n    default_gen_kwargs: dict[str, Any] | None = None,\n    string_processors: StringProcessor | list[StringProcessor] | None = None,\n    max_parallel_requests: int | None = None,\n) -&gt; None:\n    super().__init__(string_processors=string_processors)\n    self.model = model\n    if api_headers is None:\n        api_headers = {}\n    client = OpenAI(**api_headers)\n    self.api_call_func = client.completions.create\n    self.empty_response = EMPTY_RESPONSE\n    self.default_gen_kwargs = default_gen_kwargs or {}\n    self.max_parallel_requests = max_parallel_requests\n    # convert the flexeval-specific argument name to the OpenAI-specific name\n    if \"max_new_tokens\" in self.default_gen_kwargs:\n        self.default_gen_kwargs[\"max_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_api.OpenAICompletionAPI.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/language_model/openai_api.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI","title":"OpenAIChatBatchAPI","text":"<p>LanguageModel implementation using OpenAI's ChatGPT API for Batch API. NOTE: Batch size should be more than or equal to the size of the given dataset for efficient generation.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The name of the model to use.</p> </li> <li> <code>api_headers</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of headers to use when making requests to the OpenAI API.</p> </li> <li> <code>polling_interval_seconds</code>               (<code>int</code>, default:                   <code>60</code> )           \u2013            <p>The interval in seconds to poll the batch status.</p> </li> <li> <code>default_gen_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default generation kwargs to use when calling the API.</p> </li> <li> <code>developer_message</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Instructions to the model that are prioritized ahead of user messages. Previously called the system prompt.</p> </li> <li> <code>string_processors</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>A single or a list of StringProcessor objects to process the model's output.</p> </li> <li> <code>model_limit_new_tokens</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>An upper limit on the number of tokens the model can generate. For example, if a too-large <code>max_new_tokens</code> is given to generate_chat_response(), this value will cap it.</p> </li> <li> <code>tools</code>               (<code>list[dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default tools to use in chat responses when no tools are explicitly provided.</p> </li> </ul> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>class OpenAIChatBatchAPI(LanguageModel):\n    \"\"\"LanguageModel implementation using OpenAI's ChatGPT API for Batch API.\n    NOTE: Batch size should be more than or equal to the size of the given dataset for efficient generation.\n\n    Args:\n        model: The name of the model to use.\n        api_headers: A dictionary of headers to use when making requests to the OpenAI API.\n        polling_interval_seconds: The interval in seconds to poll the batch status.\n        default_gen_kwargs: Default generation kwargs to use when calling the API.\n        developer_message: Instructions to the model that are prioritized ahead of user messages.\n            Previously called the system prompt.\n        string_processors: A single or a list of StringProcessor objects to process the model's output.\n        model_limit_new_tokens: An upper limit on the number of tokens the model can generate.\n            For example, if a too-large `max_new_tokens` is given to generate_chat_response(), this value will cap it.\n        tools: Default tools to use in chat responses when no tools are explicitly provided.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        api_headers: dict[str, str] | None = None,\n        polling_interval_seconds: int = 60,\n        default_gen_kwargs: dict[str, Any] | None = None,\n        developer_message: str | None = None,\n        string_processors: StringProcessor | list[StringProcessor] | None = None,\n        model_limit_new_tokens: int | None = None,\n        tools: list[dict[str, Any]] | None = None,\n    ) -&gt; None:\n        super().__init__(string_processors=string_processors)\n        self.model = model\n        if api_headers is None:\n            api_headers = {}\n        self._client = AsyncOpenAI(**api_headers)\n        self.default_gen_kwargs = default_gen_kwargs or {}\n        # convert the flexeval-specific argument name to the OpenAI-specific name\n        if \"max_new_tokens\" in self.default_gen_kwargs:\n            self.default_gen_kwargs[\"max_completion_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n        self.temp_jsonl_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".jsonl\")\n\n        self.polling_interval_seconds = polling_interval_seconds\n        self.developer_message = developer_message\n        self.model_limit_new_tokens = model_limit_new_tokens\n        self.tools = tools\n\n    def create_batch_file(self, custom_id_2_input: dict[str, list[dict[str, list[dict[str, Any]]]]], **kwargs) -&gt; None:\n        with open(self.temp_jsonl_file.name, mode=\"w\") as f:\n            for custom_id, input_dict in custom_id_2_input.items():\n                messages = input_dict[\"messages\"]\n                tools = input_dict[\"tools\"]\n                if self.developer_message:\n                    messages = [{\"role\": \"developer\", \"content\": self.developer_message}, *messages]\n\n                f.write(\n                    json.dumps(\n                        create_request_details(self.model, custom_id, messages, tools=tools, **kwargs),\n                        ensure_ascii=False,\n                    )\n                    + \"\\n\",\n                )\n\n    async def _post_batch_requests(\n        self,\n        custom_id_2_input: dict[str, list[dict[str, list[dict[str, Any]]]]],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; str:\n        gen_kwargs = self.default_gen_kwargs.copy()\n        gen_kwargs.update(kwargs)\n        \"\"\"Send batch chat requests to the OpenAI.\"\"\"\n\n        if max_new_tokens is not None:\n            if \"max_completion_tokens\" in gen_kwargs:\n                msg = (\n                    \"You specified both `max_new_tokens` and `max_completion_tokens` in generation kwargs. \"\n                    \"Note that `max_new_tokens` overrides `max_completion_tokens` by default. \"\n                    \"It is recommended to specify only one of them to avoid unexpected behavior.\"\n                )\n                logger.warning(msg)\n            gen_kwargs[\"max_completion_tokens\"] = max_new_tokens\n\n        if self.model_limit_new_tokens and (gen_kwargs.get(\"max_completion_tokens\", 0) &gt; self.model_limit_new_tokens):\n            msg = (\n                f\"The specified `max_new_tokens` ({gen_kwargs['max_completion_tokens']}) exceeds\"\n                f\"the model\u2019s capability ({self.model_limit_new_tokens} tokens). It will be reduced.\"\n            )\n            logger.warning(msg)\n            gen_kwargs[\"max_completion_tokens\"] = self.model_limit_new_tokens\n\n        gen_kwargs[\"stop\"] = normalize_stop_sequences(\n            stop_sequences_list=[\n                stop_sequences,\n                gen_kwargs.pop(\"stop\", None),  # This is used in the OpenAI API\n                gen_kwargs.pop(\"stop_sequences\", None),  # This is a common variable name used in flexeval\n            ],\n        )\n\n        self.create_batch_file(custom_id_2_input, **gen_kwargs)\n\n        # Update batch file\n        with open(self.temp_jsonl_file.name, \"rb\") as batch_file:  # noqa: ASYNC101\n            batch_input_file = await self._client.files.create(file=batch_file, purpose=\"batch\")\n\n        # Run Job\n        # Batch Object: https://platform.openai.com/docs/api-reference/batch/object\n        batch_object = await self._client.batches.create(\n            input_file_id=batch_input_file.id,\n            endpoint=\"/v1/chat/completions\",\n            completion_window=\"24h\",\n            metadata={\"description\": \"flexeval job\"},\n        )\n        logger.info(f\"Input File ID: {batch_input_file.id}, Batch ID: {batch_object.id}\")\n        return batch_object.id\n\n    async def poll_batch_status_until_completion(\n        self,\n        batch_id: str,\n        polling_interval_seconds: int,\n    ) -&gt; tuple[Status, Batch]:\n        status = Status.validating\n        while status not in (Status.completed, Status.failed, Status.canceled):\n            await asyncio.sleep(polling_interval_seconds)\n            batch_response = await self._client.batches.retrieve(batch_id)\n            status = Status(batch_response.status)\n            logger.info(f\"Current status: {status.value}\")\n        return status, batch_response\n\n    def _retrieve_file_content(self, file_id: str) -&gt; list[dict[any, any]]:\n        file_response = asyncio.run(self._client.files.content(file_id))\n        return [json.loads(line) for line in file_response.text.strip().split(\"\\n\")]\n\n    def _execute_batch_requests(  # noqa: C901\n        self,\n        messages_list: list[list[dict[str, Any]]],\n        tools_list: list[list[dict[str, Any]] | None] | None = None,\n        **kwargs,\n    ) -&gt; list[Any]:\n        if tools_list is None:\n            tools_list = [self.tools] * len(messages_list)\n        custom_id_2_input: dict[str, list[dict[str, list[dict[str, Any]]]]] = {\n            str(uuid.uuid4()): {\"messages\": messages, \"tools\": tools}\n            for messages, tools in zip(messages_list, tools_list)\n        }\n        # The response will be an empty string if the API produces an error.\n        custom_id_2_response: dict[str, str | list[dict[str, Any]]] = {custom_id: \"\" for custom_id in custom_id_2_input}\n        exec_cnt = 1\n\n        while len(custom_id_2_input) &gt; 0:\n            if exec_cnt &gt; MAX_NUM_TRIALS:\n                break\n            logger.info(f\"Trial {exec_cnt}\")\n            exec_cnt += 1\n            batch_id = asyncio.run(self._post_batch_requests(custom_id_2_input, **kwargs))\n\n            status, batch_response = asyncio.run(\n                self.poll_batch_status_until_completion(batch_id, self.polling_interval_seconds),\n            )\n            if status is not Status.completed:\n                error_message = f\"Failed: {batch_response}\"\n                raise ValueError(error_message)\n\n            # Check error_file_id exists and if exists, log error details.\n            error_file_id = batch_response.error_file_id\n            # If any request fails, error_file_id is set.\n            if error_file_id is not None:\n                logger.warning(\"Request on some messages failed following reason.\")\n                data: list[dict[str, Any]] = self._retrieve_file_content(error_file_id)\n                # [Error](https://github.com/openai/openai-openapi/blob/master/openapi.yaml#L8857])\n                # instance is embedded in response.\n                for data_i in data:\n                    error = data_i[\"response\"]\n                    logger.warning(f\"Failed: {error}\")\n\n            output_file_id = batch_response.output_file_id\n            # If completion on all input fails, output_file_id is None.\n            if output_file_id is None:\n                logger.warning(\"All request failed. Continue...\")\n                continue\n\n            data: list[dict[str, Any]] = self._retrieve_file_content(output_file_id)\n            for data_i in data:\n                if data_i[\"error\"] is not None:\n                    continue\n\n                custom_id = data_i[\"custom_id\"]\n                custom_id_2_input.pop(custom_id)\n\n                custom_id_2_response[custom_id] = data_i[\"response\"][\"body\"]\n\n        # The remaining elements are all those that failed to complete request.\n        if custom_id_2_input:\n            logger.warning(\"The following messages failed to complete request.\")\n            logger.warning(pformat(list(custom_id_2_input.values())))\n\n        return list(custom_id_2_response.values())\n\n    def _batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        messages_list = [[{\"role\": \"user\", \"content\": text}] for text in text_list]\n        api_responses = self._execute_batch_requests(\n            messages_list,\n            stop_sequences=stop_sequences,\n            max_new_tokens=max_new_tokens,\n            **kwargs,\n        )\n        return [\n            LMOutput(text=res[\"choices\"][0][\"message\"][\"content\"], finish_reason=res[\"choices\"][0][\"finish_reason\"])\n            for res in api_responses\n        ]\n\n    def _batch_generate_chat_response(\n        self,\n        chat_messages_list: list[list[dict[str, Any]]],\n        tools_list: list[list[dict[str, Any]] | None] | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        api_responses = self._execute_batch_requests(\n            chat_messages_list,\n            tools_list=tools_list,\n            **kwargs,\n        )\n        return [\n            LMOutput(\n                text=res[\"choices\"][0][\"message\"][\"content\"],\n                finish_reason=res[\"choices\"][0][\"finish_reason\"],\n                tool_calls=res[\"choices\"][0][\"message\"].get(\"tool_calls\", None),\n            )\n            for res in api_responses\n        ]\n\n    def cleanup_resources(self) -&gt; None:\n        # in case that the program fails before the file is initialized in __init__\n        if not hasattr(self, \"temp_jsonl_file\"):\n            return\n\n        try:\n            self.temp_jsonl_file.close()\n            os.unlink(self.temp_jsonl_file.name)  # noqa: PTH108\n            logger.info(f\"Temporary file deleted: {self.temp_jsonl_file.name}\")\n        except OSError as e:\n            logger.error(f\"Error: {e.filename} - {e.strerror}.\")\n\n    def _batch_compute_chat_log_probs(\n        self,\n        prompt_list: list[list[dict[str, Any]]],\n        response_list: list[dict[str, Any]],\n        temperature: float = 0,\n        seed: int = 42,\n        top_logprobs: int = 20,\n    ) -&gt; list[float]:\n        response_contents = [resp[\"content\"] for resp in response_list]\n        for response_content in response_contents:\n            num_tokens = number_of_tokens_in_openai_model(self.model, response_content)\n            if num_tokens &gt; 1:\n                err_msg = f\"OpenAIChatAPI.batch_compute_chat_log_probs is not applicable for two or more tokens of response content: '{response_content}'\"  # noqa: E501\n                raise NotImplementedError(err_msg)\n\n        # For saving cost, remove duplication from message_list for an API request.\n        unique_prompt_list = remove_duplicates_from_prompt_list(prompt_list)\n        api_responses = self._execute_batch_requests(\n            unique_prompt_list,\n            max_new_tokens=1,\n            seed=seed,\n            logprobs=True,\n            top_logprobs=top_logprobs,\n        )\n\n        log_probs = []\n        top_logprobs_list = [res[\"choices\"][0][\"logprobs\"][\"content\"][0][\"top_logprobs\"] for res in api_responses]\n        for index, prompt in enumerate(prompt_list):\n            target_token = response_contents[index]\n            index_in_unique = unique_prompt_list.index(prompt)\n\n            log_prob = None  # if target token not in top_logprobs, return None for log_prob of the token\n            top_logprobs = top_logprobs_list[index_in_unique]\n            for token_logprob in top_logprobs:\n                if token_logprob[\"token\"] == target_token:\n                    log_prob = token_logprob[\"logprob\"]\n                    break\n            log_probs.append(log_prob)\n\n        return log_probs\n\n    def __del__(self) -&gt; None:\n        self.cleanup_resources()\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.default_gen_kwargs","title":"default_gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>default_gen_kwargs = default_gen_kwargs or {}\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.temp_jsonl_file","title":"temp_jsonl_file  <code>instance-attribute</code>","text":"<pre><code>temp_jsonl_file = NamedTemporaryFile(\n    delete=False, suffix=\".jsonl\"\n)\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.polling_interval_seconds","title":"polling_interval_seconds  <code>instance-attribute</code>","text":"<pre><code>polling_interval_seconds = polling_interval_seconds\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.developer_message","title":"developer_message  <code>instance-attribute</code>","text":"<pre><code>developer_message = developer_message\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.model_limit_new_tokens","title":"model_limit_new_tokens  <code>instance-attribute</code>","text":"<pre><code>model_limit_new_tokens = model_limit_new_tokens\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools = tools\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.__init__","title":"__init__","text":"<pre><code>__init__(\n    model: str,\n    api_headers: dict[str, str] | None = None,\n    polling_interval_seconds: int = 60,\n    default_gen_kwargs: dict[str, Any] | None = None,\n    developer_message: str | None = None,\n    string_processors: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    model_limit_new_tokens: int | None = None,\n    tools: list[dict[str, Any]] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    api_headers: dict[str, str] | None = None,\n    polling_interval_seconds: int = 60,\n    default_gen_kwargs: dict[str, Any] | None = None,\n    developer_message: str | None = None,\n    string_processors: StringProcessor | list[StringProcessor] | None = None,\n    model_limit_new_tokens: int | None = None,\n    tools: list[dict[str, Any]] | None = None,\n) -&gt; None:\n    super().__init__(string_processors=string_processors)\n    self.model = model\n    if api_headers is None:\n        api_headers = {}\n    self._client = AsyncOpenAI(**api_headers)\n    self.default_gen_kwargs = default_gen_kwargs or {}\n    # convert the flexeval-specific argument name to the OpenAI-specific name\n    if \"max_new_tokens\" in self.default_gen_kwargs:\n        self.default_gen_kwargs[\"max_completion_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n    self.temp_jsonl_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".jsonl\")\n\n    self.polling_interval_seconds = polling_interval_seconds\n    self.developer_message = developer_message\n    self.model_limit_new_tokens = model_limit_new_tokens\n    self.tools = tools\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.create_batch_file","title":"create_batch_file","text":"<pre><code>create_batch_file(\n    custom_id_2_input: dict[\n        str, list[dict[str, list[dict[str, Any]]]]\n    ],\n    **kwargs,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>def create_batch_file(self, custom_id_2_input: dict[str, list[dict[str, list[dict[str, Any]]]]], **kwargs) -&gt; None:\n    with open(self.temp_jsonl_file.name, mode=\"w\") as f:\n        for custom_id, input_dict in custom_id_2_input.items():\n            messages = input_dict[\"messages\"]\n            tools = input_dict[\"tools\"]\n            if self.developer_message:\n                messages = [{\"role\": \"developer\", \"content\": self.developer_message}, *messages]\n\n            f.write(\n                json.dumps(\n                    create_request_details(self.model, custom_id, messages, tools=tools, **kwargs),\n                    ensure_ascii=False,\n                )\n                + \"\\n\",\n            )\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.poll_batch_status_until_completion","title":"poll_batch_status_until_completion  <code>async</code>","text":"<pre><code>poll_batch_status_until_completion(\n    batch_id: str, polling_interval_seconds: int\n) -&gt; tuple[Status, Batch]\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>async def poll_batch_status_until_completion(\n    self,\n    batch_id: str,\n    polling_interval_seconds: int,\n) -&gt; tuple[Status, Batch]:\n    status = Status.validating\n    while status not in (Status.completed, Status.failed, Status.canceled):\n        await asyncio.sleep(polling_interval_seconds)\n        batch_response = await self._client.batches.retrieve(batch_id)\n        status = Status(batch_response.status)\n        logger.info(f\"Current status: {status.value}\")\n    return status, batch_response\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.cleanup_resources","title":"cleanup_resources","text":"<pre><code>cleanup_resources() -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>def cleanup_resources(self) -&gt; None:\n    # in case that the program fails before the file is initialized in __init__\n    if not hasattr(self, \"temp_jsonl_file\"):\n        return\n\n    try:\n        self.temp_jsonl_file.close()\n        os.unlink(self.temp_jsonl_file.name)  # noqa: PTH108\n        logger.info(f\"Temporary file deleted: {self.temp_jsonl_file.name}\")\n    except OSError as e:\n        logger.error(f\"Error: {e.filename} - {e.strerror}.\")\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.__del__","title":"__del__","text":"<pre><code>__del__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>def __del__(self) -&gt; None:\n    self.cleanup_resources()\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.openai_batch_api.OpenAIChatBatchAPI.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/language_model/openai_batch_api.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM","title":"VLLM","text":"<p>LanguageModel implementation using VLLM.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The name of the model to use.</p> </li> <li> <code>model_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Additional keyword arguments to pass to the model.</p> </li> <li> <code>tokenizer</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The name of the tokenizer to use. Defaults to the model_name.</p> </li> <li> <code>tokenizer_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Keyword arguments for the tokenizer instantiation by `from_pretrained().</p> </li> <li> <code>add_special_tokens</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to add special tokens to the input. Note that whether BOS or EOS tokens are added depends on the tokenizer.</p> </li> <li> <code>custom_chat_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A custom chat template for chatbot models. If specified, this overrides the default chat template of the tokenizer.</p> </li> <li> <code>system_message</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>System messages to be prepended to given messages. It applies only for chat response.</p> </li> <li> <code>default_gen_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default generation kwargs to use when calling the model.</p> </li> <li> <code>string_processors</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>A single or a list of StringProcessor objects to process the model's output.</p> </li> <li> <code>model_limit_tokens</code>               (<code>int | None | Literal['default']</code>, default:                   <code>'default'</code> )           \u2013            <p>An upper limit on the number of tokens (input + output) the model can handle. If <code>max_new_tokens</code> exceeds this limit in <code>generate_chat_response()</code>, it will be capped to this value. If this value is set to less than or equal to the model's capacity and the input exceeds it, an empty string is returned instead of raising an error. If set to \u201cdefault\u201d, the value will be automatically determined when possible.</p> </li> <li> <code>tool_parser</code>               (<code>ToolParser | None</code>, default:                   <code>None</code> )           \u2013            <p>A ToolParser object to extract the tool_calls from the model's output.</p> </li> <li> <code>tools</code>               (<code>list[dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default tools to use in chat responses when no tools are explicitly provided.</p> </li> </ul> Source code in <code>flexeval/core/language_model/vllm_model.py</code> <pre><code>class VLLM(LanguageModel):\n    \"\"\"LanguageModel implementation using VLLM.\n\n    Args:\n        model: The name of the model to use.\n        model_kwargs: Additional keyword arguments to pass to the model.\n        tokenizer: The name of the tokenizer to use. Defaults to the model_name.\n        tokenizer_kwargs: Keyword arguments for the tokenizer instantiation by `from_pretrained().\n        add_special_tokens: Whether to add special tokens to the input.\n            Note that whether BOS or EOS tokens are added depends on the tokenizer.\n        custom_chat_template: A custom chat template for chatbot models.\n            If specified, this overrides the default chat template of the tokenizer.\n        system_message: System messages to be prepended to given messages. It applies only for\n            chat response.\n        default_gen_kwargs: Default generation kwargs to use when calling the model.\n        string_processors: A single or a list of StringProcessor objects to process the model's output.\n        model_limit_tokens: An upper limit on the number of tokens (input + output) the model can handle.\n            If `max_new_tokens` exceeds this limit in `generate_chat_response()`, it will be capped to this value.\n            If this value is set to less than or equal to the model's capacity and the input exceeds it,\n            an empty string is returned instead of raising an error.\n            If set to \u201cdefault\u201d, the value will be automatically determined when possible.\n        tool_parser: A ToolParser object to extract the tool_calls from the model's output.\n        tools: Default tools to use in chat responses when no tools are explicitly provided.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        model_kwargs: dict[str, Any] | None = None,\n        tokenizer: str | None = None,\n        tokenizer_kwargs: dict[str, Any] | None = None,\n        add_special_tokens: bool = False,\n        custom_chat_template: str | None = None,\n        chat_template_kwargs: dict[str, Any] | None = None,\n        system_message: str | None = None,\n        default_gen_kwargs: dict[str, Any] | None = None,\n        string_processors: StringProcessor | list[StringProcessor] | None = None,\n        model_limit_tokens: int | None | Literal[\"default\"] = \"default\",\n        tool_parser: ToolParser | None = None,\n        tools: list[dict[str, Any]] | None = None,\n    ) -&gt; None:\n        super().__init__(string_processors=string_processors)\n        self.model_name = model\n        tokenizer = tokenizer if tokenizer else model\n        tokenizer_kwargs = tokenizer_kwargs or {}\n        self.tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(tokenizer, **tokenizer_kwargs)\n        self.custom_chat_template = custom_chat_template\n        self.chat_template_kwargs = chat_template_kwargs or {}\n        self.system_message = system_message\n        self.add_special_tokens = add_special_tokens\n        # use greedy decoding by default to make it consistent with `HuggingFaceLM`\n        self.default_gen_kwargs = default_gen_kwargs or {\"temperature\": 0.0}\n        # convert the flexeval-specific argument name to the vllm-specific name\n        if \"max_new_tokens\" in self.default_gen_kwargs:\n            self.default_gen_kwargs[\"max_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n\n        model_kwargs = model_kwargs or {}\n        # automatically set tensor_parallel_size to the number of GPUs\n        if \"tensor_parallel_size\" not in model_kwargs:\n            model_kwargs[\"tensor_parallel_size\"] = torch.cuda.device_count()\n        if \"enable_chunked_prefill\" not in model_kwargs:\n            model_kwargs[\"enable_chunked_prefill\"] = True\n            model_kwargs[\"disable_sliding_window\"] = True\n        self.model_kwargs = model_kwargs\n        # `self.llm` is initialized lazily to avoid unnecessary memory usage.\n        self.llm: LLM | None = None\n        self.model_limit_tokens = model_limit_tokens\n        self.tool_parser = tool_parser\n        self.tools = tools\n\n    @staticmethod\n    def load_model(method: Callable) -&gt; Callable:\n        \"\"\"Decorator to load the model lazily.\"\"\"\n\n        def wrapper(self: VLLM, *args: tuple, **kwargs: dict) -&gt; Callable:\n            if self.llm is None:\n                from vllm import LLM\n\n                self.llm = LLM(self.model_name, **self.model_kwargs)\n                if self.model_limit_tokens == \"default\":\n                    self.model_limit_tokens = self.llm.llm_engine.get_model_config().max_model_len\n            return method(self, *args, **kwargs)\n\n        return wrapper\n\n    @load_model\n    def _batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        gen_kwargs = self.default_gen_kwargs.copy()\n        gen_kwargs.update(kwargs)\n        if max_new_tokens is not None:\n            gen_kwargs[\"max_tokens\"] = max_new_tokens\n\n        stop_sequences = normalize_stop_sequences(\n            stop_sequences_list=[\n                stop_sequences,\n                gen_kwargs.pop(\"stop\", None),  # This is used in the vllm `SamplingParams`\n                gen_kwargs.pop(\"stop_sequences\", None),  # This is a common variable name used in flexeval\n            ],\n            bos_token=self.tokenizer.bos_token,\n            eos_token=self.tokenizer.eos_token,\n            ignore_eos=gen_kwargs.get(\"ignore_eos\", False),\n        )\n\n        model_inputs = self.tokenizer(\n            text_list,\n            add_special_tokens=self.add_special_tokens,\n            return_token_type_ids=False,\n        )\n\n        from vllm import RequestOutput, SamplingParams\n\n        prompt_token_ids: list[list[int]] = model_inputs.input_ids\n        sampling_params: list[SamplingParams] = []\n        skip_flag_list: list[bool] = []\n        for i, input_ids in enumerate(model_inputs.input_ids):\n            remaining = self.model_limit_tokens - len(input_ids)\n            instance_gen_kwargs = gen_kwargs.copy()\n            if remaining &lt;= 0:\n                prompt_token_ids[i] = input_ids[:1]\n                instance_gen_kwargs[\"max_tokens\"] = 1\n                msg = (\n                    f\"Received input that is longer than `model_limit_tokens = {self.model_limit_tokens}`. \"\n                    f\"The instane returns empty strings.\"\n                )\n                logger.warning(msg)\n            elif \"max_tokens\" not in gen_kwargs or remaining &lt; gen_kwargs[\"max_tokens\"]:\n                instance_gen_kwargs[\"max_tokens\"] = remaining\n            sampling_params.append(SamplingParams(**instance_gen_kwargs, stop=stop_sequences))\n            skip_flag_list.append(remaining &lt;= 0)\n\n        vllm_outputs: list[RequestOutput] = self.llm.generate(\n            prompt_token_ids=prompt_token_ids,\n            sampling_params=sampling_params,\n            use_tqdm=False,\n        )\n\n        outputs = []\n        for input_token_ids, vllm_output, skip_flag in zip(model_inputs.input_ids, vllm_outputs, skip_flag_list):\n            if skip_flag:\n                # Treat skipped instances as if they generated an empty string.\n                decoded_text = \"\"\n                finish_reason = \"input_length_limit\"\n            else:\n                output_token_ids = list(vllm_output.outputs[0].token_ids)\n                decoded_text = decode_for_lm_continuation(output_token_ids, input_token_ids, self.tokenizer)\n                finish_reason = \"length\"\n            # We manually remove the stop sequences from the generated texts.\n            for stop in stop_sequences:\n                stop_index = decoded_text.find(stop)\n                if stop_index != -1:\n                    decoded_text = decoded_text[:stop_index]\n                    finish_reason = \"stop\"\n\n            outputs.append(LMOutput(text=decoded_text, finish_reason=finish_reason))\n        return outputs\n\n    @load_model\n    def _batch_generate_chat_response(\n        self,\n        chat_messages_list: list[list[dict[str, Any]]],\n        tools_list: list[list[dict[str, Any]] | None] | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        if tools_list is None:\n            tools_list = [self.tools] * len(chat_messages_list)\n        if self.system_message is not None:\n            for chat_messages in chat_messages_list:\n                chat_messages.insert(0, {\"role\": \"system\", \"content\": self.system_message})\n        chat_messages_as_string = [\n            self.tokenizer.apply_chat_template(\n                deserialize_tool_calls_in_messages(chat_messages),\n                tools=tools,\n                tokenize=False,\n                add_generation_prompt=True,\n                chat_template=self.custom_chat_template,\n                **self.chat_template_kwargs,\n            )\n            for chat_messages, tools in zip(chat_messages_list, tools_list)\n        ]\n        lm_outputs = self._batch_complete_text(chat_messages_as_string, **kwargs)\n        if self.tool_parser:\n            for lm_output, tools in zip(lm_outputs, tools_list):\n                if tools is None:\n                    continue\n                parsed_tool_calling_message = self.tool_parser(lm_output.text)\n                lm_output.tool_calls = parsed_tool_calling_message.tool_call_dicts\n                lm_output.raw_text = parsed_tool_calling_message.raw_text\n                lm_output.text = parsed_tool_calling_message.text\n                lm_output.tool_call_validation_result = parsed_tool_calling_message.validation_result\n\n        return lm_outputs\n\n    @load_model\n    def _batch_compute_log_probs(\n        self, text_list: list[str], prefix_list: list[str] | None = None, stride: int | None = None\n    ) -&gt; list[float]:\n        batch_size = len(text_list)\n\n        # prepare prefix encoding\n        prefix_list = prefix_list if prefix_list else [\"\" for _ in range(batch_size)]\n        # If the prefix is an empty string, replace it with the bos token regardless of the model being trained with it.\n        # This is needed to correctly calculate the log probabilities of the first token.\n        for i in range(batch_size):\n            if prefix_list[i] == \"\":\n                prefix_list[i] = self.tokenizer.bos_token\n\n        batch_prefix_ids = tokenize_text_for_lm_prefix(\n            prefix_list,\n            self.tokenizer,\n            add_special_tokens=self.add_special_tokens,\n        )\n\n        # prepare continuation encoding\n        # If the last token is a special token, it is treated as a beginning of a new sentence.\n        batch_continuation_ids = tokenize_text_for_lm_continuation(\n            text_list,\n            self.tokenizer,\n            as_continuation=[prefix_ids[-1] not in self.tokenizer.all_special_ids for prefix_ids in batch_prefix_ids],\n        )\n\n        batch_input_ids = [\n            prefix + continuation for prefix, continuation in zip(batch_prefix_ids, batch_continuation_ids)\n        ]\n\n        max_length = self.llm.llm_engine.get_model_config().max_seq_len_to_capture\n        stride = stride or max_length // 2\n        if not (0 &lt; stride &lt; max_length):\n            msg = f\"stride must be in (0, {max_length}), but got {stride}\"\n            raise ValueError(msg)\n        sequence_length = max([len(input_ids) for input_ids in batch_input_ids])\n\n        from vllm import RequestOutput, SamplingParams\n        from vllm.sequence import Logprob\n\n        sampling_params = SamplingParams(temperature=0.0, max_tokens=1, prompt_logprobs=1)\n\n        batch_logprobs = [0.0] * batch_size\n        last_computed_index = 0\n        for chunk_start in range(0, sequence_length, stride):\n            chunk_end = min(chunk_start + max_length, sequence_length)\n            chunk_batch_input_ids = [input_ids[chunk_start:chunk_end] for input_ids in batch_input_ids]\n            chunk_batch_input_ids = [\n                [self.tokenizer.bos_token_id] if len(chunk_input_ids) == 0 else chunk_input_ids\n                for chunk_input_ids in chunk_batch_input_ids\n            ]\n            chunk_batch_outputs: list[RequestOutput] = self.llm.generate(\n                prompt_token_ids=chunk_batch_input_ids,\n                sampling_params=sampling_params,\n                use_tqdm=False,\n            )\n\n            i = 0\n            for ids, output, prefix_ids in zip(chunk_batch_input_ids, chunk_batch_outputs, batch_prefix_ids):\n                chunk_rest_prefix_length = max(len(prefix_ids) - last_computed_index, 0)\n                chunk_continuation_start = last_computed_index - chunk_start + chunk_rest_prefix_length\n\n                # `prompt_logprobs` has the same length as the input `ids`.\n                # The i-th element contains the log probabilities of the i-th token in `ids`\n                # and the highest-likelihood token at that position.\n                # The 0-th element is always `None` because the log probability cannot be computed for it.\n                prompt_logprobs: list[dict[int, Logprob] | None] = output.prompt_logprobs\n                all_token_logprobs = [\n                    cands[token_id].logprob if cands else 0.0 for cands, token_id in zip(prompt_logprobs, ids)\n                ]\n                continuation_logprob = float(sum(all_token_logprobs[chunk_continuation_start:]))\n                batch_logprobs[i] += continuation_logprob\n                i += 1\n\n            last_computed_index = chunk_end\n\n        return batch_logprobs\n\n    @load_model\n    def _batch_compute_chat_log_probs(\n        self, prompt_list: list[list[dict[str, Any]]], response_list: list[dict[str, Any]]\n    ) -&gt; list[float]:\n        prompt_as_string: list[str] = []\n        response_as_string: list[str] = []\n        for prompt, response in zip(prompt_list, response_list):\n            prompt_as_string_i, response_as_string_i = get_prefix_and_completion_from_chat(\n                prompt,\n                response,\n                self.tokenizer,\n                custom_chat_template=self.custom_chat_template,\n            )\n            prompt_as_string.append(prompt_as_string_i)\n            response_as_string.append(response_as_string_i)\n        return self._batch_compute_log_probs(response_as_string, prefix_list=prompt_as_string)\n\n    def cleanup_resources(self) -&gt; None:\n        from vllm.distributed import cleanup_dist_env_and_memory\n\n        del self.llm\n        logger.info(\"cleaning up vLLM resources...\")\n        cleanup_dist_env_and_memory()\n        time.sleep(10)  # wait for the vLLM server to release resources\n        self.llm = None\n\n    def __repr__(self) -&gt; str:\n        return f\"VLLM(model_name={self.model_name})\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name = model\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer: PreTrainedTokenizer = from_pretrained(\n    tokenizer, **tokenizer_kwargs\n)\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.custom_chat_template","title":"custom_chat_template  <code>instance-attribute</code>","text":"<pre><code>custom_chat_template = custom_chat_template\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.chat_template_kwargs","title":"chat_template_kwargs  <code>instance-attribute</code>","text":"<pre><code>chat_template_kwargs = chat_template_kwargs or {}\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.system_message","title":"system_message  <code>instance-attribute</code>","text":"<pre><code>system_message = system_message\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.add_special_tokens","title":"add_special_tokens  <code>instance-attribute</code>","text":"<pre><code>add_special_tokens = add_special_tokens\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.default_gen_kwargs","title":"default_gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>default_gen_kwargs = default_gen_kwargs or {\n    \"temperature\": 0.0\n}\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.model_kwargs","title":"model_kwargs  <code>instance-attribute</code>","text":"<pre><code>model_kwargs = model_kwargs\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.llm","title":"llm  <code>instance-attribute</code>","text":"<pre><code>llm: LLM | None = None\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.model_limit_tokens","title":"model_limit_tokens  <code>instance-attribute</code>","text":"<pre><code>model_limit_tokens = model_limit_tokens\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.tool_parser","title":"tool_parser  <code>instance-attribute</code>","text":"<pre><code>tool_parser = tool_parser\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.tools","title":"tools  <code>instance-attribute</code>","text":"<pre><code>tools = tools\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.__init__","title":"__init__","text":"<pre><code>__init__(\n    model: str,\n    model_kwargs: dict[str, Any] | None = None,\n    tokenizer: str | None = None,\n    tokenizer_kwargs: dict[str, Any] | None = None,\n    add_special_tokens: bool = False,\n    custom_chat_template: str | None = None,\n    chat_template_kwargs: dict[str, Any] | None = None,\n    system_message: str | None = None,\n    default_gen_kwargs: dict[str, Any] | None = None,\n    string_processors: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    model_limit_tokens: int\n    | None\n    | Literal[\"default\"] = \"default\",\n    tool_parser: ToolParser | None = None,\n    tools: list[dict[str, Any]] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/vllm_model.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    model_kwargs: dict[str, Any] | None = None,\n    tokenizer: str | None = None,\n    tokenizer_kwargs: dict[str, Any] | None = None,\n    add_special_tokens: bool = False,\n    custom_chat_template: str | None = None,\n    chat_template_kwargs: dict[str, Any] | None = None,\n    system_message: str | None = None,\n    default_gen_kwargs: dict[str, Any] | None = None,\n    string_processors: StringProcessor | list[StringProcessor] | None = None,\n    model_limit_tokens: int | None | Literal[\"default\"] = \"default\",\n    tool_parser: ToolParser | None = None,\n    tools: list[dict[str, Any]] | None = None,\n) -&gt; None:\n    super().__init__(string_processors=string_processors)\n    self.model_name = model\n    tokenizer = tokenizer if tokenizer else model\n    tokenizer_kwargs = tokenizer_kwargs or {}\n    self.tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(tokenizer, **tokenizer_kwargs)\n    self.custom_chat_template = custom_chat_template\n    self.chat_template_kwargs = chat_template_kwargs or {}\n    self.system_message = system_message\n    self.add_special_tokens = add_special_tokens\n    # use greedy decoding by default to make it consistent with `HuggingFaceLM`\n    self.default_gen_kwargs = default_gen_kwargs or {\"temperature\": 0.0}\n    # convert the flexeval-specific argument name to the vllm-specific name\n    if \"max_new_tokens\" in self.default_gen_kwargs:\n        self.default_gen_kwargs[\"max_tokens\"] = self.default_gen_kwargs.pop(\"max_new_tokens\")\n\n    model_kwargs = model_kwargs or {}\n    # automatically set tensor_parallel_size to the number of GPUs\n    if \"tensor_parallel_size\" not in model_kwargs:\n        model_kwargs[\"tensor_parallel_size\"] = torch.cuda.device_count()\n    if \"enable_chunked_prefill\" not in model_kwargs:\n        model_kwargs[\"enable_chunked_prefill\"] = True\n        model_kwargs[\"disable_sliding_window\"] = True\n    self.model_kwargs = model_kwargs\n    # `self.llm` is initialized lazily to avoid unnecessary memory usage.\n    self.llm: LLM | None = None\n    self.model_limit_tokens = model_limit_tokens\n    self.tool_parser = tool_parser\n    self.tools = tools\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.load_model","title":"load_model  <code>staticmethod</code>","text":"<pre><code>load_model(method: Callable) -&gt; Callable\n</code></pre> <p>Decorator to load the model lazily.</p> Source code in <code>flexeval/core/language_model/vllm_model.py</code> <pre><code>@staticmethod\ndef load_model(method: Callable) -&gt; Callable:\n    \"\"\"Decorator to load the model lazily.\"\"\"\n\n    def wrapper(self: VLLM, *args: tuple, **kwargs: dict) -&gt; Callable:\n        if self.llm is None:\n            from vllm import LLM\n\n            self.llm = LLM(self.model_name, **self.model_kwargs)\n            if self.model_limit_tokens == \"default\":\n                self.model_limit_tokens = self.llm.llm_engine.get_model_config().max_model_len\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.cleanup_resources","title":"cleanup_resources","text":"<pre><code>cleanup_resources() -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/vllm_model.py</code> <pre><code>def cleanup_resources(self) -&gt; None:\n    from vllm.distributed import cleanup_dist_env_and_memory\n\n    del self.llm\n    logger.info(\"cleaning up vLLM resources...\")\n    cleanup_dist_env_and_memory()\n    time.sleep(10)  # wait for the vLLM server to release resources\n    self.llm = None\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_model.VLLM.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/language_model/vllm_model.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"VLLM(model_name={self.model_name})\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_serve_lm.VLLMServeLM","title":"VLLMServeLM","text":"<p>The LanguageModel class that uses vLLM via <code>vllm serve</code>. This class starts a OpenAI-compatible vLLM API server in the background and communicates with it via HTTP.</p> <p>We also have the <code>VLLM</code> class that uses the vLLM Python API, but there are slight differences in the features available to each other. Using this class, for example, you can take advantage of vLLM's tool-calling support through options such as <code>--tool-call-parser</code> and <code>--enable-auto-tool-choice</code>. https://docs.vllm.ai/en/stable/features/tool_calling.html</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>)           \u2013            <p>The name or path of the model to serve.</p> </li> <li> <code>api_headers</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of headers to use when making requests to the OpenAI API.</p> </li> <li> <code>model_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Additional keyword arguments to pass as command-line options to <code>vllm serve</code>. Each key-value pair is converted to a corresponding CLI argument. (e.g. <code>{\"tool_call_parser\": \"hermes\", \"enable_auto_tool_choice\": True}</code>). Do not include the prefix \"--\". See also: https://docs.vllm.ai/en/latest/cli/index.html#options</p> </li> <li> <code>booting_timeout</code>               (<code>int</code>, default:                   <code>3600</code> )           \u2013            <p>Maximum time in seconds to wait for the server to become available.</p> </li> <li> <code>default_gen_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default generation kwargs to use when calling the API.</p> </li> <li> <code>developer_message</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Instructions to the model that are prioritized ahead of user messages. Previously called the system prompt.</p> </li> <li> <code>string_processors</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>A single or a list of StringProcessor objects to process the model's output.</p> </li> <li> <code>model_limit_new_tokens</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>An upper limit on the number of tokens the model can generate. For example, if a too-large <code>max_new_tokens</code> is given to generate_chat_response(), this value will cap it.</p> </li> <li> <code>tools</code>               (<code>list[dict[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Default tools to use in chat responses when no tools are explicitly provided.</p> </li> </ul> Source code in <code>flexeval/core/language_model/vllm_serve_lm.py</code> <pre><code>class VLLMServeLM(OpenAIChatAPI):\n    \"\"\"\n    The LanguageModel class that uses vLLM via `vllm serve`.\n    This class starts a OpenAI-compatible vLLM API server in the background and communicates with it via HTTP.\n\n    We also have the `VLLM` class that uses the vLLM Python API,\n    but there are slight differences in the features available to each other.\n    Using this class, for example, you can take advantage of vLLM's tool-calling support\n    through options such as `--tool-call-parser` and `--enable-auto-tool-choice`.\n    https://docs.vllm.ai/en/stable/features/tool_calling.html\n\n    Args:\n        model: The name or path of the model to serve.\n        api_headers: A dictionary of headers to use when making requests to the OpenAI API.\n        model_kwargs: Additional keyword arguments to pass as command-line options to `vllm serve`.\n            Each key-value pair is converted to a corresponding CLI argument.\n            (e.g. `{\"tool_call_parser\": \"hermes\", \"enable_auto_tool_choice\": True}`).\n            Do not include the prefix \"--\".\n            See also: https://docs.vllm.ai/en/latest/cli/index.html#options\n        booting_timeout: Maximum time in seconds to wait for the server to become available.\n        default_gen_kwargs: Default generation kwargs to use when calling the API.\n        developer_message: Instructions to the model that are prioritized ahead of user messages.\n            Previously called the system prompt.\n        string_processors: A single or a list of StringProcessor objects to process the model's output.\n        model_limit_new_tokens: An upper limit on the number of tokens the model can generate.\n            For example, if a too-large `max_new_tokens` is given to generate_chat_response(), this value will cap it.\n        tools: Default tools to use in chat responses when no tools are explicitly provided.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        api_headers: dict[str, str] | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n        booting_timeout: int = 3600,\n        default_gen_kwargs: dict[str, Any] | None = None,\n        developer_message: str | None = None,\n        string_processors: StringProcessor | list[StringProcessor] | None = None,\n        model_limit_new_tokens: int | None = None,\n        tools: list[dict[str, Any]] | None = None,\n    ) -&gt; None:\n        logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n        logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n        model_kwargs = model_kwargs or {}\n        if \"tensor_parallel_size\" not in model_kwargs:\n            model_kwargs[\"tensor_parallel_size\"] = torch.cuda.device_count()\n        self.manager = VLLMServerManager(model=model, model_kwargs=model_kwargs, timeout=booting_timeout)\n        if api_headers is None:\n            api_headers = {}\n        api_headers[\"base_url\"] = self.manager.base_url\n        super().__init__(\n            model=model,\n            api_headers=api_headers,\n            default_gen_kwargs=default_gen_kwargs,\n            developer_message=developer_message,\n            string_processors=string_processors,\n            model_limit_new_tokens=model_limit_new_tokens,\n            tools=tools,\n        )\n\n    @staticmethod\n    def load_model(method: Callable) -&gt; Callable:\n        \"\"\"Decorator to load the model lazily.\"\"\"\n\n        def wrapper(self: VLLMServeLM, *args: tuple, **kwargs: dict) -&gt; Callable:\n            if not self.manager.is_ready():\n                self.manager.start()\n            return method(self, *args, **kwargs)\n\n        return wrapper\n\n    @load_model\n    def _batch_complete_text(\n        self,\n        text_list: list[str],\n        stop_sequences: str | list[str] | None = None,\n        max_new_tokens: int | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        return super()._batch_complete_text(text_list, stop_sequences, max_new_tokens, **kwargs)\n\n    @load_model\n    def _batch_generate_chat_response(\n        self,\n        chat_messages_list: list[list[dict[str, Any]]],\n        tools_list: list[list[dict[str, Any]] | None] | None = None,\n        **kwargs,\n    ) -&gt; list[LMOutput]:\n        return super()._batch_generate_chat_response(\n            chat_messages_list,\n            tools_list=tools_list,\n            **kwargs,\n        )\n\n    def _batch_compute_chat_log_probs(\n        self,\n        prompt_list: list[list[dict[str, Any]]],\n        response_list: list[dict[str, Any]],\n        temperature: float = 0,\n        seed: int = 42,\n        top_logprobs: int = 20,\n    ) -&gt; list[float | None]:\n        raise NotImplementedError\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(model={self.model})\"\n\n    def cleanup_resources(self) -&gt; None:\n        if hasattr(self, \"manager\"):\n            self.manager.stop()\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_serve_lm.VLLMServeLM.manager","title":"manager  <code>instance-attribute</code>","text":"<pre><code>manager = VLLMServerManager(\n    model=model,\n    model_kwargs=model_kwargs,\n    timeout=booting_timeout,\n)\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_serve_lm.VLLMServeLM.__init__","title":"__init__","text":"<pre><code>__init__(\n    model: str,\n    api_headers: dict[str, str] | None = None,\n    model_kwargs: dict[str, Any] | None = None,\n    booting_timeout: int = 3600,\n    default_gen_kwargs: dict[str, Any] | None = None,\n    developer_message: str | None = None,\n    string_processors: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    model_limit_new_tokens: int | None = None,\n    tools: list[dict[str, Any]] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/vllm_serve_lm.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    api_headers: dict[str, str] | None = None,\n    model_kwargs: dict[str, Any] | None = None,\n    booting_timeout: int = 3600,\n    default_gen_kwargs: dict[str, Any] | None = None,\n    developer_message: str | None = None,\n    string_processors: StringProcessor | list[StringProcessor] | None = None,\n    model_limit_new_tokens: int | None = None,\n    tools: list[dict[str, Any]] | None = None,\n) -&gt; None:\n    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n    logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n    model_kwargs = model_kwargs or {}\n    if \"tensor_parallel_size\" not in model_kwargs:\n        model_kwargs[\"tensor_parallel_size\"] = torch.cuda.device_count()\n    self.manager = VLLMServerManager(model=model, model_kwargs=model_kwargs, timeout=booting_timeout)\n    if api_headers is None:\n        api_headers = {}\n    api_headers[\"base_url\"] = self.manager.base_url\n    super().__init__(\n        model=model,\n        api_headers=api_headers,\n        default_gen_kwargs=default_gen_kwargs,\n        developer_message=developer_message,\n        string_processors=string_processors,\n        model_limit_new_tokens=model_limit_new_tokens,\n        tools=tools,\n    )\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_serve_lm.VLLMServeLM.load_model","title":"load_model  <code>staticmethod</code>","text":"<pre><code>load_model(method: Callable) -&gt; Callable\n</code></pre> <p>Decorator to load the model lazily.</p> Source code in <code>flexeval/core/language_model/vllm_serve_lm.py</code> <pre><code>@staticmethod\ndef load_model(method: Callable) -&gt; Callable:\n    \"\"\"Decorator to load the model lazily.\"\"\"\n\n    def wrapper(self: VLLMServeLM, *args: tuple, **kwargs: dict) -&gt; Callable:\n        if not self.manager.is_ready():\n            self.manager.start()\n        return method(self, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_serve_lm.VLLMServeLM.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/language_model/vllm_serve_lm.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(model={self.model})\"\n</code></pre>"},{"location":"api_reference/LMOutput/#flexeval.core.language_model.vllm_serve_lm.VLLMServeLM.cleanup_resources","title":"cleanup_resources","text":"<pre><code>cleanup_resources() -&gt; None\n</code></pre> Source code in <code>flexeval/core/language_model/vllm_serve_lm.py</code> <pre><code>def cleanup_resources(self) -&gt; None:\n    if hasattr(self, \"manager\"):\n        self.manager.stop()\n</code></pre>"},{"location":"api_reference/MatchMaker/","title":"MatchMaker","text":""},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.base.MatchMaker","title":"MatchMaker","text":"<p>Generate matches between items from different models.</p> <p>The output is instances of the <code>Match</code> class.</p> Source code in <code>flexeval/core/pairwise_comparison/match_maker/base.py</code> <pre><code>class MatchMaker(ABC):\n    \"\"\"Generate matches between items from different models.\n\n    The output is instances of the `Match` class.\n    \"\"\"\n\n    @abstractmethod\n    def generate_matches(\n        self,\n        model_items: dict[str, list[T]],\n        cached_matches: list[Match] | None = None,\n    ) -&gt; Iterable[Match]:\n        pass\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.base.MatchMaker.generate_matches","title":"generate_matches  <code>abstractmethod</code>","text":"<pre><code>generate_matches(\n    model_items: dict[str, list[T]],\n    cached_matches: list[Match] | None = None,\n) -&gt; Iterable[Match]\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/match_maker/base.py</code> <pre><code>@abstractmethod\ndef generate_matches(\n    self,\n    model_items: dict[str, list[T]],\n    cached_matches: list[Match] | None = None,\n) -&gt; Iterable[Match]:\n    pass\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.all_combinations.AllCombinations","title":"AllCombinations","text":"Source code in <code>flexeval/core/pairwise_comparison/match_maker/all_combinations.py</code> <pre><code>class AllCombinations(MatchMaker):\n    def __init__(self, include_reversed: bool = True) -&gt; None:\n        self.include_reversed = include_reversed\n\n    def generate_matches(self, model_items: dict[str, list[T]], cached_matches: list[Match] | None) -&gt; Iterable[Match]:\n        model_names = sorted(model_items.keys())\n        all_combinations = list(itertools.combinations(model_names, 2))\n\n        cached_matches = cached_matches or []\n        cache_dict = {match.get_key_for_cache(): match for match in cached_matches}\n\n        if self.include_reversed:\n            all_combinations += [(m2, m1) for m1, m2 in all_combinations]\n\n        for m1, m2 in all_combinations:\n            for item1, item2 in zip(model_items[m1], model_items[m2]):\n                match = Match(m1, item1, m2, item2)\n                if cached_match := cache_dict.get(match.get_key_for_cache()):\n                    yield cached_match\n                else:\n                    yield match\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.all_combinations.AllCombinations.include_reversed","title":"include_reversed  <code>instance-attribute</code>","text":"<pre><code>include_reversed = include_reversed\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.all_combinations.AllCombinations.__init__","title":"__init__","text":"<pre><code>__init__(include_reversed: bool = True) -&gt; None\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/match_maker/all_combinations.py</code> <pre><code>def __init__(self, include_reversed: bool = True) -&gt; None:\n    self.include_reversed = include_reversed\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.all_combinations.AllCombinations.generate_matches","title":"generate_matches","text":"<pre><code>generate_matches(\n    model_items: dict[str, list[T]],\n    cached_matches: list[Match] | None,\n) -&gt; Iterable[Match]\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/match_maker/all_combinations.py</code> <pre><code>def generate_matches(self, model_items: dict[str, list[T]], cached_matches: list[Match] | None) -&gt; Iterable[Match]:\n    model_names = sorted(model_items.keys())\n    all_combinations = list(itertools.combinations(model_names, 2))\n\n    cached_matches = cached_matches or []\n    cache_dict = {match.get_key_for_cache(): match for match in cached_matches}\n\n    if self.include_reversed:\n        all_combinations += [(m2, m1) for m1, m2 in all_combinations]\n\n    for m1, m2 in all_combinations:\n        for item1, item2 in zip(model_items[m1], model_items[m2]):\n            match = Match(m1, item1, m2, item2)\n            if cached_match := cache_dict.get(match.get_key_for_cache()):\n                yield cached_match\n            else:\n                yield match\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.random_combinations.RandomCombinations","title":"RandomCombinations","text":"Source code in <code>flexeval/core/pairwise_comparison/match_maker/random_combinations.py</code> <pre><code>class RandomCombinations(MatchMaker):\n    def __init__(self, n: int = 100, incremental: bool = False, seed: int = 42) -&gt; None:\n        self.n = n\n        self.incremental = incremental\n        self.seed = seed\n\n    def generate_matches(\n        self,\n        model_items: dict[str, list[T]],\n        cached_matches: list[Match] | None = None,\n    ) -&gt; Iterable[Match]:\n        model_names = sorted(model_items.keys())\n        all_permutations = list(itertools.permutations(model_names, 2))\n\n        cached_matches = cached_matches or []\n        cache_dict = {match.get_key_for_cache(): match for match in cached_matches}\n        model_match_counter: dict[str, int] = {name: 0 for name in model_names}\n        possible_new_matches: list[Match] = []\n        matches: list[Match] = []\n        for m1, m2 in all_permutations:\n            for item1, item2 in zip(model_items[m1], model_items[m2]):\n                match = Match(m1, item1, m2, item2)\n                if cached_match := cache_dict.get(match.get_key_for_cache()):\n                    matches.append(cached_match)\n                    model_match_counter[m1] += 1\n                    model_match_counter[m2] += 1\n                else:\n                    possible_new_matches.append(Match(m1, item1, m2, item2))\n\n        # If `self.incremental` is `True`, add n more matches in addition to the cached data.\n        max_matches = self.n + len(matches) if self.incremental else self.n\n\n        random.seed(self.seed)\n\n        # For each iteration, assign the model with the fewest matches to a new match.\n        while (len(matches) &lt; max_matches) and (len(possible_new_matches) &gt; 0):\n            target_model = min(model_match_counter, key=model_match_counter.get)\n            candidate_matches = [\n                (i, match)\n                for i, match in enumerate(possible_new_matches)\n                if target_model in (match.model1, match.model2)\n            ]\n            index, selected_match = random.choice(candidate_matches)\n            matches.append(selected_match)\n            del possible_new_matches[index]\n            model_match_counter[selected_match.model1] += 1\n            model_match_counter[selected_match.model2] += 1\n\n        for match in matches:\n            yield match\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.random_combinations.RandomCombinations.n","title":"n  <code>instance-attribute</code>","text":"<pre><code>n = n\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.random_combinations.RandomCombinations.incremental","title":"incremental  <code>instance-attribute</code>","text":"<pre><code>incremental = incremental\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.random_combinations.RandomCombinations.seed","title":"seed  <code>instance-attribute</code>","text":"<pre><code>seed = seed\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.random_combinations.RandomCombinations.__init__","title":"__init__","text":"<pre><code>__init__(\n    n: int = 100, incremental: bool = False, seed: int = 42\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/match_maker/random_combinations.py</code> <pre><code>def __init__(self, n: int = 100, incremental: bool = False, seed: int = 42) -&gt; None:\n    self.n = n\n    self.incremental = incremental\n    self.seed = seed\n</code></pre>"},{"location":"api_reference/MatchMaker/#flexeval.core.pairwise_comparison.match_maker.random_combinations.RandomCombinations.generate_matches","title":"generate_matches","text":"<pre><code>generate_matches(\n    model_items: dict[str, list[T]],\n    cached_matches: list[Match] | None = None,\n) -&gt; Iterable[Match]\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/match_maker/random_combinations.py</code> <pre><code>def generate_matches(\n    self,\n    model_items: dict[str, list[T]],\n    cached_matches: list[Match] | None = None,\n) -&gt; Iterable[Match]:\n    model_names = sorted(model_items.keys())\n    all_permutations = list(itertools.permutations(model_names, 2))\n\n    cached_matches = cached_matches or []\n    cache_dict = {match.get_key_for_cache(): match for match in cached_matches}\n    model_match_counter: dict[str, int] = {name: 0 for name in model_names}\n    possible_new_matches: list[Match] = []\n    matches: list[Match] = []\n    for m1, m2 in all_permutations:\n        for item1, item2 in zip(model_items[m1], model_items[m2]):\n            match = Match(m1, item1, m2, item2)\n            if cached_match := cache_dict.get(match.get_key_for_cache()):\n                matches.append(cached_match)\n                model_match_counter[m1] += 1\n                model_match_counter[m2] += 1\n            else:\n                possible_new_matches.append(Match(m1, item1, m2, item2))\n\n    # If `self.incremental` is `True`, add n more matches in addition to the cached data.\n    max_matches = self.n + len(matches) if self.incremental else self.n\n\n    random.seed(self.seed)\n\n    # For each iteration, assign the model with the fewest matches to a new match.\n    while (len(matches) &lt; max_matches) and (len(possible_new_matches) &gt; 0):\n        target_model = min(model_match_counter, key=model_match_counter.get)\n        candidate_matches = [\n            (i, match)\n            for i, match in enumerate(possible_new_matches)\n            if target_model in (match.model1, match.model2)\n        ]\n        index, selected_match = random.choice(candidate_matches)\n        matches.append(selected_match)\n        del possible_new_matches[index]\n        model_match_counter[selected_match.model1] += 1\n        model_match_counter[selected_match.model2] += 1\n\n    for match in matches:\n        yield match\n</code></pre>"},{"location":"api_reference/Metric/","title":"Metric","text":""},{"location":"api_reference/Metric/#flexeval.core.metric.base.Metric","title":"Metric","text":"<p>Base class for metrics.</p> <p>Subclasses must implement the <code>evaluate</code> method to perform metric computation. Use utility functions from <code>flexeval.core.metric.utils</code> for common patterns like string processing and category-wise aggregation.</p> Source code in <code>flexeval/core/metric/base.py</code> <pre><code>class Metric(ABC):\n    \"\"\"\n    Base class for metrics.\n\n    Subclasses must implement the `evaluate` method to perform metric computation.\n    Use utility functions from `flexeval.core.metric.utils` for common patterns\n    like string processing and category-wise aggregation.\n    \"\"\"\n\n    @abstractmethod\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        \"\"\"\n        Evaluate the outputs of `LanguageModel` against the references.\n\n        Args:\n            lm_outputs: List of model outputs (strings or LMOutput objects).\n            references_list: List of reference outputs.\n            extra_info_list: List of task inputs and some extra information.\n        \"\"\"\n\n    def cleanup_resources(self) -&gt; None:  # noqa: B027\n        \"\"\"\n        Clean up resources if necessary.\n        This method is called when the metric is no longer needed.\n        \"\"\"\n        pass  # noqa: PIE790\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.base.Metric.evaluate","title":"evaluate  <code>abstractmethod</code>","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> <p>Evaluate the outputs of <code>LanguageModel</code> against the references.</p> <p>Parameters:</p> <ul> <li> <code>lm_outputs</code>               (<code>list[str | LMOutput]</code>)           \u2013            <p>List of model outputs (strings or LMOutput objects).</p> </li> <li> <code>references_list</code>               (<code>list[list[str]]</code>)           \u2013            <p>List of reference outputs.</p> </li> <li> <code>extra_info_list</code>               (<code>list[dict[str, str]] | None</code>, default:                   <code>None</code> )           \u2013            <p>List of task inputs and some extra information.</p> </li> </ul> Source code in <code>flexeval/core/metric/base.py</code> <pre><code>@abstractmethod\ndef evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    \"\"\"\n    Evaluate the outputs of `LanguageModel` against the references.\n\n    Args:\n        lm_outputs: List of model outputs (strings or LMOutput objects).\n        references_list: List of reference outputs.\n        extra_info_list: List of task inputs and some extra information.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.base.Metric.cleanup_resources","title":"cleanup_resources","text":"<pre><code>cleanup_resources() -&gt; None\n</code></pre> <p>Clean up resources if necessary. This method is called when the metric is no longer needed.</p> Source code in <code>flexeval/core/metric/base.py</code> <pre><code>def cleanup_resources(self) -&gt; None:  # noqa: B027\n    \"\"\"\n    Clean up resources if necessary.\n    This method is called when the metric is no longer needed.\n    \"\"\"\n    pass  # noqa: PIE790\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.base.MetricResult","title":"MetricResult  <code>dataclass</code>","text":"<p>A dataclass representing the result of a metric evaluation.</p> Source code in <code>flexeval/core/metric/base.py</code> <pre><code>@dataclass\nclass MetricResult:\n    \"\"\"\n    A dataclass representing the result of a metric evaluation.\n    \"\"\"\n\n    summary: dict[str, Any]\n    \"\"\"\n    Summary containing aggregated metric values.\n    \"\"\"\n    instance_details: list[dict[str, Any]] | None = None\n    \"\"\"\n    A list of evaluate details for each instance.\n    Useful for error analysis.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.base.MetricResult.summary","title":"summary  <code>instance-attribute</code>","text":"<pre><code>summary: dict[str, Any]\n</code></pre> <p>Summary containing aggregated metric values.</p>"},{"location":"api_reference/Metric/#flexeval.core.metric.base.MetricResult.instance_details","title":"instance_details  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>instance_details: list[dict[str, Any]] | None = None\n</code></pre> <p>A list of evaluate details for each instance. Useful for error analysis.</p>"},{"location":"api_reference/Metric/#flexeval.core.metric.base.MetricResult.__init__","title":"__init__","text":"<pre><code>__init__(\n    summary: dict[str, Any],\n    instance_details: list[dict[str, Any]] | None = None,\n) -&gt; None\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.bleu.BLEU","title":"BLEU","text":"<p>An implementation of BLEU. The calculation is based on the sacrebleu library.</p> <p>Parameters:</p> <ul> <li> <code>tokenize_option</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Tokenization option for sacrebleu. If <code>None</code>, sacrebleu will use the default tokenization. For details, see sacreBLEU https://github.com/mjpost/sacrebleu/blob/aa3cc4351af6/sacrebleu/sacrebleu.py#L121-L124</p> </li> <li> <code>lm_output_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>StringProcessor or a list of StringProcessor to be applied to the model outputs before comparison.</p> </li> <li> <code>reference_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>StringProcessor or list of StringProcessor to apply to the references before comparison.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in extra_info.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import BLEU\n&gt;&gt;&gt; bleu = BLEU()\n&gt;&gt;&gt; lm_outputs = [\"I am a student .\", \"I am a teacher .\"]\n&gt;&gt;&gt; references_list = [[\"I am a student .\", \"I am a learner .\"], [\"I am a teacher .\"]]\n&gt;&gt;&gt; result = bleu.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={\n        'bleu_score': 100.0,\n        'bleu_bp': 1.0,\n        'bleu_signature': nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.1},\n        instance_details=[\n            {'bleu_score': 100.0, 'bleu_bp': 1.0},\n            {'bleu_score': 100.0, 'bleu_bp': 1.0}\n        ]\n    )\n</code></pre> Source code in <code>flexeval/core/metric/bleu.py</code> <pre><code>class BLEU(Metric):\n    \"\"\"An implementation of [BLEU](https://aclanthology.org/P02-1040/).\n    The calculation is based on the [sacrebleu](https://github.com/mjpost/sacrebleu) library.\n\n    Args:\n        tokenize_option: Tokenization option for sacrebleu.\n            If `None`, sacrebleu will use the default tokenization.\n            For details, see sacreBLEU\n            https://github.com/mjpost/sacrebleu/blob/aa3cc4351af6/sacrebleu/sacrebleu.py#L121-L124\n        lm_output_processor:\n            StringProcessor or a list of StringProcessor to be applied to the model outputs before comparison.\n        reference_processor: StringProcessor or list of StringProcessor to apply to the references before comparison.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in extra_info.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import BLEU\n        &gt;&gt;&gt; bleu = BLEU()\n        &gt;&gt;&gt; lm_outputs = [\"I am a student .\", \"I am a teacher .\"]\n        &gt;&gt;&gt; references_list = [[\"I am a student .\", \"I am a learner .\"], [\"I am a teacher .\"]]\n        &gt;&gt;&gt; result = bleu.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={\n                'bleu_score': 100.0,\n                'bleu_bp': 1.0,\n                'bleu_signature': nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.4.1},\n                instance_details=[\n                    {'bleu_score': 100.0, 'bleu_bp': 1.0},\n                    {'bleu_score': 100.0, 'bleu_bp': 1.0}\n                ]\n            )\n    \"\"\"\n\n    def __init__(\n        self,\n        tokenize_option: str | None = None,\n        lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n        reference_processor: StringProcessor | list[StringProcessor] | None = None,\n        category_key: str | None = None,\n    ) -&gt; None:\n        self._corpus_bleu = sacrebleu.metrics.BLEU(tokenize=tokenize_option)\n        # For sentence BLEU, we need to set `effective_order=True` as recommended by sacrebleu.\n        self._sentence_bleu = sacrebleu.metrics.BLEU(tokenize=tokenize_option, effective_order=True)\n\n        self.lm_output_processors = lm_output_processor\n        self.reference_processors = reference_processor\n        self.category_key = category_key\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n        references_list = [\n            [apply_string_processors(ref, self.reference_processors) for ref in references]\n            for references in references_list\n        ]\n\n        # Restructure references for sacrebleu format\n        max_num_refs = max(len(refs) for refs in references_list)\n        references_for_sacrebleu: list[list[str]] = []\n        for i in range(max_num_refs):\n            set_of_references: list[str] = []\n            for refs_for_source in references_list:\n                if i &lt; len(refs_for_source):\n                    set_of_references.append(refs_for_source[i])\n                else:\n                    set_of_references.append(\"\")\n            references_for_sacrebleu.append(set_of_references)\n\n        # Compute metrics\n        bleu = self._corpus_bleu.corpus_score([o.strip() for o in lm_outputs], references_for_sacrebleu)\n        sentence_bleu_list = [\n            self._sentence_bleu.sentence_score(o.strip(), refs) for o, refs in zip(lm_outputs, references_list)\n        ]\n\n        summary = {\n            \"bleu_score\": bleu.score,\n            \"bleu_bp\": bleu.bp,\n            \"bleu_signature\": self._corpus_bleu.get_signature(),\n        }\n\n        if self.category_key:\n            categories = [extra_info[self.category_key] for extra_info in extra_info_list]\n            sentence_bleu_score_list = [b.score for b in sentence_bleu_list]\n            category_wise_scores = aggregate_category_wise_scores(sentence_bleu_score_list, categories)\n            for category, category_wise_score in category_wise_scores.items():\n                summary[f\"sentence_bleu_score/{category}\"] = category_wise_score\n\n        return MetricResult(\n            summary,\n            instance_details=[{\"bleu_score\": b.score, \"bleu_bp\": b.bp} for b in sentence_bleu_list],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.bleu.BLEU.lm_output_processors","title":"lm_output_processors  <code>instance-attribute</code>","text":"<pre><code>lm_output_processors = lm_output_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.bleu.BLEU.reference_processors","title":"reference_processors  <code>instance-attribute</code>","text":"<pre><code>reference_processors = reference_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.bleu.BLEU.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.bleu.BLEU.__init__","title":"__init__","text":"<pre><code>__init__(\n    tokenize_option: str | None = None,\n    lm_output_processor: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    reference_processor: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    category_key: str | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/bleu.py</code> <pre><code>def __init__(\n    self,\n    tokenize_option: str | None = None,\n    lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n    reference_processor: StringProcessor | list[StringProcessor] | None = None,\n    category_key: str | None = None,\n) -&gt; None:\n    self._corpus_bleu = sacrebleu.metrics.BLEU(tokenize=tokenize_option)\n    # For sentence BLEU, we need to set `effective_order=True` as recommended by sacrebleu.\n    self._sentence_bleu = sacrebleu.metrics.BLEU(tokenize=tokenize_option, effective_order=True)\n\n    self.lm_output_processors = lm_output_processor\n    self.reference_processors = reference_processor\n    self.category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.bleu.BLEU.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/bleu.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n    references_list = [\n        [apply_string_processors(ref, self.reference_processors) for ref in references]\n        for references in references_list\n    ]\n\n    # Restructure references for sacrebleu format\n    max_num_refs = max(len(refs) for refs in references_list)\n    references_for_sacrebleu: list[list[str]] = []\n    for i in range(max_num_refs):\n        set_of_references: list[str] = []\n        for refs_for_source in references_list:\n            if i &lt; len(refs_for_source):\n                set_of_references.append(refs_for_source[i])\n            else:\n                set_of_references.append(\"\")\n        references_for_sacrebleu.append(set_of_references)\n\n    # Compute metrics\n    bleu = self._corpus_bleu.corpus_score([o.strip() for o in lm_outputs], references_for_sacrebleu)\n    sentence_bleu_list = [\n        self._sentence_bleu.sentence_score(o.strip(), refs) for o, refs in zip(lm_outputs, references_list)\n    ]\n\n    summary = {\n        \"bleu_score\": bleu.score,\n        \"bleu_bp\": bleu.bp,\n        \"bleu_signature\": self._corpus_bleu.get_signature(),\n    }\n\n    if self.category_key:\n        categories = [extra_info[self.category_key] for extra_info in extra_info_list]\n        sentence_bleu_score_list = [b.score for b in sentence_bleu_list]\n        category_wise_scores = aggregate_category_wise_scores(sentence_bleu_score_list, categories)\n        for category, category_wise_score in category_wise_scores.items():\n            summary[f\"sentence_bleu_score/{category}\"] = category_wise_score\n\n    return MetricResult(\n        summary,\n        instance_details=[{\"bleu_score\": b.score, \"bleu_bp\": b.bp} for b in sentence_bleu_list],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.char_f1.CharF1","title":"CharF1","text":"<p>A metric that calculates how many characters in the output string are included in the characters of the expected output. If there are multiple expected outputs, the highest score is adopted.</p> <p>Parameters:</p> <ul> <li> <code>lm_output_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>StringProcessor or list of Normalizers to apply to the model outputs before comparison.</p> </li> <li> <code>reference_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>StringProcessor or list of Normalizers to apply to the references before comparison.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in extra_info.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import CharF1\n&gt;&gt;&gt; char_f1 = CharF1()\n&gt;&gt;&gt; lm_outputs = [\"abcd\", \"efgh\"]\n&gt;&gt;&gt; references_list = [[\"abcd\", \"ABCD\"], [\"efGH\"]]\n&gt;&gt;&gt; result = char_f1.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(summary={'char_f1': 0.75}, instance_details=[{'char_f1': 1.0}, {'char_f1': 0.5}])\n</code></pre> Source code in <code>flexeval/core/metric/char_f1.py</code> <pre><code>class CharF1(Metric):\n    \"\"\"\n    A metric that calculates how many characters in the output string are included\n    in the characters of the expected output.\n    If there are multiple expected outputs, the highest score is adopted.\n\n    Args:\n        lm_output_processor: StringProcessor or list of Normalizers to apply to the model outputs before comparison.\n        reference_processor: StringProcessor or list of Normalizers to apply to the references before comparison.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in extra_info.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import CharF1\n        &gt;&gt;&gt; char_f1 = CharF1()\n        &gt;&gt;&gt; lm_outputs = [\"abcd\", \"efgh\"]\n        &gt;&gt;&gt; references_list = [[\"abcd\", \"ABCD\"], [\"efGH\"]]\n        &gt;&gt;&gt; result = char_f1.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(summary={'char_f1': 0.75}, instance_details=[{'char_f1': 1.0}, {'char_f1': 0.5}])\n    \"\"\"\n\n    def __init__(\n        self,\n        lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n        reference_processor: StringProcessor | list[StringProcessor] | None = None,\n        category_key: str | None = None,\n    ) -&gt; None:\n        self.lm_output_processors = lm_output_processor\n        self.reference_processors = reference_processor\n        self.category_key = category_key\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n        references_list = [\n            [apply_string_processors(ref, self.reference_processors) for ref in references]\n            for references in references_list\n        ]\n\n        # Compute metrics\n        char_f1_scores: list[float] = []\n        for lm_output, expected_output in zip(lm_outputs, references_list):\n            score = max(fuzz.ratio(lm_output, o) for o in expected_output) / 100\n            char_f1_scores.append(score)\n\n        summary = {\"char_f1\": sum(char_f1_scores) / len(char_f1_scores)}\n\n        if self.category_key:\n            categories = [extra_info[self.category_key] for extra_info in extra_info_list]\n            category_wise_scores = aggregate_category_wise_scores(char_f1_scores, categories)\n            for category, category_wise_score in category_wise_scores.items():\n                summary[f\"char_f1/{category}\"] = category_wise_score\n\n        return MetricResult(\n            summary,\n            instance_details=[{\"char_f1\": s} for s in char_f1_scores],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.char_f1.CharF1.lm_output_processors","title":"lm_output_processors  <code>instance-attribute</code>","text":"<pre><code>lm_output_processors = lm_output_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.char_f1.CharF1.reference_processors","title":"reference_processors  <code>instance-attribute</code>","text":"<pre><code>reference_processors = reference_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.char_f1.CharF1.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.char_f1.CharF1.__init__","title":"__init__","text":"<pre><code>__init__(\n    lm_output_processor: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    reference_processor: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    category_key: str | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/char_f1.py</code> <pre><code>def __init__(\n    self,\n    lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n    reference_processor: StringProcessor | list[StringProcessor] | None = None,\n    category_key: str | None = None,\n) -&gt; None:\n    self.lm_output_processors = lm_output_processor\n    self.reference_processors = reference_processor\n    self.category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.char_f1.CharF1.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/char_f1.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n    references_list = [\n        [apply_string_processors(ref, self.reference_processors) for ref in references]\n        for references in references_list\n    ]\n\n    # Compute metrics\n    char_f1_scores: list[float] = []\n    for lm_output, expected_output in zip(lm_outputs, references_list):\n        score = max(fuzz.ratio(lm_output, o) for o in expected_output) / 100\n        char_f1_scores.append(score)\n\n    summary = {\"char_f1\": sum(char_f1_scores) / len(char_f1_scores)}\n\n    if self.category_key:\n        categories = [extra_info[self.category_key] for extra_info in extra_info_list]\n        category_wise_scores = aggregate_category_wise_scores(char_f1_scores, categories)\n        for category, category_wise_score in category_wise_scores.items():\n            summary[f\"char_f1/{category}\"] = category_wise_score\n\n    return MetricResult(\n        summary,\n        instance_details=[{\"char_f1\": s} for s in char_f1_scores],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.code_eval.CodeEval","title":"CodeEval","text":"<p>A metric that evaluates generated code with test cases.</p> <p>Parameters:</p> <ul> <li> <code>code_template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A Jinja2 template string to make the generated code. The template can contain variables from extra_info. If <code>None</code>, the code prompt will be the generated text itself.</p> </li> <li> <code>lm_output_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>String processors applied to model outputs before evaluation.</p> </li> <li> <code>evaluate_module</code>               (<code>str</code>, default:                   <code>'code_eval'</code> )           \u2013            <p>An evaluate module to use.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import CodeEval\n&gt;&gt;&gt; code_eval = CodeEval()\n&gt;&gt;&gt; lm_outputs = [\"def add(a, b):\\n    return a + b\", \"def is_equal(a, b):\\n    return a = b\"]\n&gt;&gt;&gt; references_list = [[\"assert add(1, 2) == 3\"], [\"assert is_equal(1, 2) == False\"]]\n&gt;&gt;&gt; result = code_eval.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'pass@1': 0.5},\n    instance_details=[\n        {'passed': True, 'result': 'passed'},\n        {'passed': False, 'result': 'failed: invalid syntax (&lt;string&gt;, line 2)'}\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/code_eval.py</code> <pre><code>class CodeEval(Metric):\n    \"\"\"\n    A metric that evaluates generated code with test cases.\n\n    Args:\n        code_template: A Jinja2 template string to make the generated code.\n            The template can contain variables from extra_info.\n            If `None`, the code prompt will be the generated text itself.\n        lm_output_processor: String processors applied to model outputs before evaluation.\n        evaluate_module: An evaluate module to use.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import CodeEval\n        &gt;&gt;&gt; code_eval = CodeEval()\n        &gt;&gt;&gt; lm_outputs = [\"def add(a, b):\\\\n    return a + b\", \"def is_equal(a, b):\\\\n    return a = b\"]\n        &gt;&gt;&gt; references_list = [[\"assert add(1, 2) == 3\"], [\"assert is_equal(1, 2) == False\"]]\n        &gt;&gt;&gt; result = code_eval.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'pass@1': 0.5},\n            instance_details=[\n                {'passed': True, 'result': 'passed'},\n                {'passed': False, 'result': 'failed: invalid syntax (&lt;string&gt;, line 2)'}\n            ]\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        code_template: str | None = None,\n        lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n        evaluate_module: str = \"code_eval\",\n    ) -&gt; None:\n        if code_template is None:\n            code_template = \"{{ lm_output }}\"\n\n        self.code_template = JINJA2_ENV.from_string(code_template)\n        self.code_eval = evaluate.load(evaluate_module)\n\n        self.lm_output_processors = lm_output_processor\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if extra_info_list is None:\n            extra_info_list = [{} for _ in lm_outputs]\n\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n\n        # Compute metrics\n        generated_code_list: list[str] = []\n        test_case_list: list[str] = []\n        # in code generation tasks, references_list contains the test cases\n        for lm_output, extra_info, test_cases in zip(\n            lm_outputs,\n            extra_info_list,\n            references_list,\n        ):\n            generated_code = self.code_template.render(lm_output=lm_output, **extra_info)\n            generated_code_list.append(generated_code)\n            test_case_list.append(\"\\n\".join(test_cases))\n        pass_at_k, results = self.code_eval.compute(\n            references=test_case_list,\n            predictions=[[c] for c in generated_code_list],\n            k=[1],\n        )\n\n        # `results` contain the detailed results for each test case\n        # e.g., {0: [(0, {'task_id': 0, 'passed': False, 'result': \"failed\", 'completion_id': 0})]}\n        results: dict[int, list[tuple[int, dict[str, Any]]]]\n\n        instance_details: list[dict[str, Any]] = []\n        for i in range(len(lm_outputs)):\n            first_result = results[i][0]  # we only assume one candidate code per instance, so we take the first result\n            _, detail_result = first_result  # the first element is just the index so we ignore it\n            # remove unnecessary fields to save space\n            detail_result.pop(\"completion_id\")\n            detail_result.pop(\"task_id\")\n            instance_details.append(detail_result)\n\n        return MetricResult(pass_at_k, instance_details=instance_details)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.code_eval.CodeEval.code_template","title":"code_template  <code>instance-attribute</code>","text":"<pre><code>code_template = from_string(code_template)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.code_eval.CodeEval.code_eval","title":"code_eval  <code>instance-attribute</code>","text":"<pre><code>code_eval = load(evaluate_module)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.code_eval.CodeEval.lm_output_processors","title":"lm_output_processors  <code>instance-attribute</code>","text":"<pre><code>lm_output_processors = lm_output_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.code_eval.CodeEval.__init__","title":"__init__","text":"<pre><code>__init__(\n    code_template: str | None = None,\n    lm_output_processor: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    evaluate_module: str = \"code_eval\",\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/code_eval.py</code> <pre><code>def __init__(\n    self,\n    code_template: str | None = None,\n    lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n    evaluate_module: str = \"code_eval\",\n) -&gt; None:\n    if code_template is None:\n        code_template = \"{{ lm_output }}\"\n\n    self.code_template = JINJA2_ENV.from_string(code_template)\n    self.code_eval = evaluate.load(evaluate_module)\n\n    self.lm_output_processors = lm_output_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.code_eval.CodeEval.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/code_eval.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if extra_info_list is None:\n        extra_info_list = [{} for _ in lm_outputs]\n\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n\n    # Compute metrics\n    generated_code_list: list[str] = []\n    test_case_list: list[str] = []\n    # in code generation tasks, references_list contains the test cases\n    for lm_output, extra_info, test_cases in zip(\n        lm_outputs,\n        extra_info_list,\n        references_list,\n    ):\n        generated_code = self.code_template.render(lm_output=lm_output, **extra_info)\n        generated_code_list.append(generated_code)\n        test_case_list.append(\"\\n\".join(test_cases))\n    pass_at_k, results = self.code_eval.compute(\n        references=test_case_list,\n        predictions=[[c] for c in generated_code_list],\n        k=[1],\n    )\n\n    # `results` contain the detailed results for each test case\n    # e.g., {0: [(0, {'task_id': 0, 'passed': False, 'result': \"failed\", 'completion_id': 0})]}\n    results: dict[int, list[tuple[int, dict[str, Any]]]]\n\n    instance_details: list[dict[str, Any]] = []\n    for i in range(len(lm_outputs)):\n        first_result = results[i][0]  # we only assume one candidate code per instance, so we take the first result\n        _, detail_result = first_result  # the first element is just the index so we ignore it\n        # remove unnecessary fields to save space\n        detail_result.pop(\"completion_id\")\n        detail_result.pop(\"task_id\")\n        instance_details.append(detail_result)\n\n    return MetricResult(pass_at_k, instance_details=instance_details)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.common_prefix_length.CommonPrefixLength","title":"CommonPrefixLength","text":"<p>A metric that calculates the length of the longest common prefix between the model output and the reference.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import CommonPrefixLength\n&gt;&gt;&gt; common_prefix_length = CommonPrefixLength()\n&gt;&gt;&gt; lm_outputs = [\"ABCDEFG\"]\n&gt;&gt;&gt; references_list = [[\"ABCdefg\"]]\n&gt;&gt;&gt; result = common_prefix_length.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={\"average_common_prefix_length\": 3.0, \"longest_common_prefix_length\": 3},\n    instance_details=[{\"common_prefix_length\": 3}],\n)\n</code></pre> Source code in <code>flexeval/core/metric/common_prefix_length.py</code> <pre><code>class CommonPrefixLength(Metric):\n    \"\"\"\n    A metric that calculates the length of the longest common prefix between the model output and the reference.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import CommonPrefixLength\n        &gt;&gt;&gt; common_prefix_length = CommonPrefixLength()\n        &gt;&gt;&gt; lm_outputs = [\"ABCDEFG\"]\n        &gt;&gt;&gt; references_list = [[\"ABCdefg\"]]\n        &gt;&gt;&gt; result = common_prefix_length.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={\"average_common_prefix_length\": 3.0, \"longest_common_prefix_length\": 3},\n            instance_details=[{\"common_prefix_length\": 3}],\n        )\n    \"\"\"\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Compute metrics\n        common_prefix_length_list: list[int] = []\n        for lm_output, references in zip(lm_outputs, references_list):\n            common_prefix_length = max(len(get_longest_common_prefix(lm_output, gt)) for gt in references)\n            common_prefix_length_list.append(common_prefix_length)\n\n        return MetricResult(\n            {\n                \"average_common_prefix_length\": sum(common_prefix_length_list) / len(common_prefix_length_list),\n                \"longest_common_prefix_length\": max(common_prefix_length_list),\n            },\n            instance_details=[{\"common_prefix_length\": s} for s in common_prefix_length_list],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.common_prefix_length.CommonPrefixLength.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/common_prefix_length.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Compute metrics\n    common_prefix_length_list: list[int] = []\n    for lm_output, references in zip(lm_outputs, references_list):\n        common_prefix_length = max(len(get_longest_common_prefix(lm_output, gt)) for gt in references)\n        common_prefix_length_list.append(common_prefix_length)\n\n    return MetricResult(\n        {\n            \"average_common_prefix_length\": sum(common_prefix_length_list) / len(common_prefix_length_list),\n            \"longest_common_prefix_length\": max(common_prefix_length_list),\n        },\n        instance_details=[{\"common_prefix_length\": s} for s in common_prefix_length_list],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.common_string_length.CommonStringLength","title":"CommonStringLength","text":"<p>A metric that calculates the length of the longest common substring between the model output and the reference.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import CommonStringLength\n&gt;&gt;&gt; common_string_length = CommonStringLength()\n&gt;&gt;&gt; lm_outputs = [\"aBCDEFG\"]\n&gt;&gt;&gt; references_list = [[\"ABCDefg\"]]\n&gt;&gt;&gt; result = common_string_length.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={\"average_common_string_length\": 3.0, \"longest_common_string_length\": 3},\n    instance_details=[{\"common_string_length\": 3}],\n)\n</code></pre> Source code in <code>flexeval/core/metric/common_string_length.py</code> <pre><code>class CommonStringLength(Metric):\n    \"\"\"\n    A metric that calculates the length of the longest common substring between the model output and the reference.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import CommonStringLength\n        &gt;&gt;&gt; common_string_length = CommonStringLength()\n        &gt;&gt;&gt; lm_outputs = [\"aBCDEFG\"]\n        &gt;&gt;&gt; references_list = [[\"ABCDefg\"]]\n        &gt;&gt;&gt; result = common_string_length.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={\"average_common_string_length\": 3.0, \"longest_common_string_length\": 3},\n            instance_details=[{\"common_string_length\": 3}],\n        )\n    \"\"\"\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        # Extract text from LMOutput objects\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Compute metrics\n        common_string_length_list: list[int] = []\n        for lm_output, references in zip(lm_outputs, references_list):\n            common_string_length = max(len(get_longest_common_substring(lm_output, gt)) for gt in references)\n            common_string_length_list.append(common_string_length)\n\n        return MetricResult(\n            {\n                \"average_common_string_length\": sum(common_string_length_list) / len(common_string_length_list),\n                \"longest_common_string_length\": max(common_string_length_list),\n            },\n            instance_details=[{\"common_string_length\": s} for s in common_string_length_list],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.common_string_length.CommonStringLength.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/common_string_length.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    # Extract text from LMOutput objects\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Compute metrics\n    common_string_length_list: list[int] = []\n    for lm_output, references in zip(lm_outputs, references_list):\n        common_string_length = max(len(get_longest_common_substring(lm_output, gt)) for gt in references)\n        common_string_length_list.append(common_string_length)\n\n    return MetricResult(\n        {\n            \"average_common_string_length\": sum(common_string_length_list) / len(common_string_length_list),\n            \"longest_common_string_length\": max(common_string_length_list),\n        },\n        instance_details=[{\"common_string_length\": s} for s in common_string_length_list],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.correlation.Correlation","title":"Correlation","text":"<p>Correlation metric to compute Pearson, Spearman, or Kendall correlation coefficients. The lm_outputs and references should be numeric values, optionally preprocessed by StringProcessor.</p> <p>Parameters:</p> <ul> <li> <code>method</code>               (<code>Literal['pearson', 'spearman', 'kendall']</code>, default:                   <code>'pearson'</code> )           \u2013            <p>The correlation method to use ('pearson', 'spearman', 'kendall').</p> </li> <li> <code>lm_output_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>StringProcessor or a list of StringProcessor to be applied to the model outputs before computing the correlation. If a list is provided, the processors will be applied in order.</p> </li> <li> <code>reference_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>StringProcessor or a list of StringProcessor to be applied to the references before computing the correlation. If a list is provided, the processors will be applied in order.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import Correlation\n&gt;&gt;&gt; correlation = Correlation(method='pearson')\n&gt;&gt;&gt; lm_outputs = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n&gt;&gt;&gt; references = [[\"5\"], [\"4\"], [\"3\"], [\"2\"], [\"1\"]]\n&gt;&gt;&gt; result = correlation.evaluate(lm_outputs, references)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={\"pearson_correlation\": -1.0, \"pearson_pvalue\": 0.0},\n    instance_details=[],\n)\n</code></pre> Source code in <code>flexeval/core/metric/correlation.py</code> <pre><code>class Correlation(Metric):\n    \"\"\"\n    Correlation metric to compute Pearson, Spearman, or Kendall correlation coefficients.\n    The lm_outputs and references should be numeric values, optionally preprocessed by StringProcessor.\n\n    Args:\n        method: The correlation method to use ('pearson', 'spearman', 'kendall').\n        lm_output_processor: StringProcessor or a list of StringProcessor to be applied to the model outputs before\n            computing the correlation. If a list is provided, the processors will be applied in order.\n        reference_processor: StringProcessor or a list of StringProcessor to be applied to the references before\n            computing the correlation. If a list is provided, the processors will be applied in order.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import Correlation\n        &gt;&gt;&gt; correlation = Correlation(method='pearson')\n        &gt;&gt;&gt; lm_outputs = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n        &gt;&gt;&gt; references = [[\"5\"], [\"4\"], [\"3\"], [\"2\"], [\"1\"]]\n        &gt;&gt;&gt; result = correlation.evaluate(lm_outputs, references)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={\"pearson_correlation\": -1.0, \"pearson_pvalue\": 0.0},\n            instance_details=[],\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        method: Literal[\"pearson\", \"spearman\", \"kendall\"] = \"pearson\",\n        lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n        reference_processor: StringProcessor | list[StringProcessor] | None = None,\n    ) -&gt; None:\n        if method not in {\"pearson\", \"spearman\", \"kendall\"}:\n            msg = f\"Invalid method '{method}'. Choose from 'pearson', 'spearman', 'kendall'.\"\n            raise ValueError(msg)\n        self.method = method\n\n        self.lm_output_processors = lm_output_processor\n        self.reference_processors = reference_processor\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n        lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n\n        references = [refs[0] for refs in references_list]\n        references = [apply_string_processors(ref, self.reference_processors) for ref in references]\n\n        # Convert to numeric values\n        lm_outputs_as_float: list[float] = []\n        for output in lm_outputs:\n            try:\n                lm_outputs_as_float.append(float(output))\n            except ValueError:  # noqa:PERF203\n                warnings.warn(f\"Failed to convert model output '{output}' to float. Treating it as 0.\", stacklevel=2)\n                lm_outputs_as_float.append(0.0)\n\n        references_as_float = [float(ref) for ref in references]\n\n        # Compute metrics\n        if self.method == \"pearson\":\n            correlation, pvalue = pearsonr(lm_outputs_as_float, references_as_float)\n        elif self.method == \"spearman\":\n            correlation, pvalue = spearmanr(lm_outputs_as_float, references_as_float)\n        elif self.method == \"kendall\":\n            correlation, pvalue = kendalltau(lm_outputs_as_float, references_as_float)\n        else:\n            msg = f\"Unsupported method: {self.method}\"\n            raise ValueError(msg)\n\n        return MetricResult(\n            {f\"{self.method}_correlation\": correlation, f\"{self.method}_pvalue\": pvalue},\n            instance_details=[],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.correlation.Correlation.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method = method\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.correlation.Correlation.lm_output_processors","title":"lm_output_processors  <code>instance-attribute</code>","text":"<pre><code>lm_output_processors = lm_output_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.correlation.Correlation.reference_processors","title":"reference_processors  <code>instance-attribute</code>","text":"<pre><code>reference_processors = reference_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.correlation.Correlation.__init__","title":"__init__","text":"<pre><code>__init__(\n    method: Literal[\n        \"pearson\", \"spearman\", \"kendall\"\n    ] = \"pearson\",\n    lm_output_processor: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    reference_processor: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/correlation.py</code> <pre><code>def __init__(\n    self,\n    method: Literal[\"pearson\", \"spearman\", \"kendall\"] = \"pearson\",\n    lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n    reference_processor: StringProcessor | list[StringProcessor] | None = None,\n) -&gt; None:\n    if method not in {\"pearson\", \"spearman\", \"kendall\"}:\n        msg = f\"Invalid method '{method}'. Choose from 'pearson', 'spearman', 'kendall'.\"\n        raise ValueError(msg)\n    self.method = method\n\n    self.lm_output_processors = lm_output_processor\n    self.reference_processors = reference_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.correlation.Correlation.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/correlation.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n    lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n\n    references = [refs[0] for refs in references_list]\n    references = [apply_string_processors(ref, self.reference_processors) for ref in references]\n\n    # Convert to numeric values\n    lm_outputs_as_float: list[float] = []\n    for output in lm_outputs:\n        try:\n            lm_outputs_as_float.append(float(output))\n        except ValueError:  # noqa:PERF203\n            warnings.warn(f\"Failed to convert model output '{output}' to float. Treating it as 0.\", stacklevel=2)\n            lm_outputs_as_float.append(0.0)\n\n    references_as_float = [float(ref) for ref in references]\n\n    # Compute metrics\n    if self.method == \"pearson\":\n        correlation, pvalue = pearsonr(lm_outputs_as_float, references_as_float)\n    elif self.method == \"spearman\":\n        correlation, pvalue = spearmanr(lm_outputs_as_float, references_as_float)\n    elif self.method == \"kendall\":\n        correlation, pvalue = kendalltau(lm_outputs_as_float, references_as_float)\n    else:\n        msg = f\"Unsupported method: {self.method}\"\n        raise ValueError(msg)\n\n    return MetricResult(\n        {f\"{self.method}_correlation\": correlation, f\"{self.method}_pvalue\": pvalue},\n        instance_details=[],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.exact_match.ExactMatch","title":"ExactMatch","text":"<p>Exact match metric. If there are multiple references, the output is considered correct if it matches any of the references.</p> <p>Parameters:</p> <ul> <li> <code>lm_output_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>StringProcessor or a list of StringProcessor to be applied to the model outputs before comparison.</p> </li> <li> <code>reference_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>StringProcessor or list of StringProcessor to apply to the references before comparison.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in extra_info.</p> </li> <li> <code>metric_key</code>               (<code>str</code>, default:                   <code>'exact_match'</code> )           \u2013            <p>The metric name to store the mean score in metrics.json. Use this if you try multiple ExactMatch in differenct settings at once, e.g. difference string processors.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import ExactMatch\n&gt;&gt;&gt; exact_match = ExactMatch()\n&gt;&gt;&gt; lm_outputs = [\"ABC\", \"DEF\"]\n&gt;&gt;&gt; references_list = [[\"ABC\"], [\"DEFG\"]]\n&gt;&gt;&gt; result = exact_match.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={\"exact_match\": 0.5},\n    instance_details=[{\"exact_match\": True}, {\"exact_match\": False}],\n)\n</code></pre> Source code in <code>flexeval/core/metric/exact_match.py</code> <pre><code>class ExactMatch(Metric):\n    \"\"\"\n    Exact match metric.\n    If there are multiple references, the output is considered correct if it matches any of the references.\n\n    Args:\n        lm_output_processor:\n            StringProcessor or a list of StringProcessor to be applied to the model outputs before comparison.\n        reference_processor: StringProcessor or list of StringProcessor to apply to the references before comparison.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in extra_info.\n        metric_key: The metric name to store the mean score in metrics.json.\n            Use this if you try multiple ExactMatch in differenct settings at once, e.g. difference string processors.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import ExactMatch\n        &gt;&gt;&gt; exact_match = ExactMatch()\n        &gt;&gt;&gt; lm_outputs = [\"ABC\", \"DEF\"]\n        &gt;&gt;&gt; references_list = [[\"ABC\"], [\"DEFG\"]]\n        &gt;&gt;&gt; result = exact_match.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={\"exact_match\": 0.5},\n            instance_details=[{\"exact_match\": True}, {\"exact_match\": False}],\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n        reference_processor: StringProcessor | list[StringProcessor] | None = None,\n        category_key: str | None = None,\n        metric_key: str = \"exact_match\",\n    ) -&gt; None:\n        self.lm_output_processors = lm_output_processor\n        self.reference_processors = reference_processor\n        self.category_key = category_key\n        self.metric_key = metric_key\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        # Extract text from LMOutput objects and normalize text data\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n        references_list = [\n            [apply_string_processors(ref, self.reference_processors) for ref in references]\n            for references in references_list\n        ]\n\n        # Compute metrics\n        exact_match_list = [\n            lm_output in expected_output for lm_output, expected_output in zip(lm_outputs, references_list)\n        ]\n        summary = {self.metric_key: sum(exact_match_list) / len(exact_match_list)}\n\n        if self.category_key:\n            categories = [extra_info[self.category_key] for extra_info in extra_info_list]\n            category_wise_scores = aggregate_category_wise_scores(exact_match_list, categories)\n            for category, category_wise_score in category_wise_scores.items():\n                summary[f\"{self.metric_key}/{category}\"] = category_wise_score\n\n        return MetricResult(\n            summary,\n            instance_details=[{self.metric_key: s} for s in exact_match_list],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.exact_match.ExactMatch.lm_output_processors","title":"lm_output_processors  <code>instance-attribute</code>","text":"<pre><code>lm_output_processors = lm_output_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.exact_match.ExactMatch.reference_processors","title":"reference_processors  <code>instance-attribute</code>","text":"<pre><code>reference_processors = reference_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.exact_match.ExactMatch.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.exact_match.ExactMatch.metric_key","title":"metric_key  <code>instance-attribute</code>","text":"<pre><code>metric_key = metric_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.exact_match.ExactMatch.__init__","title":"__init__","text":"<pre><code>__init__(\n    lm_output_processor: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    reference_processor: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    category_key: str | None = None,\n    metric_key: str = \"exact_match\",\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/exact_match.py</code> <pre><code>def __init__(\n    self,\n    lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n    reference_processor: StringProcessor | list[StringProcessor] | None = None,\n    category_key: str | None = None,\n    metric_key: str = \"exact_match\",\n) -&gt; None:\n    self.lm_output_processors = lm_output_processor\n    self.reference_processors = reference_processor\n    self.category_key = category_key\n    self.metric_key = metric_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.exact_match.ExactMatch.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/exact_match.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    # Extract text from LMOutput objects and normalize text data\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n    references_list = [\n        [apply_string_processors(ref, self.reference_processors) for ref in references]\n        for references in references_list\n    ]\n\n    # Compute metrics\n    exact_match_list = [\n        lm_output in expected_output for lm_output, expected_output in zip(lm_outputs, references_list)\n    ]\n    summary = {self.metric_key: sum(exact_match_list) / len(exact_match_list)}\n\n    if self.category_key:\n        categories = [extra_info[self.category_key] for extra_info in extra_info_list]\n        category_wise_scores = aggregate_category_wise_scores(exact_match_list, categories)\n        for category, category_wise_score in category_wise_scores.items():\n            summary[f\"{self.metric_key}/{category}\"] = category_wise_score\n\n    return MetricResult(\n        summary,\n        instance_details=[{self.metric_key: s} for s in exact_match_list],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.finish_reason.FinishReasonCount","title":"FinishReasonCount","text":"<p>Metric to compute the ratio of different finish_reason values.</p> Source code in <code>flexeval/core/metric/finish_reason.py</code> <pre><code>class FinishReasonCount(Metric):\n    \"\"\"\n    Metric to compute the ratio of different finish_reason values.\n    \"\"\"\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        # Count finish_reason occurrences from messages\n        finish_reason_counter = Counter()\n        for lm_output in lm_outputs:\n            if not isinstance(lm_output, LMOutput):\n                msg = \"FinishReasonMetric expects lm_outputs to be an LMOutput, but received a different type.\"\n                raise TypeError(msg)\n            finish_reason_counter[lm_output.finish_reason] += 1\n\n        total_count = sum(finish_reason_counter.values())\n        summary = {}\n        if total_count &gt; 0:\n            for finish_reason, count in finish_reason_counter.items():\n                summary[f\"finish_reason_ratio-{finish_reason}\"] = count / total_count\n\n        return MetricResult(\n            summary=summary, instance_details=[{\"finish_reason\": lm_output.finish_reason} for lm_output in lm_outputs]\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.finish_reason.FinishReasonCount.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/finish_reason.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    # Count finish_reason occurrences from messages\n    finish_reason_counter = Counter()\n    for lm_output in lm_outputs:\n        if not isinstance(lm_output, LMOutput):\n            msg = \"FinishReasonMetric expects lm_outputs to be an LMOutput, but received a different type.\"\n            raise TypeError(msg)\n        finish_reason_counter[lm_output.finish_reason] += 1\n\n    total_count = sum(finish_reason_counter.values())\n    summary = {}\n    if total_count &gt; 0:\n        for finish_reason, count in finish_reason_counter.items():\n            summary[f\"finish_reason_ratio-{finish_reason}\"] = count / total_count\n\n    return MetricResult(\n        summary=summary, instance_details=[{\"finish_reason\": lm_output.finish_reason} for lm_output in lm_outputs]\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore","title":"ChatLLMGEvalScore","text":"<p>A metric that evaluates the output of <code>LanguageModel.batch_generate_chat_response</code>. Unlike ChatLLMScore, this metric let the model output logprobs for all valid scores and calculate weighted score among them. Note that due to constraint for OpenAI models, the number of valid scores must not exceed 20.</p> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>required</code>)           \u2013            <p>An instance of <code>LanguageModel</code> to evaluate the output of the model.</p> </li> <li> <code>prompt_template</code>               (<code>required</code>)           \u2013            <p>An instance of <code>PromptTemplate</code> to embed the input for the evaluator.</p> </li> <li> <code>valid_score_range</code>               (<code>required</code>)           \u2013            <p>A tuple of two integers representing the valid score range. If the parsed score is out of the range, it will be ignored.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The batch size for the evaluator.</p> </li> <li> <code>system_message</code>               (<code>str | PromptTemplate | None</code>, default:                   <code>None</code> )           \u2013            <p>A system message to be prepended to the input for the evaluator.</p> </li> <li> <code>disable_tqdm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to disable the progress bar.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in extra_info.</p> </li> <li> <code>prob_threshold</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>For considering low probability among all of valid scores, return None (invalid) if sum of the all probability among vaild scores is less than this value.</p> </li> <li> <code>metric_prefix</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A prefix to be added to the metric keys in the summary and instance details.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import ChatLLMGEvalScore, HuggingFaceLM, Jinja2PromptTemplate\n&gt;&gt;&gt; language_model = HuggingFaceLM(\"Qwen/Qwen2.5-0.5B-Instruct\")\n&gt;&gt;&gt; template = \"Evaluate the quality of this text.\\n`{{ lm_output }}`\\nOutput only a number from 1 to 5.\"\n&gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n&gt;&gt;&gt; system_message = \"This is the system message.\"\n&gt;&gt;&gt; llm_score = ChatLLMGEvalScore(language_model, prompt_template, [1, 5], system_message=system_message)\n&gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n&gt;&gt;&gt; llm_score.evaluate(lm_outputs)\nMetricResult(\n    summary={'llm_geval_score': 1.179980414173022, 'num_failed_score_parses': 0},\n    instance_details=[\n        {\n            'llm_geval_score': 1.1509989197179789,\n            'llm_geval_score_input': [\n                {'role': 'system', 'content': 'This is the system message.'},\n                {'role': 'user', 'content': 'Evaluate the quality of this text...'}\n            ],\n            'llm_geval_score_logprobs': {\n                '1': -0.06977498531341553,\n                '2': -3.687819004058838,\n                '3': -3.937819480895996,\n                '4': -5.812800884246826,\n                '5': -3.937807083129883\n            },\n            'llm_geval_score_generation_probs': {\n                1: 0.932603645815178,\n                2: 0.02502652531327666,\n                3: 0.01949066821765914,\n                4: 0.002989046364034347,\n                5: 0.019490909859903\n            }\n        },\n        {\n            'llm_geval_score': 1.208961908628065,\n            'llm_geval_score_input': [\n                {'role': 'system', 'content': 'This is the system message.'},\n                {'role': 'user', 'content': 'Evaluate the quality of this text...'}\n            ],\n            'llm_geval_score_logprobs': {\n                '1': -0.13043057918548584,\n                '2': -2.8754935264587402,\n                '3': -3.000467538833618,\n                '4': -4.750283241271973,\n                '5': -5.000345706939697\n            },\n            'llm_geval_score_generation_probs': {\n                1: 0.8777174226922144,\n                2: 0.05638830351569556,\n                3: 0.04976379642068341,\n                4: 0.008649245032977617,\n                5: 0.006735618046639277\n            }\n        }\n    ])\n</code></pre> Source code in <code>flexeval/core/metric/llm_geval_score.py</code> <pre><code>class ChatLLMGEvalScore(Metric):\n    \"\"\"A metric that evaluates the output of `LanguageModel.batch_generate_chat_response`.\n    Unlike ChatLLMScore, this metric let the model output logprobs for all valid scores and\n    calculate weighted score among them.\n    Note that due to constraint for OpenAI models, the number of valid scores must not exceed 20.\n\n    Args:\n        language_model (required): An instance of `LanguageModel` to evaluate the output of the model.\n        prompt_template (required): An instance of `PromptTemplate` to embed the input for the evaluator.\n        valid_score_range (required): A tuple of two integers representing the valid score range.\n            If the parsed score is out of the range, it will be ignored.\n        batch_size: The batch size for the evaluator.\n        system_message: A system message to be prepended to the input for the evaluator.\n        disable_tqdm: Whether to disable the progress bar.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in extra_info.\n        prob_threshold: For considering low probability among all of valid scores,\n            return None (invalid) if sum of the all probability among vaild scores is less than this value.\n        metric_prefix: A prefix to be added to the metric keys in the summary and instance details.\n\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import ChatLLMGEvalScore, HuggingFaceLM, Jinja2PromptTemplate\n        &gt;&gt;&gt; language_model = HuggingFaceLM(\"Qwen/Qwen2.5-0.5B-Instruct\")\n        &gt;&gt;&gt; template = \"Evaluate the quality of this text.\\\\n`{{ lm_output }}`\\\\nOutput only a number from 1 to 5.\"\n        &gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n        &gt;&gt;&gt; system_message = \"This is the system message.\"\n        &gt;&gt;&gt; llm_score = ChatLLMGEvalScore(language_model, prompt_template, [1, 5], system_message=system_message)\n        &gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n        &gt;&gt;&gt; llm_score.evaluate(lm_outputs)\n        MetricResult(\n            summary={'llm_geval_score': 1.179980414173022, 'num_failed_score_parses': 0},\n            instance_details=[\n                {\n                    'llm_geval_score': 1.1509989197179789,\n                    'llm_geval_score_input': [\n                        {'role': 'system', 'content': 'This is the system message.'},\n                        {'role': 'user', 'content': 'Evaluate the quality of this text...'}\n                    ],\n                    'llm_geval_score_logprobs': {\n                        '1': -0.06977498531341553,\n                        '2': -3.687819004058838,\n                        '3': -3.937819480895996,\n                        '4': -5.812800884246826,\n                        '5': -3.937807083129883\n                    },\n                    'llm_geval_score_generation_probs': {\n                        1: 0.932603645815178,\n                        2: 0.02502652531327666,\n                        3: 0.01949066821765914,\n                        4: 0.002989046364034347,\n                        5: 0.019490909859903\n                    }\n                },\n                {\n                    'llm_geval_score': 1.208961908628065,\n                    'llm_geval_score_input': [\n                        {'role': 'system', 'content': 'This is the system message.'},\n                        {'role': 'user', 'content': 'Evaluate the quality of this text...'}\n                    ],\n                    'llm_geval_score_logprobs': {\n                        '1': -0.13043057918548584,\n                        '2': -2.8754935264587402,\n                        '3': -3.000467538833618,\n                        '4': -4.750283241271973,\n                        '5': -5.000345706939697\n                    },\n                    'llm_geval_score_generation_probs': {\n                        1: 0.8777174226922144,\n                        2: 0.05638830351569556,\n                        3: 0.04976379642068341,\n                        4: 0.008649245032977617,\n                        5: 0.006735618046639277\n                    }\n                }\n            ])\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        valid_score_range: tuple[int, int],\n        batch_size: int = 4,\n        system_message: str | PromptTemplate | None = None,\n        disable_tqdm: bool = False,\n        category_key: str | None = None,\n        prob_threshold: float = 0,\n        metric_prefix: str | None = None,\n    ) -&gt; None:\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.batch_size = batch_size\n        self.system_message = system_message\n        self.disable_tqdm = disable_tqdm\n        self.valid_score_range = valid_score_range\n        self.category_key = category_key\n        self.prob_threshold = prob_threshold\n        self.metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n        self.valid_labels = [str(score) for score in range(valid_score_range[0], valid_score_range[1] + 1)]\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]] | None = None,\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if extra_info_list is None:\n            extra_info_list = [{} for _ in lm_outputs]\n        if references_list is None:\n            references_list = [[] for _ in lm_outputs]\n\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Compute metrics\n        evaluator_input_list = prepare_chat_input_for_evaluator(\n            lm_outputs, references_list, extra_info_list, self.prompt_template, self.system_message\n        )\n        evaluator_logprobs_list: list[dict[str, float]] = generate_evaluation_logprobs(\n            evaluator_input_list,\n            self.language_model,\n            self.valid_labels,\n            self.batch_size,\n            self.disable_tqdm,\n            \"Calculating logprobs\",\n        )\n\n        evaluator_score_list: list[int | None] = []\n        evaluator_probs_list: list[dict[int, float]] = []\n        for evaluator_logprobs in evaluator_logprobs_list:\n            evaluator_score, evaluator_probs = calculate_weighted_average(\n                evaluator_logprobs,\n                self.valid_score_range,\n                self.prob_threshold,\n            )\n            if evaluator_score is None:\n                logger.warning(f\"Failed to parse score from evaluator logprobs: {evaluator_logprobs}\")\n            evaluator_score_list.append(evaluator_score)\n            evaluator_probs_list.append(evaluator_probs)\n\n        summary = summarize_evaluator_geval_scores(\n            evaluator_score_list,\n            extra_info_list,\n            self.category_key,\n        )\n\n        return MetricResult(\n            {self.metric_prefix + key: value for key, value in summary.items()},\n            instance_details=[\n                {\n                    f\"{self.metric_prefix}llm_geval_score\": eval_score,\n                    f\"{self.metric_prefix}llm_geval_score_input\": eval_in,\n                    f\"{self.metric_prefix}llm_geval_score_logprobs\": eval_logprobs,\n                    f\"{self.metric_prefix}llm_geval_score_generation_probs\": eval_probs,\n                }\n                for eval_score, eval_in, eval_logprobs, eval_probs in zip(\n                    evaluator_score_list,\n                    evaluator_input_list,\n                    evaluator_logprobs_list,\n                    evaluator_probs_list,\n                )\n            ],\n        )\n\n    def cleanup_resources(self) -&gt; None:\n        self.language_model.cleanup_resources()\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.system_message","title":"system_message  <code>instance-attribute</code>","text":"<pre><code>system_message = system_message\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.disable_tqdm","title":"disable_tqdm  <code>instance-attribute</code>","text":"<pre><code>disable_tqdm = disable_tqdm\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.valid_score_range","title":"valid_score_range  <code>instance-attribute</code>","text":"<pre><code>valid_score_range = valid_score_range\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.prob_threshold","title":"prob_threshold  <code>instance-attribute</code>","text":"<pre><code>prob_threshold = prob_threshold\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.metric_prefix","title":"metric_prefix  <code>instance-attribute</code>","text":"<pre><code>metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.valid_labels","title":"valid_labels  <code>instance-attribute</code>","text":"<pre><code>valid_labels = [\n    str(score)\n    for score in range(\n        valid_score_range[0], valid_score_range[1] + 1\n    )\n]\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.__init__","title":"__init__","text":"<pre><code>__init__(\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    valid_score_range: tuple[int, int],\n    batch_size: int = 4,\n    system_message: str | PromptTemplate | None = None,\n    disable_tqdm: bool = False,\n    category_key: str | None = None,\n    prob_threshold: float = 0,\n    metric_prefix: str | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_geval_score.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    valid_score_range: tuple[int, int],\n    batch_size: int = 4,\n    system_message: str | PromptTemplate | None = None,\n    disable_tqdm: bool = False,\n    category_key: str | None = None,\n    prob_threshold: float = 0,\n    metric_prefix: str | None = None,\n) -&gt; None:\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.batch_size = batch_size\n    self.system_message = system_message\n    self.disable_tqdm = disable_tqdm\n    self.valid_score_range = valid_score_range\n    self.category_key = category_key\n    self.prob_threshold = prob_threshold\n    self.metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n    self.valid_labels = [str(score) for score in range(valid_score_range[0], valid_score_range[1] + 1)]\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/llm_geval_score.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if extra_info_list is None:\n        extra_info_list = [{} for _ in lm_outputs]\n    if references_list is None:\n        references_list = [[] for _ in lm_outputs]\n\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Compute metrics\n    evaluator_input_list = prepare_chat_input_for_evaluator(\n        lm_outputs, references_list, extra_info_list, self.prompt_template, self.system_message\n    )\n    evaluator_logprobs_list: list[dict[str, float]] = generate_evaluation_logprobs(\n        evaluator_input_list,\n        self.language_model,\n        self.valid_labels,\n        self.batch_size,\n        self.disable_tqdm,\n        \"Calculating logprobs\",\n    )\n\n    evaluator_score_list: list[int | None] = []\n    evaluator_probs_list: list[dict[int, float]] = []\n    for evaluator_logprobs in evaluator_logprobs_list:\n        evaluator_score, evaluator_probs = calculate_weighted_average(\n            evaluator_logprobs,\n            self.valid_score_range,\n            self.prob_threshold,\n        )\n        if evaluator_score is None:\n            logger.warning(f\"Failed to parse score from evaluator logprobs: {evaluator_logprobs}\")\n        evaluator_score_list.append(evaluator_score)\n        evaluator_probs_list.append(evaluator_probs)\n\n    summary = summarize_evaluator_geval_scores(\n        evaluator_score_list,\n        extra_info_list,\n        self.category_key,\n    )\n\n    return MetricResult(\n        {self.metric_prefix + key: value for key, value in summary.items()},\n        instance_details=[\n            {\n                f\"{self.metric_prefix}llm_geval_score\": eval_score,\n                f\"{self.metric_prefix}llm_geval_score_input\": eval_in,\n                f\"{self.metric_prefix}llm_geval_score_logprobs\": eval_logprobs,\n                f\"{self.metric_prefix}llm_geval_score_generation_probs\": eval_probs,\n            }\n            for eval_score, eval_in, eval_logprobs, eval_probs in zip(\n                evaluator_score_list,\n                evaluator_input_list,\n                evaluator_logprobs_list,\n                evaluator_probs_list,\n            )\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.cleanup_resources","title":"cleanup_resources","text":"<pre><code>cleanup_resources() -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_geval_score.py</code> <pre><code>def cleanup_resources(self) -&gt; None:\n    self.language_model.cleanup_resources()\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.ChatLLMGEvalScore.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/llm_geval_score.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return (\n        f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore","title":"LLMGEvalScore","text":"<p>Let LanguageModel evaluate the output of another LanguageModel. Unlike LLMScore, this metric let the model output logprobs for all valid scores and calculate weighted score among them. Note that due to constraint for OpenAI models, the number of valid scores must not exceed 20. For detail, see https://aclanthology.org/2023.emnlp-main.153/</p> <p>You can specify the evaluation criteria in <code>PromptTemplate</code>.</p> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>required</code>)           \u2013            <p>An instance of <code>LanguageModel</code> to evaluate the output of the model.</p> </li> <li> <code>prompt_template</code>               (<code>required</code>)           \u2013            <p>An instance of <code>PromptTemplate</code> to embed the input for the evaluator.</p> </li> <li> <code>valid_score_range</code>               (<code>required</code>)           \u2013            <p>A tuple of two integers representing the valid score range. If the parsed score is out of the range, it will be ignored.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The batch size for the evaluator.</p> </li> <li> <code>disable_tqdm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to disable the progress bar.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in extra_info.</p> </li> <li> <code>prob_threshold</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>For considering low probability among all of valid scores, return None (invalid) if sum of the all probability among vaild scores is less than this value.</p> </li> <li> <code>metric_prefix</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A prefix to be added to the metric keys in the summary and instance details.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import LLMGEvalScore, HuggingFaceLM, Jinja2PromptTemplate\n&gt;&gt;&gt; language_model = HuggingFaceLM(\"Qwen/Qwen2.5-0.5B-Instruct\")\n&gt;&gt;&gt; template = \"Evaluate the quality of this text.\\n`{{ lm_output }}`\\nOutput only a number from 1 to 5.\"\n&gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n&gt;&gt;&gt; llm_score = LLMGEvalScore(language_model, prompt_template, [1, 5])\n&gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n&gt;&gt;&gt; llm_score.evaluate(lm_outputs)\nMetricResult(\n    summary={'llm_geval_score': 1.4399980931290486, 'num_failed_score_parses': 0},\n    instance_details=[\n        {\n            'llm_geval_score': 1.418920817254956,\n            'llm_geval_score_input': 'Evaluate the quality of this text...',\n            'llm_geval_score_logprobs': {\n                '1': -4.0625,\n                '2': -7.75,\n                '3': -8.25,\n                '4': -8.0625,\n                '5': -6.4375\n            },\n            'llm_geval_score_generation_probs': {\n                1: 0.017205950425851383,\n                2: 0.00043074254057568753,\n                3: 0.00026125855730166754,\n                4: 0.000315137974737356,\n                5: 0.0016004026902445643\n            }\n        },\n        {\n            'llm_geval_score': 1.461075369003141\n            'llm_geval_score_input': 'Evaluate the quality of this text...',\n            'llm_geval_score_logprobs': {\n                '1': -4.25,\n                '2': -8.1875,\n                '3': -8.375,\n                '4': -8.125,\n                '5': -6.5\n            },\n            'llm_geval_score_generation_probs': {\n                1: 0.014264233908999256,\n                2: 0.00027810828659249914,\n                3: 0.00023055986759244163,\n                4: 0.0002960447300568554,\n                5: 0.0015034391929775724\n            }\n        }\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/llm_geval_score.py</code> <pre><code>class LLMGEvalScore(Metric):\n    \"\"\"Let LanguageModel evaluate the output of another LanguageModel.\n    Unlike LLMScore, this metric let the model output logprobs for all valid scores and\n    calculate weighted score among them.\n    Note that due to constraint for OpenAI models, the number of valid scores must not exceed 20.\n    For detail, see https://aclanthology.org/2023.emnlp-main.153/\n\n    You can specify the evaluation criteria in `PromptTemplate`.\n\n    Args:\n        language_model (required): An instance of `LanguageModel` to evaluate the output of the model.\n        prompt_template (required): An instance of `PromptTemplate` to embed the input for the evaluator.\n        valid_score_range (required): A tuple of two integers representing the valid score range.\n            If the parsed score is out of the range, it will be ignored.\n        batch_size: The batch size for the evaluator.\n        disable_tqdm: Whether to disable the progress bar.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in extra_info.\n        prob_threshold: For considering low probability among all of valid scores,\n            return None (invalid) if sum of the all probability among vaild scores is less than this value.\n        metric_prefix: A prefix to be added to the metric keys in the summary and instance details.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import LLMGEvalScore, HuggingFaceLM, Jinja2PromptTemplate\n        &gt;&gt;&gt; language_model = HuggingFaceLM(\"Qwen/Qwen2.5-0.5B-Instruct\")\n        &gt;&gt;&gt; template = \"Evaluate the quality of this text.\\\\n`{{ lm_output }}`\\\\nOutput only a number from 1 to 5.\"\n        &gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n        &gt;&gt;&gt; llm_score = LLMGEvalScore(language_model, prompt_template, [1, 5])\n        &gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n        &gt;&gt;&gt; llm_score.evaluate(lm_outputs)\n        MetricResult(\n            summary={'llm_geval_score': 1.4399980931290486, 'num_failed_score_parses': 0},\n            instance_details=[\n                {\n                    'llm_geval_score': 1.418920817254956,\n                    'llm_geval_score_input': 'Evaluate the quality of this text...',\n                    'llm_geval_score_logprobs': {\n                        '1': -4.0625,\n                        '2': -7.75,\n                        '3': -8.25,\n                        '4': -8.0625,\n                        '5': -6.4375\n                    },\n                    'llm_geval_score_generation_probs': {\n                        1: 0.017205950425851383,\n                        2: 0.00043074254057568753,\n                        3: 0.00026125855730166754,\n                        4: 0.000315137974737356,\n                        5: 0.0016004026902445643\n                    }\n                },\n                {\n                    'llm_geval_score': 1.461075369003141\n                    'llm_geval_score_input': 'Evaluate the quality of this text...',\n                    'llm_geval_score_logprobs': {\n                        '1': -4.25,\n                        '2': -8.1875,\n                        '3': -8.375,\n                        '4': -8.125,\n                        '5': -6.5\n                    },\n                    'llm_geval_score_generation_probs': {\n                        1: 0.014264233908999256,\n                        2: 0.00027810828659249914,\n                        3: 0.00023055986759244163,\n                        4: 0.0002960447300568554,\n                        5: 0.0015034391929775724\n                    }\n                }\n            ]\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        valid_score_range: tuple[int, int],\n        batch_size: int = 4,\n        disable_tqdm: bool = False,\n        category_key: str | None = None,\n        prob_threshold: float = 0,\n        metric_prefix: str | None = None,\n    ) -&gt; None:\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.batch_size = batch_size\n        self.disable_tqdm = disable_tqdm\n        self.valid_score_range = valid_score_range\n        self.category_key = category_key\n        self.prob_threshold = prob_threshold\n        self.metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n        self.valid_labels = [str(score) for score in range(valid_score_range[0], valid_score_range[1] + 1)]\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]] | None = None,\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if extra_info_list is None:\n            extra_info_list = [{} for _ in lm_outputs]\n        if references_list is None:\n            references_list = [[] for _ in lm_outputs]\n\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Compute metrics\n        evaluator_input_list: list[str] = prepare_text_input_for_evaluator(\n            lm_outputs, references_list, extra_info_list, self.prompt_template\n        )\n        evaluator_logprobs_list: list[dict[str, float]] = generate_evaluation_logprobs(\n            evaluator_input_list,\n            self.language_model,\n            self.valid_labels,\n            self.batch_size,\n            self.disable_tqdm,\n            \"Calculating logprobs\",\n        )\n\n        evaluator_score_list: list[int | None] = []\n        evaluator_probs_list: list[dict[int, float]] = []\n        for evaluator_logprobs in evaluator_logprobs_list:\n            evaluator_score, evaluator_probs = calculate_weighted_average(\n                evaluator_logprobs,\n                self.valid_score_range,\n                self.prob_threshold,\n            )\n            if evaluator_score is None:\n                logger.warning(f\"Failed to parse score from evaluator logprobs: {evaluator_logprobs}\")\n            evaluator_score_list.append(evaluator_score)\n            evaluator_probs_list.append(evaluator_probs)\n\n        summary = summarize_evaluator_geval_scores(\n            evaluator_score_list,\n            extra_info_list,\n            self.category_key,\n        )\n\n        return MetricResult(\n            {self.metric_prefix + key: value for key, value in summary.items()},\n            instance_details=[\n                {\n                    f\"{self.metric_prefix}llm_geval_score\": eval_score,\n                    f\"{self.metric_prefix}llm_geval_score_input\": eval_in,\n                    f\"{self.metric_prefix}llm_geval_score_logprobs\": eval_logprobs,\n                    f\"{self.metric_prefix}llm_geval_score_generation_probs\": eval_probs,\n                }\n                for eval_score, eval_in, eval_logprobs, eval_probs in zip(\n                    evaluator_score_list,\n                    evaluator_input_list,\n                    evaluator_logprobs_list,\n                    evaluator_probs_list,\n                )\n            ],\n        )\n\n    def cleanup_resources(self) -&gt; None:\n        self.language_model.cleanup_resources()\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore.disable_tqdm","title":"disable_tqdm  <code>instance-attribute</code>","text":"<pre><code>disable_tqdm = disable_tqdm\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore.valid_score_range","title":"valid_score_range  <code>instance-attribute</code>","text":"<pre><code>valid_score_range = valid_score_range\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore.prob_threshold","title":"prob_threshold  <code>instance-attribute</code>","text":"<pre><code>prob_threshold = prob_threshold\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore.metric_prefix","title":"metric_prefix  <code>instance-attribute</code>","text":"<pre><code>metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore.valid_labels","title":"valid_labels  <code>instance-attribute</code>","text":"<pre><code>valid_labels = [\n    str(score)\n    for score in range(\n        valid_score_range[0], valid_score_range[1] + 1\n    )\n]\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore.__init__","title":"__init__","text":"<pre><code>__init__(\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    valid_score_range: tuple[int, int],\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    category_key: str | None = None,\n    prob_threshold: float = 0,\n    metric_prefix: str | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_geval_score.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    valid_score_range: tuple[int, int],\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    category_key: str | None = None,\n    prob_threshold: float = 0,\n    metric_prefix: str | None = None,\n) -&gt; None:\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.batch_size = batch_size\n    self.disable_tqdm = disable_tqdm\n    self.valid_score_range = valid_score_range\n    self.category_key = category_key\n    self.prob_threshold = prob_threshold\n    self.metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n    self.valid_labels = [str(score) for score in range(valid_score_range[0], valid_score_range[1] + 1)]\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/llm_geval_score.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if extra_info_list is None:\n        extra_info_list = [{} for _ in lm_outputs]\n    if references_list is None:\n        references_list = [[] for _ in lm_outputs]\n\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Compute metrics\n    evaluator_input_list: list[str] = prepare_text_input_for_evaluator(\n        lm_outputs, references_list, extra_info_list, self.prompt_template\n    )\n    evaluator_logprobs_list: list[dict[str, float]] = generate_evaluation_logprobs(\n        evaluator_input_list,\n        self.language_model,\n        self.valid_labels,\n        self.batch_size,\n        self.disable_tqdm,\n        \"Calculating logprobs\",\n    )\n\n    evaluator_score_list: list[int | None] = []\n    evaluator_probs_list: list[dict[int, float]] = []\n    for evaluator_logprobs in evaluator_logprobs_list:\n        evaluator_score, evaluator_probs = calculate_weighted_average(\n            evaluator_logprobs,\n            self.valid_score_range,\n            self.prob_threshold,\n        )\n        if evaluator_score is None:\n            logger.warning(f\"Failed to parse score from evaluator logprobs: {evaluator_logprobs}\")\n        evaluator_score_list.append(evaluator_score)\n        evaluator_probs_list.append(evaluator_probs)\n\n    summary = summarize_evaluator_geval_scores(\n        evaluator_score_list,\n        extra_info_list,\n        self.category_key,\n    )\n\n    return MetricResult(\n        {self.metric_prefix + key: value for key, value in summary.items()},\n        instance_details=[\n            {\n                f\"{self.metric_prefix}llm_geval_score\": eval_score,\n                f\"{self.metric_prefix}llm_geval_score_input\": eval_in,\n                f\"{self.metric_prefix}llm_geval_score_logprobs\": eval_logprobs,\n                f\"{self.metric_prefix}llm_geval_score_generation_probs\": eval_probs,\n            }\n            for eval_score, eval_in, eval_logprobs, eval_probs in zip(\n                evaluator_score_list,\n                evaluator_input_list,\n                evaluator_logprobs_list,\n                evaluator_probs_list,\n            )\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore.cleanup_resources","title":"cleanup_resources","text":"<pre><code>cleanup_resources() -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_geval_score.py</code> <pre><code>def cleanup_resources(self) -&gt; None:\n    self.language_model.cleanup_resources()\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_geval_score.LLMGEvalScore.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/llm_geval_score.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return (\n        f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel","title":"ChatLLMLabel","text":"<p>A metric that evaluates the output of <code>LanguageModel.batch_generate_chat_response</code>.</p> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>LanguageModel</code>)           \u2013            <p>An instance of <code>LanguageModel</code> to evaluate the output of the model.</p> </li> <li> <code>prompt_template</code>               (<code>PromptTemplate</code>)           \u2013            <p>An instance of <code>PromptTemplate</code> to embed the input for the evaluator.</p> </li> <li> <code>label_names</code>               (<code>list[str]</code>)           \u2013            <p>A list of valid label names.</p> </li> <li> <code>label_points</code>               (<code>list[float | int] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of points for each label specified in label_names.</p> </li> <li> <code>system_message</code>               (<code>str | PromptTemplate | None</code>, default:                   <code>None</code> )           \u2013            <p>A system message to be prepended to the input for the evaluator.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The batch size for the evaluator.</p> </li> <li> <code>disable_tqdm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to disable the progress bar.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in extra_info.</p> </li> <li> <code>metric_prefix</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A prefix to be added to the metric keys in the summary and instance details.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import ChatLLMScore, OpenAIChatAPI, Jinja2PromptTemplate\n&gt;&gt;&gt; language_model = OpenAIChatAPI(model_name=\"gpt-3.5-turbo\")\n&gt;&gt;&gt; template = \"Evaluate the quality of this text on a scale of Good/Bad.\\n`{{ lm_output }}`\\nPut the label at the end like [[Good]].\"\n&gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n&gt;&gt;&gt; system_message = \"This is the system message.\"\n&gt;&gt;&gt; label_names = [\"Good\", \"Bad\"]\n&gt;&gt;&gt; label_points = [1.0, 0.0]\n&gt;&gt;&gt; llm_label = ChatLLMLabel(language_model, prompt_template, label_names, label_points)\n&gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n&gt;&gt;&gt; result = llm_label.evaluate(lm_outputs)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'llm_score': 0.5, 'llm_label_distribution': {'Good': 0.5, 'Bad': 0.5}, 'num_failed_score_parses': 0},\n    instance_details=[\n        {\n            'llm_label': 'Good',\n            'llm_score': 1.0,\n            'llm_label_input': 'Evaluate the quality of this text...',\n            'llm_label_output': 'This text is natural, ... [[Good]]'\n        },\n        {\n            'llm_label': 'Bad',\n            'llm_score': 0.0,\n            'llm_label_input': 'Evaluate the quality of this text on a scale of Good/Bad.\\n`Good mrrrning!`\\nPut the label at the end like [[Good]].',\n            'llm_label_output': 'This text contains a spelling error, ... [[Bad]]'\n        }\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>class ChatLLMLabel(Metric):\n    \"\"\"\n    A metric that evaluates the output of `LanguageModel.batch_generate_chat_response`.\n\n    Args:\n        language_model: An instance of `LanguageModel` to evaluate the output of the model.\n        prompt_template: An instance of `PromptTemplate` to embed the input for the evaluator.\n        label_names: A list of valid label names.\n        label_points: A list of points for each label specified in label_names.\n        system_message: A system message to be prepended to the input for the evaluator.\n        batch_size: The batch size for the evaluator.\n        disable_tqdm: Whether to disable the progress bar.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in extra_info.\n        metric_prefix: A prefix to be added to the metric keys in the summary and instance details.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import ChatLLMScore, OpenAIChatAPI, Jinja2PromptTemplate\n        &gt;&gt;&gt; language_model = OpenAIChatAPI(model_name=\"gpt-3.5-turbo\")\n        &gt;&gt;&gt; template = \"Evaluate the quality of this text on a scale of Good/Bad.\\\\n`{{ lm_output }}`\\\\nPut the label at the end like [[Good]].\"\n        &gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n        &gt;&gt;&gt; system_message = \"This is the system message.\"\n        &gt;&gt;&gt; label_names = [\"Good\", \"Bad\"]\n        &gt;&gt;&gt; label_points = [1.0, 0.0]\n        &gt;&gt;&gt; llm_label = ChatLLMLabel(language_model, prompt_template, label_names, label_points)\n        &gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n        &gt;&gt;&gt; result = llm_label.evaluate(lm_outputs)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'llm_score': 0.5, 'llm_label_distribution': {'Good': 0.5, 'Bad': 0.5}, 'num_failed_score_parses': 0},\n            instance_details=[\n                {\n                    'llm_label': 'Good',\n                    'llm_score': 1.0,\n                    'llm_label_input': 'Evaluate the quality of this text...',\n                    'llm_label_output': 'This text is natural, ... [[Good]]'\n                },\n                {\n                    'llm_label': 'Bad',\n                    'llm_score': 0.0,\n                    'llm_label_input': 'Evaluate the quality of this text on a scale of Good/Bad.\\\\n`Good mrrrning!`\\\\nPut the label at the end like [[Good]].',\n                    'llm_label_output': 'This text contains a spelling error, ... [[Bad]]'\n                }\n            ]\n        )\n    \"\"\"  # noqa: E501\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        label_names: list[str],\n        label_points: list[float | int] | None = None,\n        system_message: str | PromptTemplate | None = None,\n        batch_size: int = 4,\n        disable_tqdm: bool = False,\n        category_key: str | None = None,\n        metric_prefix: str | None = None,\n    ) -&gt; None:\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.label_names = [re.escape(label) for label in label_names]\n\n        if label_points:\n            if len(self.label_names) != len(label_points):\n                msg = \"The lengths of label_names and weights do not match.\"\n                raise ValueError(msg)\n            label_points: list[float] = list(map(float, label_points))\n        else:\n            label_points = [0.0] * len(label_names)\n            label_points[0] = 1.0\n\n        self.weights = label_points\n        self.system_message = system_message\n        self.batch_size = batch_size\n        self.disable_tqdm = disable_tqdm\n        self.category_key = category_key\n        self.metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]] | None = None,\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if extra_info_list is None:\n            extra_info_list = [{} for _ in lm_outputs]\n        if references_list is None:\n            references_list = [[] for _ in lm_outputs]\n\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        # Extract text from LMOutput objects\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Compute metrics\n        evaluator_input_list = prepare_chat_input_for_evaluator(\n            lm_outputs, references_list, extra_info_list, self.prompt_template, self.system_message\n        )\n\n        evaluator_output_list: list[LMOutput] = generate_evaluations(\n            evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating ChatLLM score\"\n        )\n\n        evaluator_label_list: list[str] = []\n        for evaluator_output in evaluator_output_list:\n            evaluator_label = parse_label_from_evaluator_output(\n                evaluator_output.text,\n                label_names=self.label_names,\n            )\n            if evaluator_label is None:\n                logger.warning(f\"Failed to parse label from evaluator output: {evaluator_output}\")\n            evaluator_label_list.append(evaluator_label)\n\n        label2point = dict(zip(self.label_names, self.weights))\n        evaluator_score_list: list[float | None] = [label2point.get(label) for label in evaluator_label_list]\n\n        summary = summarize_evaluator_labels(\n            evaluator_label_list,\n            extra_info_list,\n            self.label_names,\n            self.weights,\n            self.category_key,\n        )\n\n        return MetricResult(\n            {self.metric_prefix + key: value for key, value in summary.items()},\n            instance_details=[\n                {\n                    f\"{self.metric_prefix}llm_label\": eval_label,\n                    f\"{self.metric_prefix}llm_score\": eval_score,\n                    f\"{self.metric_prefix}llm_label_input\": eval_in,\n                    f\"{self.metric_prefix}llm_label_output\": eval_out.text,\n                }\n                for eval_label, eval_score, eval_in, eval_out in zip(\n                    evaluator_label_list,\n                    evaluator_score_list,\n                    evaluator_input_list,\n                    evaluator_output_list,\n                )\n            ],\n        )\n\n    def cleanup_resources(self) -&gt; None:\n        self.language_model.cleanup_resources()\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.label_names","title":"label_names  <code>instance-attribute</code>","text":"<pre><code>label_names = [escape(label) for label in label_names]\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.weights","title":"weights  <code>instance-attribute</code>","text":"<pre><code>weights = label_points\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.system_message","title":"system_message  <code>instance-attribute</code>","text":"<pre><code>system_message = system_message\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.disable_tqdm","title":"disable_tqdm  <code>instance-attribute</code>","text":"<pre><code>disable_tqdm = disable_tqdm\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.metric_prefix","title":"metric_prefix  <code>instance-attribute</code>","text":"<pre><code>metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.__init__","title":"__init__","text":"<pre><code>__init__(\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    label_names: list[str],\n    label_points: list[float | int] | None = None,\n    system_message: str | PromptTemplate | None = None,\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    category_key: str | None = None,\n    metric_prefix: str | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    label_names: list[str],\n    label_points: list[float | int] | None = None,\n    system_message: str | PromptTemplate | None = None,\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    category_key: str | None = None,\n    metric_prefix: str | None = None,\n) -&gt; None:\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.label_names = [re.escape(label) for label in label_names]\n\n    if label_points:\n        if len(self.label_names) != len(label_points):\n            msg = \"The lengths of label_names and weights do not match.\"\n            raise ValueError(msg)\n        label_points: list[float] = list(map(float, label_points))\n    else:\n        label_points = [0.0] * len(label_names)\n        label_points[0] = 1.0\n\n    self.weights = label_points\n    self.system_message = system_message\n    self.batch_size = batch_size\n    self.disable_tqdm = disable_tqdm\n    self.category_key = category_key\n    self.metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if extra_info_list is None:\n        extra_info_list = [{} for _ in lm_outputs]\n    if references_list is None:\n        references_list = [[] for _ in lm_outputs]\n\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    # Extract text from LMOutput objects\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Compute metrics\n    evaluator_input_list = prepare_chat_input_for_evaluator(\n        lm_outputs, references_list, extra_info_list, self.prompt_template, self.system_message\n    )\n\n    evaluator_output_list: list[LMOutput] = generate_evaluations(\n        evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating ChatLLM score\"\n    )\n\n    evaluator_label_list: list[str] = []\n    for evaluator_output in evaluator_output_list:\n        evaluator_label = parse_label_from_evaluator_output(\n            evaluator_output.text,\n            label_names=self.label_names,\n        )\n        if evaluator_label is None:\n            logger.warning(f\"Failed to parse label from evaluator output: {evaluator_output}\")\n        evaluator_label_list.append(evaluator_label)\n\n    label2point = dict(zip(self.label_names, self.weights))\n    evaluator_score_list: list[float | None] = [label2point.get(label) for label in evaluator_label_list]\n\n    summary = summarize_evaluator_labels(\n        evaluator_label_list,\n        extra_info_list,\n        self.label_names,\n        self.weights,\n        self.category_key,\n    )\n\n    return MetricResult(\n        {self.metric_prefix + key: value for key, value in summary.items()},\n        instance_details=[\n            {\n                f\"{self.metric_prefix}llm_label\": eval_label,\n                f\"{self.metric_prefix}llm_score\": eval_score,\n                f\"{self.metric_prefix}llm_label_input\": eval_in,\n                f\"{self.metric_prefix}llm_label_output\": eval_out.text,\n            }\n            for eval_label, eval_score, eval_in, eval_out in zip(\n                evaluator_label_list,\n                evaluator_score_list,\n                evaluator_input_list,\n                evaluator_output_list,\n            )\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.cleanup_resources","title":"cleanup_resources","text":"<pre><code>cleanup_resources() -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def cleanup_resources(self) -&gt; None:\n    self.language_model.cleanup_resources()\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.ChatLLMLabel.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return (\n        f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel","title":"LLMLabel","text":"<p>Let LanguageModel to evaluate the output of another LanguageModel.</p> <p>You can specify the evaluation criteria in <code>PromptTemplate</code>. The last label value found in the output of the evaluator is used to compute the evaluation score. You can assign a score to each label. The final output is the average score and the distribution of the labels.</p> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>LanguageModel</code>)           \u2013            <p>An instance of <code>LanguageModel</code> to evaluate the output of the model.</p> </li> <li> <code>prompt_template</code>               (<code>PromptTemplate</code>)           \u2013            <p>An instance of <code>PromptTemplate</code> to embed the input for the evaluator.</p> </li> <li> <code>label_names</code>               (<code>list[str]</code>)           \u2013            <p>A list of valid label names.</p> </li> <li> <code>label_points</code>               (<code>list[float | int] | None</code>, default:                   <code>None</code> )           \u2013            <p>A list of points for each label specified in label_names.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The batch size for the evaluator.</p> </li> <li> <code>disable_tqdm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to disable the progress bar.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in extra_info.</p> </li> <li> <code>metric_prefix</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A prefix to be added to the metric keys in the summary and instance details.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import OpenAIChatAPI, Jinja2PromptTemplate, LLMLabel\n&gt;&gt;&gt; language_model = OpenAIChatAPI(model=\"gpt-3.5-turbo\")\n&gt;&gt;&gt; template = \"Evaluate the quality of this text on a scale of Good/Bad.\\n`{{ lm_output }}`\\nPut the label at the end like [[Good]].\"\n&gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n&gt;&gt;&gt; label_names = [\"Good\", \"Bad\"]\n&gt;&gt;&gt; label_points = [1.0, 0.0]\n&gt;&gt;&gt; llm_label = LLMLabel(language_model, prompt_template, label_names, label_points)\n&gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good mrrrning!\"]\n&gt;&gt;&gt; result = llm_label.evaluate(lm_outputs)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'llm_score': 0.5, 'llm_label_distribution': {'Good': 0.5, 'Bad': 0.5}, 'num_failed_score_parses': 0},\n    instance_details=[\n        {\n            'llm_label': 'Good',\n            'llm_score': 1.0,\n            'llm_label_input': 'Evaluate the quality of this text...',\n            'llm_label_output': 'This text is natural, ... [[Good]]'\n        },\n        {\n            'llm_label': 'Bad',\n            'llm_score': 0.0,\n            'llm_label_input': 'Evaluate the quality of this text on a scale of Good/Bad.\\n`Good mrrrning!`\\nPut the label at the end like [[Good]].',\n            'llm_label_output': 'This text contains a spelling error, ... [[Bad]]'\n        }\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>class LLMLabel(Metric):\n    \"\"\"Let LanguageModel to evaluate the output of another LanguageModel.\n\n    You can specify the evaluation criteria in `PromptTemplate`.\n    The last label value found in the output of the evaluator is used to compute the evaluation score.\n    You can assign a score to each label.\n    The final output is the average score and the distribution of the labels.\n\n    Args:\n        language_model: An instance of `LanguageModel` to evaluate the output of the model.\n        prompt_template: An instance of `PromptTemplate` to embed the input for the evaluator.\n        label_names: A list of valid label names.\n        label_points: A list of points for each label specified in label_names.\n        batch_size: The batch size for the evaluator.\n        disable_tqdm: Whether to disable the progress bar.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in extra_info.\n        metric_prefix: A prefix to be added to the metric keys in the summary and instance details.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import OpenAIChatAPI, Jinja2PromptTemplate, LLMLabel\n        &gt;&gt;&gt; language_model = OpenAIChatAPI(model=\"gpt-3.5-turbo\")\n        &gt;&gt;&gt; template = \"Evaluate the quality of this text on a scale of Good/Bad.\\\\n`{{ lm_output }}`\\\\nPut the label at the end like [[Good]].\"\n        &gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n        &gt;&gt;&gt; label_names = [\"Good\", \"Bad\"]\n        &gt;&gt;&gt; label_points = [1.0, 0.0]\n        &gt;&gt;&gt; llm_label = LLMLabel(language_model, prompt_template, label_names, label_points)\n        &gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good mrrrning!\"]\n        &gt;&gt;&gt; result = llm_label.evaluate(lm_outputs)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'llm_score': 0.5, 'llm_label_distribution': {'Good': 0.5, 'Bad': 0.5}, 'num_failed_score_parses': 0},\n            instance_details=[\n                {\n                    'llm_label': 'Good',\n                    'llm_score': 1.0,\n                    'llm_label_input': 'Evaluate the quality of this text...',\n                    'llm_label_output': 'This text is natural, ... [[Good]]'\n                },\n                {\n                    'llm_label': 'Bad',\n                    'llm_score': 0.0,\n                    'llm_label_input': 'Evaluate the quality of this text on a scale of Good/Bad.\\\\n`Good mrrrning!`\\\\nPut the label at the end like [[Good]].',\n                    'llm_label_output': 'This text contains a spelling error, ... [[Bad]]'\n                }\n            ]\n        )\n    \"\"\"  # noqa: E501\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        label_names: list[str],\n        label_points: list[float | int] | None = None,\n        batch_size: int = 4,\n        disable_tqdm: bool = False,\n        valid_score_range: tuple[int, int] | None = None,\n        category_key: str | None = None,\n        metric_prefix: str | None = None,\n    ) -&gt; None:\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.label_names = [re.escape(label) for label in label_names]\n\n        if label_points:\n            if len(self.label_names) != len(label_points):\n                msg = \"The lengths of label_names and weights do not match.\"\n                raise ValueError(msg)\n            label_points: list[float] = list(map(float, label_points))\n        else:\n            label_points = [0.0] * len(label_names)\n            label_points[0] = 1.0\n\n        self.weights = label_points\n        self.batch_size = batch_size\n        self.disable_tqdm = disable_tqdm\n        self.valid_score_range = valid_score_range\n        self.category_key = category_key\n        self.metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]] | None = None,\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if extra_info_list is None:\n            extra_info_list = [{} for _ in lm_outputs]\n        if references_list is None:\n            references_list = [[] for _ in lm_outputs]\n\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        # Extract text from LMOutput objects\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Compute metrics\n        evaluator_input_list: list[str] = prepare_text_input_for_evaluator(\n            lm_outputs, references_list, extra_info_list, self.prompt_template\n        )\n        evaluator_output_list: list[LMOutput] = generate_evaluations(\n            evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating LLM score\"\n        )\n\n        evaluator_label_list: list[int | None] = []\n        for evaluator_output in evaluator_output_list:\n            evaluator_label = parse_label_from_evaluator_output(\n                evaluator_output.text,\n                label_names=self.label_names,\n            )\n            if evaluator_label is None:\n                logger.warning(f\"Failed to parse label from evaluator output: {evaluator_output}\")\n            evaluator_label_list.append(evaluator_label)\n\n        label2point = dict(zip(self.label_names, self.weights))\n        evaluator_score_list: list[float | None] = [label2point.get(label) for label in evaluator_label_list]\n\n        summary = summarize_evaluator_labels(\n            evaluator_label_list,\n            extra_info_list,\n            self.label_names,\n            self.weights,\n            self.category_key,\n        )\n\n        return MetricResult(\n            {self.metric_prefix + key: value for key, value in summary.items()},\n            instance_details=[\n                {\n                    f\"{self.metric_prefix}llm_label\": eval_label,\n                    f\"{self.metric_prefix}llm_score\": eval_score,\n                    f\"{self.metric_prefix}llm_label_input\": eval_in,\n                    f\"{self.metric_prefix}llm_label_output\": eval_out.text,\n                }\n                for eval_label, eval_score, eval_in, eval_out in zip(\n                    evaluator_label_list,\n                    evaluator_score_list,\n                    evaluator_input_list,\n                    evaluator_output_list,\n                )\n            ],\n        )\n\n    def cleanup_resources(self) -&gt; None:\n        self.language_model.cleanup_resources()\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.label_names","title":"label_names  <code>instance-attribute</code>","text":"<pre><code>label_names = [escape(label) for label in label_names]\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.weights","title":"weights  <code>instance-attribute</code>","text":"<pre><code>weights = label_points\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.disable_tqdm","title":"disable_tqdm  <code>instance-attribute</code>","text":"<pre><code>disable_tqdm = disable_tqdm\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.valid_score_range","title":"valid_score_range  <code>instance-attribute</code>","text":"<pre><code>valid_score_range = valid_score_range\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.metric_prefix","title":"metric_prefix  <code>instance-attribute</code>","text":"<pre><code>metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.__init__","title":"__init__","text":"<pre><code>__init__(\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    label_names: list[str],\n    label_points: list[float | int] | None = None,\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    valid_score_range: tuple[int, int] | None = None,\n    category_key: str | None = None,\n    metric_prefix: str | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    label_names: list[str],\n    label_points: list[float | int] | None = None,\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    valid_score_range: tuple[int, int] | None = None,\n    category_key: str | None = None,\n    metric_prefix: str | None = None,\n) -&gt; None:\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.label_names = [re.escape(label) for label in label_names]\n\n    if label_points:\n        if len(self.label_names) != len(label_points):\n            msg = \"The lengths of label_names and weights do not match.\"\n            raise ValueError(msg)\n        label_points: list[float] = list(map(float, label_points))\n    else:\n        label_points = [0.0] * len(label_names)\n        label_points[0] = 1.0\n\n    self.weights = label_points\n    self.batch_size = batch_size\n    self.disable_tqdm = disable_tqdm\n    self.valid_score_range = valid_score_range\n    self.category_key = category_key\n    self.metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if extra_info_list is None:\n        extra_info_list = [{} for _ in lm_outputs]\n    if references_list is None:\n        references_list = [[] for _ in lm_outputs]\n\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    # Extract text from LMOutput objects\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Compute metrics\n    evaluator_input_list: list[str] = prepare_text_input_for_evaluator(\n        lm_outputs, references_list, extra_info_list, self.prompt_template\n    )\n    evaluator_output_list: list[LMOutput] = generate_evaluations(\n        evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating LLM score\"\n    )\n\n    evaluator_label_list: list[int | None] = []\n    for evaluator_output in evaluator_output_list:\n        evaluator_label = parse_label_from_evaluator_output(\n            evaluator_output.text,\n            label_names=self.label_names,\n        )\n        if evaluator_label is None:\n            logger.warning(f\"Failed to parse label from evaluator output: {evaluator_output}\")\n        evaluator_label_list.append(evaluator_label)\n\n    label2point = dict(zip(self.label_names, self.weights))\n    evaluator_score_list: list[float | None] = [label2point.get(label) for label in evaluator_label_list]\n\n    summary = summarize_evaluator_labels(\n        evaluator_label_list,\n        extra_info_list,\n        self.label_names,\n        self.weights,\n        self.category_key,\n    )\n\n    return MetricResult(\n        {self.metric_prefix + key: value for key, value in summary.items()},\n        instance_details=[\n            {\n                f\"{self.metric_prefix}llm_label\": eval_label,\n                f\"{self.metric_prefix}llm_score\": eval_score,\n                f\"{self.metric_prefix}llm_label_input\": eval_in,\n                f\"{self.metric_prefix}llm_label_output\": eval_out.text,\n            }\n            for eval_label, eval_score, eval_in, eval_out in zip(\n                evaluator_label_list,\n                evaluator_score_list,\n                evaluator_input_list,\n                evaluator_output_list,\n            )\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.cleanup_resources","title":"cleanup_resources","text":"<pre><code>cleanup_resources() -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def cleanup_resources(self) -&gt; None:\n    self.language_model.cleanup_resources()\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_label.LLMLabel.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/llm_label.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return (\n        f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore","title":"ChatLLMScore","text":"<p>A metric that evaluates the output of <code>LanguageModel.batch_generate_chat_response</code>.</p> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>LanguageModel</code>)           \u2013            <p>An instance of <code>LanguageModel</code> to evaluate the output of the model.</p> </li> <li> <code>prompt_template</code>               (<code>PromptTemplate</code>)           \u2013            <p>An instance of <code>PromptTemplate</code> to embed the input for the evaluator.</p> </li> <li> <code>system_message</code>               (<code>str | PromptTemplate | None</code>, default:                   <code>None</code> )           \u2013            <p>A system message to be prepended to the input for the evaluator.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The batch size for the evaluator.</p> </li> <li> <code>disable_tqdm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to disable the progress bar.</p> </li> <li> <code>valid_score_range</code>               (<code>tuple[int, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>A tuple of two integers representing the valid score range. If the parsed score is out of the range, it will be ignored.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in extra_info.</p> </li> <li> <code>metric_prefix</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A prefix to be added to the metric keys in the summary and instance details.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import ChatLLMScore, OpenAIChatAPI, Jinja2PromptTemplate\n&gt;&gt;&gt; language_model = OpenAIChatAPI(model_name=\"gpt-3.5-turbo\")\n&gt;&gt;&gt; template = \"Evaluate the quality of this text.\\n`{{ lm_output }}`\\nPut the score at the end like [[5]].\"\n&gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n&gt;&gt;&gt; system_message = \"This is the system message.\"\n&gt;&gt;&gt; llm_score = ChatLLMScore(language_model, prompt_template, system_message)\n&gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n&gt;&gt;&gt; result = llm_score.evaluate(lm_outputs)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'llm_score': 3.0, 'num_failed_score_parses': 0},\n    instance_details=[\n        {\n            'llm_score': 2,\n            'llm_score_input': [{'role': 'user', 'content': 'Evaluate the quality of this text...'}],\n            'llm_score_output': 'This text is very simple,... Therefore, its quality is average. [[2]]'},\n        {\n            'llm_score': 4,\n            'llm_score_input': [{'role': 'user', 'content': 'Evaluate the quality of this text...'}],\n            'llm_score_output': '... Overall, the quality of the text is good but basic. [[4]]'}\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>class ChatLLMScore(Metric):\n    \"\"\"\n    A metric that evaluates the output of `LanguageModel.batch_generate_chat_response`.\n\n    Args:\n        language_model: An instance of `LanguageModel` to evaluate the output of the model.\n        prompt_template: An instance of `PromptTemplate` to embed the input for the evaluator.\n        system_message: A system message to be prepended to the input for the evaluator.\n        batch_size: The batch size for the evaluator.\n        disable_tqdm: Whether to disable the progress bar.\n        valid_score_range: A tuple of two integers representing the valid score range.\n            If the parsed score is out of the range, it will be ignored.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in extra_info.\n        metric_prefix: A prefix to be added to the metric keys in the summary and instance details.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import ChatLLMScore, OpenAIChatAPI, Jinja2PromptTemplate\n        &gt;&gt;&gt; language_model = OpenAIChatAPI(model_name=\"gpt-3.5-turbo\")\n        &gt;&gt;&gt; template = \"Evaluate the quality of this text.\\\\n`{{ lm_output }}`\\\\nPut the score at the end like [[5]].\"\n        &gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n        &gt;&gt;&gt; system_message = \"This is the system message.\"\n        &gt;&gt;&gt; llm_score = ChatLLMScore(language_model, prompt_template, system_message)\n        &gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n        &gt;&gt;&gt; result = llm_score.evaluate(lm_outputs)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'llm_score': 3.0, 'num_failed_score_parses': 0},\n            instance_details=[\n                {\n                    'llm_score': 2,\n                    'llm_score_input': [{'role': 'user', 'content': 'Evaluate the quality of this text...'}],\n                    'llm_score_output': 'This text is very simple,... Therefore, its quality is average. [[2]]'},\n                {\n                    'llm_score': 4,\n                    'llm_score_input': [{'role': 'user', 'content': 'Evaluate the quality of this text...'}],\n                    'llm_score_output': '... Overall, the quality of the text is good but basic. [[4]]'}\n            ]\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        system_message: str | PromptTemplate | None = None,\n        batch_size: int = 4,\n        disable_tqdm: bool = False,\n        valid_score_range: tuple[int, int] | None = None,\n        category_key: str | None = None,\n        metric_prefix: str | None = None,\n    ) -&gt; None:\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.system_message = system_message\n        self.batch_size = batch_size\n        self.disable_tqdm = disable_tqdm\n        self.valid_score_range = valid_score_range\n        self.category_key = category_key\n        self.metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]] | None = None,\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if extra_info_list is None:\n            extra_info_list = [{} for _ in lm_outputs]\n        if references_list is None:\n            references_list = [[] for _ in lm_outputs]\n\n        # Extract text from LMOutput objects\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Compute metrics\n        evaluator_input_list = prepare_chat_input_for_evaluator(\n            lm_outputs, references_list, extra_info_list, self.prompt_template, self.system_message\n        )\n        evaluator_output_list: list[LMOutput] = generate_evaluations(\n            evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating ChatLLM score\"\n        )\n\n        evaluator_score_list: list[int] = []\n        for evaluator_output in evaluator_output_list:\n            evaluator_score = parse_score_from_evaluator_output(\n                evaluator_output.text,\n                valid_score_range=self.valid_score_range,\n            )\n            if evaluator_score is None:\n                logger.warning(f\"Failed to parse score from evaluator output: {evaluator_output}\")\n            evaluator_score_list.append(evaluator_score)\n\n        summary = summarize_evaluator_scores(\n            evaluator_score_list,\n            extra_info_list,\n            self.category_key,\n        )\n\n        return MetricResult(\n            {self.metric_prefix + key: value for key, value in summary.items()},\n            instance_details=[\n                {\n                    f\"{self.metric_prefix}llm_score\": eval_score,\n                    f\"{self.metric_prefix}llm_score_input\": eval_in,\n                    f\"{self.metric_prefix}llm_score_output\": eval_out.text,\n                }\n                for eval_score, eval_in, eval_out in zip(\n                    evaluator_score_list,\n                    evaluator_input_list,\n                    evaluator_output_list,\n                )\n            ],\n        )\n\n    def cleanup_resources(self) -&gt; None:\n        self.language_model.cleanup_resources()\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.system_message","title":"system_message  <code>instance-attribute</code>","text":"<pre><code>system_message = system_message\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.disable_tqdm","title":"disable_tqdm  <code>instance-attribute</code>","text":"<pre><code>disable_tqdm = disable_tqdm\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.valid_score_range","title":"valid_score_range  <code>instance-attribute</code>","text":"<pre><code>valid_score_range = valid_score_range\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.metric_prefix","title":"metric_prefix  <code>instance-attribute</code>","text":"<pre><code>metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.__init__","title":"__init__","text":"<pre><code>__init__(\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    system_message: str | PromptTemplate | None = None,\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    valid_score_range: tuple[int, int] | None = None,\n    category_key: str | None = None,\n    metric_prefix: str | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    system_message: str | PromptTemplate | None = None,\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    valid_score_range: tuple[int, int] | None = None,\n    category_key: str | None = None,\n    metric_prefix: str | None = None,\n) -&gt; None:\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.system_message = system_message\n    self.batch_size = batch_size\n    self.disable_tqdm = disable_tqdm\n    self.valid_score_range = valid_score_range\n    self.category_key = category_key\n    self.metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if extra_info_list is None:\n        extra_info_list = [{} for _ in lm_outputs]\n    if references_list is None:\n        references_list = [[] for _ in lm_outputs]\n\n    # Extract text from LMOutput objects\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Compute metrics\n    evaluator_input_list = prepare_chat_input_for_evaluator(\n        lm_outputs, references_list, extra_info_list, self.prompt_template, self.system_message\n    )\n    evaluator_output_list: list[LMOutput] = generate_evaluations(\n        evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating ChatLLM score\"\n    )\n\n    evaluator_score_list: list[int] = []\n    for evaluator_output in evaluator_output_list:\n        evaluator_score = parse_score_from_evaluator_output(\n            evaluator_output.text,\n            valid_score_range=self.valid_score_range,\n        )\n        if evaluator_score is None:\n            logger.warning(f\"Failed to parse score from evaluator output: {evaluator_output}\")\n        evaluator_score_list.append(evaluator_score)\n\n    summary = summarize_evaluator_scores(\n        evaluator_score_list,\n        extra_info_list,\n        self.category_key,\n    )\n\n    return MetricResult(\n        {self.metric_prefix + key: value for key, value in summary.items()},\n        instance_details=[\n            {\n                f\"{self.metric_prefix}llm_score\": eval_score,\n                f\"{self.metric_prefix}llm_score_input\": eval_in,\n                f\"{self.metric_prefix}llm_score_output\": eval_out.text,\n            }\n            for eval_score, eval_in, eval_out in zip(\n                evaluator_score_list,\n                evaluator_input_list,\n                evaluator_output_list,\n            )\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.cleanup_resources","title":"cleanup_resources","text":"<pre><code>cleanup_resources() -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def cleanup_resources(self) -&gt; None:\n    self.language_model.cleanup_resources()\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.ChatLLMScore.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return (\n        f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore","title":"LLMScore","text":"<p>Let LanguageModel to evaluate the output of another LanguageModel.</p> <p>You can specify the evaluation criteria in <code>PromptTemplate</code>. The last integer value in the output of the evaluator is used as the evaluation score.</p> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>LanguageModel</code>)           \u2013            <p>An instance of <code>LanguageModel</code> to evaluate the output of the model.</p> </li> <li> <code>prompt_template</code>               (<code>PromptTemplate</code>)           \u2013            <p>An instance of <code>PromptTemplate</code> to embed the input for the evaluator.</p> </li> <li> <code>batch_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The batch size for the evaluator.</p> </li> <li> <code>disable_tqdm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to disable the progress bar.</p> </li> <li> <code>valid_score_range</code>               (<code>tuple[int, int] | None</code>, default:                   <code>None</code> )           \u2013            <p>A tuple of two integers representing the valid score range. If the parsed score is out of the range, it will be ignored.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in extra_info.</p> </li> <li> <code>metric_prefix</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A prefix to be added to the metric keys in the summary and instance details.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import LLMScore, OpenAIChatAPI, Jinja2PromptTemplate\n&gt;&gt;&gt; language_model = OpenAIChatAPI(model_name=\"gpt-3.5-turbo\")\n&gt;&gt;&gt; template = \"Evaluate the quality of this text.\\n`{{ lm_output }}`\\nPut the score at the end like [[5]].\"\n&gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n&gt;&gt;&gt; llm_score = LLMScore(language_model, prompt_template)\n&gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n&gt;&gt;&gt; result = llm_score.evaluate(lm_outputs)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'llm_score': 3.0, 'num_failed_score_parses': 0},\n    instance_details=[\n        {\n            'llm_score': 2,\n            'llm_score_input': 'Evaluate the quality of this text...',\n            'llm_score_output': 'This text is very simple,... Therefore, its quality is average. [[2]]'},\n        {\n            'llm_score': 4,\n            'llm_score_input': 'Evaluate the quality of this text...',\n            'llm_score_output': '... Overall, the quality of the text is good but basic. [[4]]'}\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>class LLMScore(Metric):\n    \"\"\"Let LanguageModel to evaluate the output of another LanguageModel.\n\n    You can specify the evaluation criteria in `PromptTemplate`.\n    The last integer value in the output of the evaluator is used as the evaluation score.\n\n    Args:\n        language_model: An instance of `LanguageModel` to evaluate the output of the model.\n        prompt_template: An instance of `PromptTemplate` to embed the input for the evaluator.\n        batch_size: The batch size for the evaluator.\n        disable_tqdm: Whether to disable the progress bar.\n        valid_score_range: A tuple of two integers representing the valid score range.\n            If the parsed score is out of the range, it will be ignored.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in extra_info.\n        metric_prefix: A prefix to be added to the metric keys in the summary and instance details.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import LLMScore, OpenAIChatAPI, Jinja2PromptTemplate\n        &gt;&gt;&gt; language_model = OpenAIChatAPI(model_name=\"gpt-3.5-turbo\")\n        &gt;&gt;&gt; template = \"Evaluate the quality of this text.\\\\n`{{ lm_output }}`\\\\nPut the score at the end like [[5]].\"\n        &gt;&gt;&gt; prompt_template = Jinja2PromptTemplate(template)\n        &gt;&gt;&gt; llm_score = LLMScore(language_model, prompt_template)\n        &gt;&gt;&gt; lm_outputs = [\"Hello, world!\", \"Good morning!\"]\n        &gt;&gt;&gt; result = llm_score.evaluate(lm_outputs)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'llm_score': 3.0, 'num_failed_score_parses': 0},\n            instance_details=[\n                {\n                    'llm_score': 2,\n                    'llm_score_input': 'Evaluate the quality of this text...',\n                    'llm_score_output': 'This text is very simple,... Therefore, its quality is average. [[2]]'},\n                {\n                    'llm_score': 4,\n                    'llm_score_input': 'Evaluate the quality of this text...',\n                    'llm_score_output': '... Overall, the quality of the text is good but basic. [[4]]'}\n            ]\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        batch_size: int = 4,\n        disable_tqdm: bool = False,\n        valid_score_range: tuple[int, int] | None = None,\n        category_key: str | None = None,\n        metric_prefix: str | None = None,\n    ) -&gt; None:\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.batch_size = batch_size\n        self.disable_tqdm = disable_tqdm\n        self.valid_score_range = valid_score_range\n        self.category_key = category_key\n        self.metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]] | None = None,\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if extra_info_list is None:\n            extra_info_list = [{} for _ in lm_outputs]\n        if references_list is None:\n            references_list = [[] for _ in lm_outputs]\n\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        # Extract text from LMOutput objects\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Compute metrics\n        evaluator_input_list: list[str] = prepare_text_input_for_evaluator(\n            lm_outputs, references_list, extra_info_list, self.prompt_template\n        )\n        evaluator_output_list: list[LMOutput] = generate_evaluations(\n            evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating LLM score\"\n        )\n\n        evaluator_score_list: list[int | None] = []\n        for evaluator_output in evaluator_output_list:\n            evaluator_score = parse_score_from_evaluator_output(\n                evaluator_output.text,\n                valid_score_range=self.valid_score_range,\n            )\n            if evaluator_score is None:\n                logger.warning(f\"Failed to parse score from evaluator output: {evaluator_output}\")\n            evaluator_score_list.append(evaluator_score)\n\n        summary = summarize_evaluator_scores(\n            evaluator_score_list,\n            extra_info_list,\n            self.category_key,\n        )\n\n        return MetricResult(\n            {self.metric_prefix + key: value for key, value in summary.items()},\n            instance_details=[\n                {\n                    f\"{self.metric_prefix}llm_score\": eval_score,\n                    f\"{self.metric_prefix}llm_score_input\": eval_in,\n                    f\"{self.metric_prefix}llm_score_output\": eval_out.text,\n                }\n                for eval_score, eval_in, eval_out in zip(\n                    evaluator_score_list,\n                    evaluator_input_list,\n                    evaluator_output_list,\n                )\n            ],\n        )\n\n    def cleanup_resources(self) -&gt; None:\n        self.language_model.cleanup_resources()\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.batch_size","title":"batch_size  <code>instance-attribute</code>","text":"<pre><code>batch_size = batch_size\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.disable_tqdm","title":"disable_tqdm  <code>instance-attribute</code>","text":"<pre><code>disable_tqdm = disable_tqdm\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.valid_score_range","title":"valid_score_range  <code>instance-attribute</code>","text":"<pre><code>valid_score_range = valid_score_range\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.metric_prefix","title":"metric_prefix  <code>instance-attribute</code>","text":"<pre><code>metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.__init__","title":"__init__","text":"<pre><code>__init__(\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    valid_score_range: tuple[int, int] | None = None,\n    category_key: str | None = None,\n    metric_prefix: str | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    batch_size: int = 4,\n    disable_tqdm: bool = False,\n    valid_score_range: tuple[int, int] | None = None,\n    category_key: str | None = None,\n    metric_prefix: str | None = None,\n) -&gt; None:\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.batch_size = batch_size\n    self.disable_tqdm = disable_tqdm\n    self.valid_score_range = valid_score_range\n    self.category_key = category_key\n    self.metric_prefix = f\"{metric_prefix}-\" if metric_prefix else \"\"\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if extra_info_list is None:\n        extra_info_list = [{} for _ in lm_outputs]\n    if references_list is None:\n        references_list = [[] for _ in lm_outputs]\n\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    # Extract text from LMOutput objects\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Compute metrics\n    evaluator_input_list: list[str] = prepare_text_input_for_evaluator(\n        lm_outputs, references_list, extra_info_list, self.prompt_template\n    )\n    evaluator_output_list: list[LMOutput] = generate_evaluations(\n        evaluator_input_list, self.language_model, self.batch_size, self.disable_tqdm, \"Calculating LLM score\"\n    )\n\n    evaluator_score_list: list[int | None] = []\n    for evaluator_output in evaluator_output_list:\n        evaluator_score = parse_score_from_evaluator_output(\n            evaluator_output.text,\n            valid_score_range=self.valid_score_range,\n        )\n        if evaluator_score is None:\n            logger.warning(f\"Failed to parse score from evaluator output: {evaluator_output}\")\n        evaluator_score_list.append(evaluator_score)\n\n    summary = summarize_evaluator_scores(\n        evaluator_score_list,\n        extra_info_list,\n        self.category_key,\n    )\n\n    return MetricResult(\n        {self.metric_prefix + key: value for key, value in summary.items()},\n        instance_details=[\n            {\n                f\"{self.metric_prefix}llm_score\": eval_score,\n                f\"{self.metric_prefix}llm_score_input\": eval_in,\n                f\"{self.metric_prefix}llm_score_output\": eval_out.text,\n            }\n            for eval_score, eval_in, eval_out in zip(\n                evaluator_score_list,\n                evaluator_input_list,\n                evaluator_output_list,\n            )\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.cleanup_resources","title":"cleanup_resources","text":"<pre><code>cleanup_resources() -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def cleanup_resources(self) -&gt; None:\n    self.language_model.cleanup_resources()\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.llm_score.LLMScore.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/metric/llm_score.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return (\n        f\"{self.__class__.__name__}(language_model={self.language_model}, prompt_template={self.prompt_template})\"\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.math.MathVerify","title":"MathVerify","text":"<p>The proportion of correct answers verified by <code>math-verify</code> https://github.com/huggingface/Math-Verify If there are multiple references, the output is considered correct if it matches any of the references. Args:     lm_output_processor:         StringProcessor or a list of StringProcessors to be applied to the model outputs before comparison.     reference_processor: StringProcessor or list of Normalizers to apply to the references before comparison. Examples:     &gt;&gt;&gt; from flexeval.core.metric import MathVerify     &gt;&gt;&gt; math_verify = MathVerify()     &gt;&gt;&gt; lm_outputs = [\"The answer is $a+b$\", \"The answer: $1$\"]     &gt;&gt;&gt; references_list = [[\"Final answer: $b+a$\"], [\"A: $2$\"]]     &gt;&gt;&gt; result = math_verify.evaluate(lm_outputs, references_list)     &gt;&gt;&gt; print(result)     MetricResult(         summary={\"verified\": 0.5},         instance_details=[{\"verified\": True}, {\"verified\": False}],     )</p> Source code in <code>flexeval/core/metric/math.py</code> <pre><code>class MathVerify(Metric):\n    \"\"\"\n    The proportion of correct answers verified by `math-verify` https://github.com/huggingface/Math-Verify\n    If there are multiple references, the output is considered correct if it matches any of the references.\n    Args:\n        lm_output_processor:\n            StringProcessor or a list of StringProcessors to be applied to the model outputs before comparison.\n        reference_processor: StringProcessor or list of Normalizers to apply to the references before comparison.\n    Examples:\n        &gt;&gt;&gt; from flexeval.core.metric import MathVerify\n        &gt;&gt;&gt; math_verify = MathVerify()\n        &gt;&gt;&gt; lm_outputs = [\"The answer is $a+b$\", \"The answer: $1$\"]\n        &gt;&gt;&gt; references_list = [[\"Final answer: $b+a$\"], [\"A: $2$\"]]\n        &gt;&gt;&gt; result = math_verify.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={\"verified\": 0.5},\n            instance_details=[{\"verified\": True}, {\"verified\": False}],\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n        reference_processor: StringProcessor | list[StringProcessor] | None = None,\n    ) -&gt; None:\n        if isinstance(lm_output_processor, StringProcessor):\n            lm_output_processor = [lm_output_processor]\n        if isinstance(reference_processor, StringProcessor):\n            reference_processor = [reference_processor]\n\n        self.lm_output_processors = lm_output_processor\n        self.reference_processors = reference_processor\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        if len(lm_outputs) != len(references_list):\n            msg = (\n                f\"Number of model outputs ({len(lm_outputs)}) and number of references ({len(references_list)}) \"\n                \"should be the same.\"\n            )\n            raise ValueError(msg)\n\n        # Extract text from LMOutput objects\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        if self.lm_output_processors:\n            lm_outputs = [\n                functools.reduce(lambda x, norm: norm(x), self.lm_output_processors, output) for output in lm_outputs\n            ]\n\n        if self.reference_processors:\n            references_list = [\n                [functools.reduce(lambda x, norm: norm(x), self.reference_processors, ref) for ref in references]\n                for references in references_list\n            ]\n\n        verified_list: list[bool] = []\n        extracted_answers: list[list] = []\n        for lm_output, expected_outputs in zip(lm_outputs, references_list):\n            is_correct = False\n            answer = []\n            for expected_output in expected_outputs:\n                try:\n                    expected = math_verify.parse(expected_output)\n                    answer = math_verify.parse(lm_output)\n                    verified = math_verify.verify(expected, answer)\n                except Exception as e:  # noqa: BLE001\n                    warnings.warn(f\"Could not run math_verify on {lm_output} : {e}\", stacklevel=1)\n                    answer = [\"n/a\"]\n                    verified = False\n                is_correct |= verified\n            verified_list.append(is_correct)\n            extracted_answers.append(answer)\n\n        return MetricResult(\n            {\"math_verify_accuracy\": sum(verified_list) / len(verified_list)},\n            instance_details=[\n                {\"math_verify_match\": s, \"extracted_answer\": ans} for s, ans in zip(verified_list, extracted_answers)\n            ],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.math.MathVerify.lm_output_processors","title":"lm_output_processors  <code>instance-attribute</code>","text":"<pre><code>lm_output_processors = lm_output_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.math.MathVerify.reference_processors","title":"reference_processors  <code>instance-attribute</code>","text":"<pre><code>reference_processors = reference_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.math.MathVerify.__init__","title":"__init__","text":"<pre><code>__init__(\n    lm_output_processor: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n    reference_processor: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/math.py</code> <pre><code>def __init__(\n    self,\n    lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n    reference_processor: StringProcessor | list[StringProcessor] | None = None,\n) -&gt; None:\n    if isinstance(lm_output_processor, StringProcessor):\n        lm_output_processor = [lm_output_processor]\n    if isinstance(reference_processor, StringProcessor):\n        reference_processor = [reference_processor]\n\n    self.lm_output_processors = lm_output_processor\n    self.reference_processors = reference_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.math.MathVerify.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/math.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    if len(lm_outputs) != len(references_list):\n        msg = (\n            f\"Number of model outputs ({len(lm_outputs)}) and number of references ({len(references_list)}) \"\n            \"should be the same.\"\n        )\n        raise ValueError(msg)\n\n    # Extract text from LMOutput objects\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    if self.lm_output_processors:\n        lm_outputs = [\n            functools.reduce(lambda x, norm: norm(x), self.lm_output_processors, output) for output in lm_outputs\n        ]\n\n    if self.reference_processors:\n        references_list = [\n            [functools.reduce(lambda x, norm: norm(x), self.reference_processors, ref) for ref in references]\n            for references in references_list\n        ]\n\n    verified_list: list[bool] = []\n    extracted_answers: list[list] = []\n    for lm_output, expected_outputs in zip(lm_outputs, references_list):\n        is_correct = False\n        answer = []\n        for expected_output in expected_outputs:\n            try:\n                expected = math_verify.parse(expected_output)\n                answer = math_verify.parse(lm_output)\n                verified = math_verify.verify(expected, answer)\n            except Exception as e:  # noqa: BLE001\n                warnings.warn(f\"Could not run math_verify on {lm_output} : {e}\", stacklevel=1)\n                answer = [\"n/a\"]\n                verified = False\n            is_correct |= verified\n        verified_list.append(is_correct)\n        extracted_answers.append(answer)\n\n    return MetricResult(\n        {\"math_verify_accuracy\": sum(verified_list) / len(verified_list)},\n        instance_details=[\n            {\"math_verify_match\": s, \"extracted_answer\": ans} for s, ans in zip(verified_list, extracted_answers)\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.output_length_stats.OutputLengthStats","title":"OutputLengthStats","text":"<p>Compute statistics on the length of the outputs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import OutputLengthStats\n&gt;&gt;&gt; output_length_stats = OutputLengthStats()\n&gt;&gt;&gt; lm_outputs = [\"123456\", \"123456789\"]\n&gt;&gt;&gt; result = output_length_stats.evaluate(lm_outputs)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'avg_output_length': 7.5, 'max_output_length': 9, 'min_output_length': 6},\n    instance_details=[{'output_length': 6}, {'output_length': 9}]\n)\n</code></pre> Source code in <code>flexeval/core/metric/output_length_stats.py</code> <pre><code>class OutputLengthStats(Metric):\n    \"\"\"\n    Compute statistics on the length of the outputs.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import OutputLengthStats\n        &gt;&gt;&gt; output_length_stats = OutputLengthStats()\n        &gt;&gt;&gt; lm_outputs = [\"123456\", \"123456789\"]\n        &gt;&gt;&gt; result = output_length_stats.evaluate(lm_outputs)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'avg_output_length': 7.5, 'max_output_length': 9, 'min_output_length': 6},\n            instance_details=[{'output_length': 6}, {'output_length': 9}]\n        )\n    \"\"\"\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]] | None = None,\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        # Extract text from LMOutput objects\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Compute metrics\n        output_length_list = [len(output) for output in lm_outputs]\n        return MetricResult(\n            {\n                \"avg_output_length\": sum(output_length_list) / len(output_length_list),\n                \"max_output_length\": max(output_length_list),\n                \"min_output_length\": min(output_length_list),\n            },\n            instance_details=[{\"output_length\": s} for s in output_length_list],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.output_length_stats.OutputLengthStats.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/output_length_stats.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    # Extract text from LMOutput objects\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Compute metrics\n    output_length_list = [len(output) for output in lm_outputs]\n    return MetricResult(\n        {\n            \"avg_output_length\": sum(output_length_list) / len(output_length_list),\n            \"max_output_length\": max(output_length_list),\n            \"min_output_length\": min(output_length_list),\n        },\n        instance_details=[{\"output_length\": s} for s in output_length_list],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.perspective_api.PerspectiveAPI","title":"PerspectiveAPI","text":"<p>A metric that evaluates text outputs using the Perspective API. Please set <code>PERSPECTIVE_API_KEY</code> in the environment variable.</p> <p>Parameters:</p> <ul> <li> <code>languages</code>               (<code>list[str]</code>)           \u2013            <p>A list of languages to analyze.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import PerspectiveAPI\n&gt;&gt;&gt; perspective_api = PerspectiveAPI(languages=[\"en\"])\n&gt;&gt;&gt; lm_outputs = [\"I love you\", \"I hate you\"]\n&gt;&gt;&gt; result = perspective_api.evaluate(lm_outputs)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'TOXICITY': 0.35407552, ..., 'THREAT': 0.0265799825},\n    instance_details=[\n        {'TOXICITY': 0.02543884, ..., 'THREAT': 0.009204263},\n        {'TOXICITY': 0.6827122, ..., 'THREAT': 0.043955702}\n        ]\n    )\n</code></pre> Source code in <code>flexeval/core/metric/perspective_api.py</code> <pre><code>class PerspectiveAPI(Metric):\n    \"\"\"A metric that evaluates text outputs using the Perspective API.\n    Please set `PERSPECTIVE_API_KEY` in the environment variable.\n\n    Args:\n        languages: A list of languages to analyze.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import PerspectiveAPI\n        &gt;&gt;&gt; perspective_api = PerspectiveAPI(languages=[\"en\"])\n        &gt;&gt;&gt; lm_outputs = [\"I love you\", \"I hate you\"]\n        &gt;&gt;&gt; result = perspective_api.evaluate(lm_outputs)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'TOXICITY': 0.35407552, ..., 'THREAT': 0.0265799825},\n            instance_details=[\n                {'TOXICITY': 0.02543884, ..., 'THREAT': 0.009204263},\n                {'TOXICITY': 0.6827122, ..., 'THREAT': 0.043955702}\n                ]\n            )\n    \"\"\"\n\n    def __init__(self, languages: list[str]) -&gt; None:\n        self.client = discovery.build(\n            \"commentanalyzer\",\n            \"v1alpha1\",\n            developerKey=PERSPECTIVE_API_KEY,\n            discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n            static_discovery=False,\n        )\n        self.languages = languages\n        self.attributes = [\"TOXICITY\", \"SEVERE_TOXICITY\", \"IDENTITY_ATTACK\", \"INSULT\", \"PROFANITY\", \"THREAT\"]\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]] | None = None,\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        # Extract text from LMOutput objects\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Compute metrics\n        instance_details = []\n        for lm_output in lm_outputs:\n            if lm_output == \"\":\n                instance_details.append({att: 0.0 for att in self.attributes})\n                continue\n            analyze_request = {\n                \"comment\": {\"text\": lm_output},\n                \"languages\": self.languages,\n                \"requestedAttributes\": {att: {} for att in self.attributes},\n            }\n            response = retry_on_error(perspectiveapi_call=self.client.comments().analyze(body=analyze_request).execute)\n            instance_details.append(\n                {att: response[\"attributeScores\"][att][\"summaryScore\"][\"value\"] for att in self.attributes},\n            )\n        scores_for_attribute = {att: [] for att in self.attributes}\n        for instance in instance_details:\n            for att in self.attributes:\n                scores_for_attribute[att].append(instance[att])\n        average_scores = {att: np.mean(scores_for_attribute[att]) for att in self.attributes}\n        return MetricResult(average_scores, instance_details=instance_details)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.perspective_api.PerspectiveAPI.client","title":"client  <code>instance-attribute</code>","text":"<pre><code>client = build(\n    \"commentanalyzer\",\n    \"v1alpha1\",\n    developerKey=PERSPECTIVE_API_KEY,\n    discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n    static_discovery=False,\n)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.perspective_api.PerspectiveAPI.languages","title":"languages  <code>instance-attribute</code>","text":"<pre><code>languages = languages\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.perspective_api.PerspectiveAPI.attributes","title":"attributes  <code>instance-attribute</code>","text":"<pre><code>attributes = [\n    \"TOXICITY\",\n    \"SEVERE_TOXICITY\",\n    \"IDENTITY_ATTACK\",\n    \"INSULT\",\n    \"PROFANITY\",\n    \"THREAT\",\n]\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.perspective_api.PerspectiveAPI.__init__","title":"__init__","text":"<pre><code>__init__(languages: list[str]) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/perspective_api.py</code> <pre><code>def __init__(self, languages: list[str]) -&gt; None:\n    self.client = discovery.build(\n        \"commentanalyzer\",\n        \"v1alpha1\",\n        developerKey=PERSPECTIVE_API_KEY,\n        discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n        static_discovery=False,\n    )\n    self.languages = languages\n    self.attributes = [\"TOXICITY\", \"SEVERE_TOXICITY\", \"IDENTITY_ATTACK\", \"INSULT\", \"PROFANITY\", \"THREAT\"]\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.perspective_api.PerspectiveAPI.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/perspective_api.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]] | None = None,\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    # Extract text from LMOutput objects\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Compute metrics\n    instance_details = []\n    for lm_output in lm_outputs:\n        if lm_output == \"\":\n            instance_details.append({att: 0.0 for att in self.attributes})\n            continue\n        analyze_request = {\n            \"comment\": {\"text\": lm_output},\n            \"languages\": self.languages,\n            \"requestedAttributes\": {att: {} for att in self.attributes},\n        }\n        response = retry_on_error(perspectiveapi_call=self.client.comments().analyze(body=analyze_request).execute)\n        instance_details.append(\n            {att: response[\"attributeScores\"][att][\"summaryScore\"][\"value\"] for att in self.attributes},\n        )\n    scores_for_attribute = {att: [] for att in self.attributes}\n    for instance in instance_details:\n        for att in self.attributes:\n            scores_for_attribute[att].append(instance[att])\n    average_scores = {att: np.mean(scores_for_attribute[att]) for att in self.attributes}\n    return MetricResult(average_scores, instance_details=instance_details)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.repetition_count.RepetitionCount","title":"RepetitionCount","text":"<p>A metric that counts the number of repetitions of the most repeated pattern in the model's output.</p> <p>Parameters:</p> <ul> <li> <code>lm_output_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>None</code> )           \u2013            <p>StringProcessor or list of Normalizers to apply to the model outputs before analysis.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import RepetitionCount\n&gt;&gt;&gt; repetition_count = RepetitionCount()\n&gt;&gt;&gt; lm_outputs = [\"hello hello hello hello hello hello hello hello hello hello\"]\n&gt;&gt;&gt; references_list = [[]]  # Not used for this metric\n&gt;&gt;&gt; result = repetition_count.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'repetition_ratio': 1.0},\n    instance_details=[{'most_repeated_pattern': 'hello hell', 'repetition_count': 9, 'is_repetition': True}]\n)\n</code></pre> Source code in <code>flexeval/core/metric/repetition_count.py</code> <pre><code>class RepetitionCount(Metric):\n    \"\"\"\n    A metric that counts the number of repetitions of the most repeated pattern in the model's output.\n\n    Args:\n        lm_output_processor: StringProcessor or list of Normalizers to apply to the model outputs before analysis.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import RepetitionCount\n        &gt;&gt;&gt; repetition_count = RepetitionCount()\n        &gt;&gt;&gt; lm_outputs = [\"hello hello hello hello hello hello hello hello hello hello\"]\n        &gt;&gt;&gt; references_list = [[]]  # Not used for this metric\n        &gt;&gt;&gt; result = repetition_count.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'repetition_ratio': 1.0},\n            instance_details=[{'most_repeated_pattern': 'hello hell', 'repetition_count': 9, 'is_repetition': True}]\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        count_threshold: int = 30,\n        threshold_length: int = 10,\n        lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n    ) -&gt; None:\n        self.count_threshold = count_threshold\n        self.threshold_length = threshold_length\n        self.lm_output_processors = lm_output_processor\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],  # Not used in this metric\n        extra_info_list: list[dict[str, str]] | None = None,  # Not used in this metric\n    ) -&gt; MetricResult:\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        # Extract text from LMOutput objects\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Normalize text data\n        lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n\n        # Compute metrics\n        repetition_details: list[dict[str, Any]] = []\n        num_repetitions = 0\n        for output in lm_outputs:\n            most_repeated_pattern, count = get_most_repeated_pattern(output, threshold_length=self.threshold_length)\n            is_repetition = count &gt;= self.count_threshold\n            repetition_details.append(\n                {\n                    \"most_repeated_pattern\": most_repeated_pattern,\n                    \"repetition_count\": count,\n                    \"is_repetition\": is_repetition,\n                }\n            )\n            num_repetitions += int(is_repetition)\n\n        repetition_rate = num_repetitions / len(lm_outputs)\n\n        return MetricResult(\n            summary={\"repetition_ratio\": repetition_rate},\n            instance_details=repetition_details,\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.repetition_count.RepetitionCount.count_threshold","title":"count_threshold  <code>instance-attribute</code>","text":"<pre><code>count_threshold = count_threshold\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.repetition_count.RepetitionCount.threshold_length","title":"threshold_length  <code>instance-attribute</code>","text":"<pre><code>threshold_length = threshold_length\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.repetition_count.RepetitionCount.lm_output_processors","title":"lm_output_processors  <code>instance-attribute</code>","text":"<pre><code>lm_output_processors = lm_output_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.repetition_count.RepetitionCount.__init__","title":"__init__","text":"<pre><code>__init__(\n    count_threshold: int = 30,\n    threshold_length: int = 10,\n    lm_output_processor: StringProcessor\n    | list[StringProcessor]\n    | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/repetition_count.py</code> <pre><code>def __init__(\n    self,\n    count_threshold: int = 30,\n    threshold_length: int = 10,\n    lm_output_processor: StringProcessor | list[StringProcessor] | None = None,\n) -&gt; None:\n    self.count_threshold = count_threshold\n    self.threshold_length = threshold_length\n    self.lm_output_processors = lm_output_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.repetition_count.RepetitionCount.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/repetition_count.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],  # Not used in this metric\n    extra_info_list: list[dict[str, str]] | None = None,  # Not used in this metric\n) -&gt; MetricResult:\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    # Extract text from LMOutput objects\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Normalize text data\n    lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n\n    # Compute metrics\n    repetition_details: list[dict[str, Any]] = []\n    num_repetitions = 0\n    for output in lm_outputs:\n        most_repeated_pattern, count = get_most_repeated_pattern(output, threshold_length=self.threshold_length)\n        is_repetition = count &gt;= self.count_threshold\n        repetition_details.append(\n            {\n                \"most_repeated_pattern\": most_repeated_pattern,\n                \"repetition_count\": count,\n                \"is_repetition\": is_repetition,\n            }\n        )\n        num_repetitions += int(is_repetition)\n\n    repetition_rate = num_repetitions / len(lm_outputs)\n\n    return MetricResult(\n        summary={\"repetition_ratio\": repetition_rate},\n        instance_details=repetition_details,\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.rouge.ROUGE","title":"ROUGE","text":"<p>An implementation of ROUGE.</p> <p>The calculation is based on the rouge library.</p> <p>Parameters:</p> <ul> <li> <code>tokenizer</code>               (<code>Tokenizer</code>)           \u2013            <p>An instance of <code>Tokenizer</code> to tokenize the input and output strings.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import ROUGE\n&gt;&gt;&gt; from flexeval import WhitespaceTokenizer\n&gt;&gt;&gt; tokenizer = WhitespaceTokenizer()\n&gt;&gt;&gt; rouge = ROUGE(tokenizer)\n&gt;&gt;&gt; lm_outputs = [\"I am a student .\", \"I am a teacher .\"]\n&gt;&gt;&gt; references_list = [[\"I am a student .\", \"I am a learner .\"], [\"I am a teacher .\"]]\n&gt;&gt;&gt; result = rouge.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'rouge1': 0.999999995, 'rouge2': 0.999999995, 'rougeL': 0.999999995},\n    instance_details=[\n        {'rouge1': 0.999999995, 'rouge2': 0.999999995, 'rougeL': 0.999999995},\n        {'rouge1': 0.999999995, 'rouge2': 0.999999995, 'rougeL': 0.999999995}\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/rouge.py</code> <pre><code>class ROUGE(Metric):\n    \"\"\"An implementation of [ROUGE](https://aclanthology.org/W04-1013/).\n\n    The calculation is based on the [rouge](https://github.com/pltrdy/rouge) library.\n\n    Args:\n        tokenizer: An instance of `Tokenizer` to tokenize the input and output strings.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import ROUGE\n        &gt;&gt;&gt; from flexeval import WhitespaceTokenizer\n        &gt;&gt;&gt; tokenizer = WhitespaceTokenizer()\n        &gt;&gt;&gt; rouge = ROUGE(tokenizer)\n        &gt;&gt;&gt; lm_outputs = [\"I am a student .\", \"I am a teacher .\"]\n        &gt;&gt;&gt; references_list = [[\"I am a student .\", \"I am a learner .\"], [\"I am a teacher .\"]]\n        &gt;&gt;&gt; result = rouge.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'rouge1': 0.999999995, 'rouge2': 0.999999995, 'rougeL': 0.999999995},\n            instance_details=[\n                {'rouge1': 0.999999995, 'rouge2': 0.999999995, 'rougeL': 0.999999995},\n                {'rouge1': 0.999999995, 'rouge2': 0.999999995, 'rougeL': 0.999999995}\n            ]\n        )\n    \"\"\"\n\n    def __init__(self, tokenizer: Tokenizer) -&gt; None:\n        self._tokenizer = tokenizer\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        # Extract text from LMOutput objects\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Normalize text data - we only need the first reference\n        target_summaries = [references[0] for references in references_list]\n\n        tokenized_lm_outputs = [\" \".join(self._tokenizer.tokenize(lm_output)) for lm_output in lm_outputs]\n        tokenized_target_summaries = [\n            \" \".join(self._tokenizer.tokenize(target_summary)) for target_summary in target_summaries\n        ]\n\n        # replace empty string with \" \" to avoid \"ValueError: Hypothesis is empty\" from rouge\n        tokenized_lm_outputs = [o if o else \" \" for o in tokenized_lm_outputs]\n\n        # Compute metrics\n        rouge = RougeCalculator()\n        score_outputs = rouge.get_scores(\n            tokenized_lm_outputs,\n            tokenized_target_summaries,\n        )\n\n        rouge1_list = [o[\"rouge-1\"][\"f\"] for o in score_outputs]\n        rouge2_list = [o[\"rouge-2\"][\"f\"] for o in score_outputs]\n        rouge_l_list = [o[\"rouge-l\"][\"f\"] for o in score_outputs]\n\n        # we only need the f1 score\n        return MetricResult(\n            {\n                \"rouge1\": sum(rouge1_list) / len(rouge1_list),\n                \"rouge2\": sum(rouge2_list) / len(rouge2_list),\n                \"rougeL\": sum(rouge_l_list) / len(rouge_l_list),\n            },\n            instance_details=[\n                {\"rouge1\": r1, \"rouge2\": r2, \"rougeL\": rL} for r1, r2, rL in zip(rouge1_list, rouge2_list, rouge_l_list)\n            ],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.rouge.ROUGE.__init__","title":"__init__","text":"<pre><code>__init__(tokenizer: Tokenizer) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/rouge.py</code> <pre><code>def __init__(self, tokenizer: Tokenizer) -&gt; None:\n    self._tokenizer = tokenizer\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.rouge.ROUGE.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/rouge.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    # Extract text from LMOutput objects\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Normalize text data - we only need the first reference\n    target_summaries = [references[0] for references in references_list]\n\n    tokenized_lm_outputs = [\" \".join(self._tokenizer.tokenize(lm_output)) for lm_output in lm_outputs]\n    tokenized_target_summaries = [\n        \" \".join(self._tokenizer.tokenize(target_summary)) for target_summary in target_summaries\n    ]\n\n    # replace empty string with \" \" to avoid \"ValueError: Hypothesis is empty\" from rouge\n    tokenized_lm_outputs = [o if o else \" \" for o in tokenized_lm_outputs]\n\n    # Compute metrics\n    rouge = RougeCalculator()\n    score_outputs = rouge.get_scores(\n        tokenized_lm_outputs,\n        tokenized_target_summaries,\n    )\n\n    rouge1_list = [o[\"rouge-1\"][\"f\"] for o in score_outputs]\n    rouge2_list = [o[\"rouge-2\"][\"f\"] for o in score_outputs]\n    rouge_l_list = [o[\"rouge-l\"][\"f\"] for o in score_outputs]\n\n    # we only need the f1 score\n    return MetricResult(\n        {\n            \"rouge1\": sum(rouge1_list) / len(rouge1_list),\n            \"rouge2\": sum(rouge2_list) / len(rouge2_list),\n            \"rougeL\": sum(rouge_l_list) / len(rouge_l_list),\n        },\n        instance_details=[\n            {\"rouge1\": r1, \"rouge2\": r2, \"rougeL\": rL} for r1, r2, rL in zip(rouge1_list, rouge2_list, rouge_l_list)\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.sari.SARI","title":"SARI","text":"<p>An implementation of SARI, a metric for evaluating text simplification.</p> <p>Based on the original implementation [1], modified to allow configurable settings for the maximum n-gram size and tokenizer. Additionally, it fixes a bug present in the original implementation [2]. When used with the default parameters, it produces scores that are consistent with the HuggingFace/evaluate implementation [3].</p> <p>[1] https://github.com/cocoxu/simplification/blob/master/SARI.py [2] https://github.com/cocoxu/simplification/issues/6 [3] https://huggingface.co/spaces/evaluate-metric/sari/blob/main/sari.py</p> <p>Parameters:</p> <ul> <li> <code>tokenizer</code>               (<code>Tokenizer | Literal['default']</code>, default:                   <code>'default'</code> )           \u2013            <p>An instance of <code>Tokenizer</code> to tokenize the input and output strings.</p> </li> <li> <code>max_ngrams</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>The maximum n-gram order to consider. Defaults to <code>4</code>.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>A key to create category-wise mean score. The category key is expected to be in extra_info.</p> </li> <li> <code>lm_output_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>DEFAULT_STRING_PROCESSOR</code> )           \u2013            <p>StringProcessor or a list of StringProcessor to be applied to the model outputs before comparison.</p> </li> <li> <code>reference_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>DEFAULT_STRING_PROCESSOR</code> )           \u2013            <p>StringProcessor or list of StringProcessor to apply to the references before comparison.</p> </li> <li> <code>source_processor</code>               (<code>StringProcessor | list[StringProcessor] | None</code>, default:                   <code>DEFAULT_STRING_PROCESSOR</code> )           \u2013            <p>StringProcessor or list of StringProcessor to apply to the source sentences before comparison.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import SARI\n&gt;&gt;&gt; sari_scorer = SARI(source_key=\"source\")\n&gt;&gt;&gt; lm_outputs = [\"About 95 you now get in.\"]\n&gt;&gt;&gt; references_list = [[\"About 95 species are currently known.\", \"About 95 species are now accepted.\", \"95 species are now accepted.\"]]\n&gt;&gt;&gt; extra_info_list = [{\"source\": \"About 95 species are currently accepted.\"}]\n&gt;&gt;&gt; result = sari_scorer.evaluate(lm_outputs, references_list, extra_info_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={\n        'sari_score': 0.2695360195360195,\n        'sari_add': 0.08333333333333333,\n        'sari_keep': 0.22527472527472525,\n        'sari_del': 0.5\n    },\n    instance_details=[{'sari_score': 0.2695360195360195, 'sari_add': 0.08333333333333333, 'sari_keep': 0.22527472527472525, 'sari_del': 0.5}]\n)\n</code></pre> Source code in <code>flexeval/core/metric/sari.py</code> <pre><code>class SARI(Metric):\n    \"\"\"An implementation of SARI, a metric for evaluating text simplification.\n\n    Based on the original implementation [1], modified to allow configurable settings\n    for the maximum n-gram size and tokenizer.\n    Additionally, it fixes a bug present in the original implementation [2].\n    When used with the default parameters, it produces scores that are\n    consistent with the HuggingFace/evaluate implementation [3].\n\n    [1] https://github.com/cocoxu/simplification/blob/master/SARI.py\n    [2] https://github.com/cocoxu/simplification/issues/6\n    [3] https://huggingface.co/spaces/evaluate-metric/sari/blob/main/sari.py\n\n    Args:\n        tokenizer: An instance of `Tokenizer` to tokenize the input and output strings.\n        max_ngrams: The maximum n-gram order to consider. Defaults to `4`.\n        category_key: A key to create category-wise mean score.\n            The category key is expected to be in extra_info.\n        lm_output_processor:\n            StringProcessor or a list of StringProcessor to be applied to the model outputs before comparison.\n        reference_processor: StringProcessor or list of StringProcessor to apply to the references before comparison.\n        source_processor: StringProcessor or list of StringProcessor to apply to the source sentences before comparison.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import SARI\n        &gt;&gt;&gt; sari_scorer = SARI(source_key=\"source\")\n        &gt;&gt;&gt; lm_outputs = [\"About 95 you now get in.\"]\n        &gt;&gt;&gt; references_list = [[\"About 95 species are currently known.\", \"About 95 species are now accepted.\", \"95 species are now accepted.\"]]\n        &gt;&gt;&gt; extra_info_list = [{\"source\": \"About 95 species are currently accepted.\"}]\n        &gt;&gt;&gt; result = sari_scorer.evaluate(lm_outputs, references_list, extra_info_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={\n                'sari_score': 0.2695360195360195,\n                'sari_add': 0.08333333333333333,\n                'sari_keep': 0.22527472527472525,\n                'sari_del': 0.5\n            },\n            instance_details=[{'sari_score': 0.2695360195360195, 'sari_add': 0.08333333333333333, 'sari_keep': 0.22527472527472525, 'sari_del': 0.5}]\n        )\n    \"\"\"  # noqa: E501\n\n    def __init__(\n        self,\n        source_key: str,\n        tokenizer: Tokenizer | Literal[\"default\"] = \"default\",\n        max_ngrams: int = 4,\n        category_key: str | None = None,\n        source_processor: StringProcessor | list[StringProcessor] | None = DEFAULT_STRING_PROCESSOR,\n        lm_output_processor: StringProcessor | list[StringProcessor] | None = DEFAULT_STRING_PROCESSOR,\n        reference_processor: StringProcessor | list[StringProcessor] | None = DEFAULT_STRING_PROCESSOR,\n    ) -&gt; None:\n        if tokenizer == \"default\":\n            tokenizer = SacreBleuTokenizer(\"13a\")\n        self._tokenizer = tokenizer\n        self.source_key = source_key\n        self.max_ngrams = max_ngrams\n        self.category_key = category_key\n\n        self.source_processors = source_processor\n        self.lm_output_processors = lm_output_processor\n        self.reference_processors = reference_processor\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        if extra_info_list is None:\n            msg = \"SARI requires extra_info_list\"\n            raise ValueError(msg)\n        sources = [extra_info[self.source_key] for extra_info in extra_info_list]\n\n        # Normalize text data\n        sources = [apply_string_processors(src, self.source_processors) for src in sources]\n        lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n        references_list = [\n            [apply_string_processors(ref, self.reference_processors) for ref in references]\n            for references in references_list\n        ]\n\n        # Compute metrics\n        sari_instance_list = [\n            self._calc_sentence_sari(source, lm_output, references)\n            for source, lm_output, references in zip(sources, lm_outputs, references_list)\n        ]\n\n        metric_name2scores = {\n            name: [s[name] for s in sari_instance_list] for name in [\"sari_score\", \"sari_add\", \"sari_keep\", \"sari_del\"]\n        }\n\n        num_instances = len(sari_instance_list)\n        summary = {\n            metric_name: sum(score_list) / num_instances for metric_name, score_list in metric_name2scores.items()\n        }\n\n        if self.category_key:\n            categories = [extra_info[self.category_key] for extra_info in extra_info_list]\n            for metric_name, score_list in metric_name2scores.items():\n                category_wise_scores = aggregate_category_wise_scores(score_list, categories)\n                for category, category_wise_score in category_wise_scores.items():\n                    summary[f\"{metric_name}/{category}\"] = category_wise_score\n\n        return MetricResult(\n            summary,\n            instance_details=sari_instance_list,\n        )\n\n    def _calc_sentence_sari(self, source: str, lm_output: str, references: list[str]) -&gt; dict[str, float]:\n        s_words = self._tokenizer.tokenize(source)\n        c_words = self._tokenizer.tokenize(lm_output)\n        r_words_list = [self._tokenizer.tokenize(reference) for reference in references]\n\n        sari_score, sari_add, sari_keep, sari_del = 0.0, 0.0, 0.0, 0.0\n        for n in range(1, self.max_ngrams + 1):\n            s_ngrams = to_ngram(s_words, n)\n            c_ngrams = to_ngram(c_words, n)\n            r_ngrams_list = [to_ngram(r_words, n) for r_words in r_words_list]\n\n            sari_n_score, sari_n_add, sari_n_keep, sari_n_del = self._sari_n(s_ngrams, c_ngrams, r_ngrams_list)\n            sari_score += sari_n_score\n            sari_add += sari_n_add\n            sari_keep += sari_n_keep\n            sari_del += sari_n_del\n\n        sari_score /= self.max_ngrams\n        sari_add /= self.max_ngrams\n        sari_keep /= self.max_ngrams\n        sari_del /= self.max_ngrams\n\n        return {\"sari_score\": sari_score, \"sari_add\": sari_add, \"sari_keep\": sari_keep, \"sari_del\": sari_del}\n\n    def _sari_n(\n        self, s_grams: list[str], c_grams: list[str], r_grams_list: list[list[str]]\n    ) -&gt; tuple[float, float, float, float]:\n        num_ref = len(r_grams_list)\n        r_grams_all = [r_gram for r_grams in r_grams_list for r_gram in r_grams]\n        r_gram_counter = Counter(r_grams_all)\n\n        s_gram_counter = Counter(s_grams)\n        c_gram_counter = Counter(c_grams)\n\n        s_gram_rep = Counter({k: v * num_ref for k, v in s_gram_counter.items()})\n        c_gram_rep = Counter({k: v * num_ref for k, v in c_gram_counter.items()})\n\n        # ADD\n        add_grams = set(c_gram_counter) - set(s_gram_counter)\n        add_good = add_grams &amp; set(r_gram_counter)\n        add_all = set(r_gram_counter) - set(s_gram_counter)\n\n        add_prec = len(add_good) / len(add_grams) if add_grams else 1\n        add_recall = len(add_good) / len(add_all) if add_all else 1\n        add_f1 = 2 * add_prec * add_recall / (add_prec + add_recall) if (add_prec + add_recall) &gt; 0 else 0\n\n        # KEEP\n        keep_rep = s_gram_rep &amp; c_gram_rep\n        keep_good = keep_rep &amp; r_gram_counter\n        keep_all = s_gram_rep &amp; r_gram_counter\n\n        keep_prec = sum(keep_good[g] / keep_rep[g] for g in keep_good) / len(keep_rep) if keep_rep else 1\n        keep_recall = sum(keep_good[g] for g in keep_good) / sum(keep_all.values()) if keep_all else 1\n        keep_f1 = 2 * keep_prec * keep_recall / (keep_prec + keep_recall) if (keep_prec + keep_recall) &gt; 0 else 0\n\n        # DELETE\n        del_rep = s_gram_rep - c_gram_rep\n        del_good = del_rep - r_gram_counter\n\n        del_prec = sum(del_good[g] / del_rep[g] for g in del_good) / len(del_rep) if del_rep else 1\n\n        return (add_f1 + keep_f1 + del_prec) / 3, add_f1, keep_f1, del_prec\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.sari.SARI.source_key","title":"source_key  <code>instance-attribute</code>","text":"<pre><code>source_key = source_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.sari.SARI.max_ngrams","title":"max_ngrams  <code>instance-attribute</code>","text":"<pre><code>max_ngrams = max_ngrams\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.sari.SARI.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.sari.SARI.source_processors","title":"source_processors  <code>instance-attribute</code>","text":"<pre><code>source_processors = source_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.sari.SARI.lm_output_processors","title":"lm_output_processors  <code>instance-attribute</code>","text":"<pre><code>lm_output_processors = lm_output_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.sari.SARI.reference_processors","title":"reference_processors  <code>instance-attribute</code>","text":"<pre><code>reference_processors = reference_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.sari.SARI.__init__","title":"__init__","text":"<pre><code>__init__(\n    source_key: str,\n    tokenizer: Tokenizer | Literal[\"default\"] = \"default\",\n    max_ngrams: int = 4,\n    category_key: str | None = None,\n    source_processor: StringProcessor\n    | list[StringProcessor]\n    | None = DEFAULT_STRING_PROCESSOR,\n    lm_output_processor: StringProcessor\n    | list[StringProcessor]\n    | None = DEFAULT_STRING_PROCESSOR,\n    reference_processor: StringProcessor\n    | list[StringProcessor]\n    | None = DEFAULT_STRING_PROCESSOR,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/sari.py</code> <pre><code>def __init__(\n    self,\n    source_key: str,\n    tokenizer: Tokenizer | Literal[\"default\"] = \"default\",\n    max_ngrams: int = 4,\n    category_key: str | None = None,\n    source_processor: StringProcessor | list[StringProcessor] | None = DEFAULT_STRING_PROCESSOR,\n    lm_output_processor: StringProcessor | list[StringProcessor] | None = DEFAULT_STRING_PROCESSOR,\n    reference_processor: StringProcessor | list[StringProcessor] | None = DEFAULT_STRING_PROCESSOR,\n) -&gt; None:\n    if tokenizer == \"default\":\n        tokenizer = SacreBleuTokenizer(\"13a\")\n    self._tokenizer = tokenizer\n    self.source_key = source_key\n    self.max_ngrams = max_ngrams\n    self.category_key = category_key\n\n    self.source_processors = source_processor\n    self.lm_output_processors = lm_output_processor\n    self.reference_processors = reference_processor\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.sari.SARI.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/sari.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    if extra_info_list is None:\n        msg = \"SARI requires extra_info_list\"\n        raise ValueError(msg)\n    sources = [extra_info[self.source_key] for extra_info in extra_info_list]\n\n    # Normalize text data\n    sources = [apply_string_processors(src, self.source_processors) for src in sources]\n    lm_outputs = [apply_string_processors(output, self.lm_output_processors) for output in lm_outputs]\n    references_list = [\n        [apply_string_processors(ref, self.reference_processors) for ref in references]\n        for references in references_list\n    ]\n\n    # Compute metrics\n    sari_instance_list = [\n        self._calc_sentence_sari(source, lm_output, references)\n        for source, lm_output, references in zip(sources, lm_outputs, references_list)\n    ]\n\n    metric_name2scores = {\n        name: [s[name] for s in sari_instance_list] for name in [\"sari_score\", \"sari_add\", \"sari_keep\", \"sari_del\"]\n    }\n\n    num_instances = len(sari_instance_list)\n    summary = {\n        metric_name: sum(score_list) / num_instances for metric_name, score_list in metric_name2scores.items()\n    }\n\n    if self.category_key:\n        categories = [extra_info[self.category_key] for extra_info in extra_info_list]\n        for metric_name, score_list in metric_name2scores.items():\n            category_wise_scores = aggregate_category_wise_scores(score_list, categories)\n            for category, category_wise_score in category_wise_scores.items():\n                summary[f\"{metric_name}/{category}\"] = category_wise_score\n\n    return MetricResult(\n        summary,\n        instance_details=sari_instance_list,\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.substring_match.SubstringMatch","title":"SubstringMatch","text":"<p>A metric that calculates how many outputs contain any of the expected substrings.</p> <p>Parameters:</p> <ul> <li> <code>mode</code>               (<code>Literal['any', 'all']</code>, default:                   <code>'any'</code> )           \u2013            <p>The mode to calculate the substring match. - \"any\": If any of the expected substrings are in the output, it is a match. - \"all\": If all of the expected substrings are in the output, it is a match.</p> </li> <li> <code>category_key</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional key to group scores by category from extra_info_list.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import SubstringMatch\n&gt;&gt;&gt; substring_match = SubstringMatch()\n&gt;&gt;&gt; lm_outputs = [\"This is a cat .\", \"This is a dog .\"]\n&gt;&gt;&gt; references_list = [[\"cat\", \"dog\"], [\"mouse\"]]\n&gt;&gt;&gt; result = substring_match.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'substring_match': 0.5},\n    instance_details=[{'substring_match': True}, {'substring_match': False}]\n)\n</code></pre> Source code in <code>flexeval/core/metric/substring_match.py</code> <pre><code>class SubstringMatch(Metric):\n    \"\"\"\n    A metric that calculates how many outputs contain any of the expected substrings.\n\n    Args:\n        mode: The mode to calculate the substring match.\n            - \"any\": If any of the expected substrings are in the output, it is a match.\n            - \"all\": If all of the expected substrings are in the output, it is a match.\n        category_key: Optional key to group scores by category from extra_info_list.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import SubstringMatch\n        &gt;&gt;&gt; substring_match = SubstringMatch()\n        &gt;&gt;&gt; lm_outputs = [\"This is a cat .\", \"This is a dog .\"]\n        &gt;&gt;&gt; references_list = [[\"cat\", \"dog\"], [\"mouse\"]]\n        &gt;&gt;&gt; result = substring_match.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'substring_match': 0.5},\n            instance_details=[{'substring_match': True}, {'substring_match': False}]\n        )\n    \"\"\"\n\n    def __init__(self, mode: Literal[\"any\", \"all\"] = \"any\", category_key: str | None = None) -&gt; None:\n        self.mode = mode\n        self.category_key = category_key\n        if mode == \"all\":\n            self.match_func = all\n        elif mode == \"any\":\n            self.match_func = any\n        else:\n            msg = f\"mode must be 'any' or 'all', but got '{mode}'.\"\n            raise ValueError(msg)\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Compute metrics\n        match_list = [\n            self.match_func(substring in lm_output for substring in expected_output)\n            for lm_output, expected_output in zip(lm_outputs, references_list)\n        ]\n\n        score = 0.0\n        if len(match_list):\n            score = sum(match_list) / len(match_list)\n\n        summary = {f\"substring_match-{self.mode}\": score}\n\n        if self.category_key:\n            categories = [extra_info[self.category_key] for extra_info in extra_info_list]\n            category_wise_scores = aggregate_category_wise_scores(match_list, categories)\n            for category, category_wise_score in category_wise_scores.items():\n                summary[f\"substring_match-{self.mode}/{category}\"] = category_wise_score\n\n        return MetricResult(\n            summary,\n            instance_details=[{\"substring_match\": match} for match in match_list],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.substring_match.SubstringMatch.mode","title":"mode  <code>instance-attribute</code>","text":"<pre><code>mode = mode\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.substring_match.SubstringMatch.category_key","title":"category_key  <code>instance-attribute</code>","text":"<pre><code>category_key = category_key\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.substring_match.SubstringMatch.match_func","title":"match_func  <code>instance-attribute</code>","text":"<pre><code>match_func = all\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.substring_match.SubstringMatch.__init__","title":"__init__","text":"<pre><code>__init__(\n    mode: Literal[\"any\", \"all\"] = \"any\",\n    category_key: str | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/substring_match.py</code> <pre><code>def __init__(self, mode: Literal[\"any\", \"all\"] = \"any\", category_key: str | None = None) -&gt; None:\n    self.mode = mode\n    self.category_key = category_key\n    if mode == \"all\":\n        self.match_func = all\n    elif mode == \"any\":\n        self.match_func = any\n    else:\n        msg = f\"mode must be 'any' or 'all', but got '{mode}'.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.substring_match.SubstringMatch.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/substring_match.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Compute metrics\n    match_list = [\n        self.match_func(substring in lm_output for substring in expected_output)\n        for lm_output, expected_output in zip(lm_outputs, references_list)\n    ]\n\n    score = 0.0\n    if len(match_list):\n        score = sum(match_list) / len(match_list)\n\n    summary = {f\"substring_match-{self.mode}\": score}\n\n    if self.category_key:\n        categories = [extra_info[self.category_key] for extra_info in extra_info_list]\n        category_wise_scores = aggregate_category_wise_scores(match_list, categories)\n        for category, category_wise_score in category_wise_scores.items():\n            summary[f\"substring_match-{self.mode}/{category}\"] = category_wise_score\n\n    return MetricResult(\n        summary,\n        instance_details=[{\"substring_match\": match} for match in match_list],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.tool_call.ToolCallCount","title":"ToolCallCount","text":"<p>Metric to compute the ratio of different tool_call_validation_result values.</p> Source code in <code>flexeval/core/metric/tool_call.py</code> <pre><code>class ToolCallCount(Metric):\n    \"\"\"\n    Metric to compute the ratio of different tool_call_validation_result values.\n    \"\"\"\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        # Calculate the finish_reason and validation statistics\n        tool_call_validation_result_counter = Counter()\n        for lm_output in lm_outputs:\n            if not isinstance(lm_output, LMOutput):\n                msg = \"ToolCallCount expects lm_outputs to be an LMOutput, but received a different type.\"\n                raise TypeError(msg)\n            tool_call_validation_result_counter[lm_output.tool_call_validation_result] += 1\n\n        total_count = sum(tool_call_validation_result_counter.values())\n        summary = {}\n        if total_count &gt; 0:\n            for validation_result, count in tool_call_validation_result_counter.items():\n                summary[f\"tool_call_validation_result_ratio-{validation_result}\"] = count / total_count\n        return MetricResult(\n            summary=summary,\n            instance_details=[\n                {\"tool_call_validation_result\": lm_output.tool_call_validation_result} for lm_output in lm_outputs\n            ],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.tool_call.ToolCallCount.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/tool_call.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    # Calculate the finish_reason and validation statistics\n    tool_call_validation_result_counter = Counter()\n    for lm_output in lm_outputs:\n        if not isinstance(lm_output, LMOutput):\n            msg = \"ToolCallCount expects lm_outputs to be an LMOutput, but received a different type.\"\n            raise TypeError(msg)\n        tool_call_validation_result_counter[lm_output.tool_call_validation_result] += 1\n\n    total_count = sum(tool_call_validation_result_counter.values())\n    summary = {}\n    if total_count &gt; 0:\n        for validation_result, count in tool_call_validation_result_counter.items():\n            summary[f\"tool_call_validation_result_ratio-{validation_result}\"] = count / total_count\n    return MetricResult(\n        summary=summary,\n        instance_details=[\n            {\"tool_call_validation_result\": lm_output.tool_call_validation_result} for lm_output in lm_outputs\n        ],\n    )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.xer.XER","title":"XER","text":"<p>Calculate the Character Error Rate (CER) and Word Error Rate (WER) between the model outputs and the references. The calculation is based on the jiwer library.</p> <p>Parameters:</p> <ul> <li> <code>tokenizer</code>               (<code>Tokenizer | None</code>, default:                   <code>None</code> )           \u2013            <p>An instance of <code>Tokenizer</code> to tokenize the input and output strings.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import XER\n&gt;&gt;&gt; xer = XER()\n&gt;&gt;&gt; lm_outputs = [\"I am a student .\", \"I am a teacher .\"]\n&gt;&gt;&gt; references_list = [[\"I am a student .\", \"I am a learner .\"], [\"Are you the student ?\"]]\n&gt;&gt;&gt; result = xer.evaluate(lm_outputs, references_list)\n&gt;&gt;&gt; print(result)\nMetricResult(\n    summary={'cer_score': 0.43243243243243246, 'wer_score': 0.5},\n    instance_details=[{'cer_score': 0.0, 'wer_score': 0.0}, {'cer_score': 0.7619047619047619, 'wer_score': 1.0}\n    ]\n)\n</code></pre> Source code in <code>flexeval/core/metric/xer.py</code> <pre><code>class XER(Metric):\n    \"\"\"\n    Calculate the Character Error Rate (CER) and Word Error Rate (WER) between the model outputs and the references.\n    The calculation is based on the [jiwer](https://github.com/jitsi/jiwer) library.\n\n    Args:\n        tokenizer: An instance of `Tokenizer` to tokenize the input and output strings.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import XER\n        &gt;&gt;&gt; xer = XER()\n        &gt;&gt;&gt; lm_outputs = [\"I am a student .\", \"I am a teacher .\"]\n        &gt;&gt;&gt; references_list = [[\"I am a student .\", \"I am a learner .\"], [\"Are you the student ?\"]]\n        &gt;&gt;&gt; result = xer.evaluate(lm_outputs, references_list)\n        &gt;&gt;&gt; print(result)\n        MetricResult(\n            summary={'cer_score': 0.43243243243243246, 'wer_score': 0.5},\n            instance_details=[{'cer_score': 0.0, 'wer_score': 0.0}, {'cer_score': 0.7619047619047619, 'wer_score': 1.0}\n            ]\n        )\n    \"\"\"\n\n    def __init__(self, tokenizer: Tokenizer | None = None) -&gt; None:\n        self.tokenizer = tokenizer\n\n    def evaluate(\n        self,\n        lm_outputs: list[str | LMOutput],\n        references_list: list[list[str]],\n        extra_info_list: list[dict[str, str]] | None = None,\n    ) -&gt; MetricResult:\n        validate_inputs(lm_outputs, references_list, extra_info_list)\n\n        lm_outputs = extract_text_from_outputs(lm_outputs)\n\n        # Normalize text data - we only need the first reference\n        references = [references[0] for references in references_list]\n\n        if self.tokenizer:\n            tokenized_lm_outputs = [\" \".join(self.tokenizer.tokenize(lm_output)) for lm_output in lm_outputs]\n            tokenized_references = [\" \".join(self.tokenizer.tokenize(reference)) for reference in references]\n        else:\n            tokenized_lm_outputs = lm_outputs\n            tokenized_references = references\n\n        # Compute metrics\n        cer_score = cer(references, lm_outputs)\n        wer_score = wer(tokenized_references, tokenized_lm_outputs)\n\n        return MetricResult(\n            {\n                \"cer_score\": cer_score,\n                \"wer_score\": wer_score,\n            },\n            instance_details=[\n                {\n                    \"cer_score\": cer(reference, lm_output),\n                    \"wer_score\": wer(reference, lm_output),\n                }\n                for lm_output, reference in zip(lm_outputs, references)\n            ],\n        )\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.xer.XER.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = tokenizer\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.xer.XER.__init__","title":"__init__","text":"<pre><code>__init__(tokenizer: Tokenizer | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/metric/xer.py</code> <pre><code>def __init__(self, tokenizer: Tokenizer | None = None) -&gt; None:\n    self.tokenizer = tokenizer\n</code></pre>"},{"location":"api_reference/Metric/#flexeval.core.metric.xer.XER.evaluate","title":"evaluate","text":"<pre><code>evaluate(\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult\n</code></pre> Source code in <code>flexeval/core/metric/xer.py</code> <pre><code>def evaluate(\n    self,\n    lm_outputs: list[str | LMOutput],\n    references_list: list[list[str]],\n    extra_info_list: list[dict[str, str]] | None = None,\n) -&gt; MetricResult:\n    validate_inputs(lm_outputs, references_list, extra_info_list)\n\n    lm_outputs = extract_text_from_outputs(lm_outputs)\n\n    # Normalize text data - we only need the first reference\n    references = [references[0] for references in references_list]\n\n    if self.tokenizer:\n        tokenized_lm_outputs = [\" \".join(self.tokenizer.tokenize(lm_output)) for lm_output in lm_outputs]\n        tokenized_references = [\" \".join(self.tokenizer.tokenize(reference)) for reference in references]\n    else:\n        tokenized_lm_outputs = lm_outputs\n        tokenized_references = references\n\n    # Compute metrics\n    cer_score = cer(references, lm_outputs)\n    wer_score = wer(tokenized_references, tokenized_lm_outputs)\n\n    return MetricResult(\n        {\n            \"cer_score\": cer_score,\n            \"wer_score\": wer_score,\n        },\n        instance_details=[\n            {\n                \"cer_score\": cer(reference, lm_output),\n                \"wer_score\": wer(reference, lm_output),\n            }\n            for lm_output, reference in zip(lm_outputs, references)\n        ],\n    )\n</code></pre>"},{"location":"api_reference/PairwiseJudge/","title":"PairwiseJudge","text":""},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.PairwiseJudge","title":"PairwiseJudge","text":"<p>Judge which model is better given two items.</p> <p>The output is a tuple of the winner and the rationale.</p> Source code in <code>flexeval/core/pairwise_comparison/judge/base.py</code> <pre><code>class PairwiseJudge(ABC):\n    \"\"\"Judge which model is better given two items.\n\n    The output is a tuple of the winner and the rationale.\n    \"\"\"\n\n    @abstractmethod\n    def batch_judge(\n        self,\n        batch_model_items: list[tuple[dict[str, Any], dict[str, Any]]],\n    ) -&gt; list[tuple[Winner, str]]:\n        \"\"\"\n        Judge which model is better given a batch of item pairs.\n\n        Args:\n            batch_model_items: A list of tuples, each containing two model items.\n        \"\"\"\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.PairwiseJudge.batch_judge","title":"batch_judge  <code>abstractmethod</code>","text":"<pre><code>batch_judge(\n    batch_model_items: list[\n        tuple[dict[str, Any], dict[str, Any]]\n    ],\n) -&gt; list[tuple[Winner, str]]\n</code></pre> <p>Judge which model is better given a batch of item pairs.</p> <p>Parameters:</p> <ul> <li> <code>batch_model_items</code>               (<code>list[tuple[dict[str, Any], dict[str, Any]]]</code>)           \u2013            <p>A list of tuples, each containing two model items.</p> </li> </ul> Source code in <code>flexeval/core/pairwise_comparison/judge/base.py</code> <pre><code>@abstractmethod\ndef batch_judge(\n    self,\n    batch_model_items: list[tuple[dict[str, Any], dict[str, Any]]],\n) -&gt; list[tuple[Winner, str]]:\n    \"\"\"\n    Judge which model is better given a batch of item pairs.\n\n    Args:\n        batch_model_items: A list of tuples, each containing two model items.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.Winner","title":"Winner","text":"<p>Enum class to indicate the winner of a pairwise comparison.</p> Source code in <code>flexeval/core/pairwise_comparison/judge/base.py</code> <pre><code>class Winner(Enum):\n    \"\"\"\n    Enum class to indicate the winner of a pairwise comparison.\n    \"\"\"\n\n    MODEL1 = \"model1\"\n    MODEL2 = \"model2\"\n    DRAW = \"draw\"\n\n    def __str__(self) -&gt; str:\n        # used when serializing to JSON\n        return self.value\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.Winner.MODEL1","title":"MODEL1  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MODEL1 = 'model1'\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.Winner.MODEL2","title":"MODEL2  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MODEL2 = 'model2'\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.Winner.DRAW","title":"DRAW  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DRAW = 'draw'\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.base.Winner.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/judge/base.py</code> <pre><code>def __str__(self) -&gt; str:\n    # used when serializing to JSON\n    return self.value\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.llm_judge.ChatLLMPairwiseJudge","title":"ChatLLMPairwiseJudge","text":"<p>Pairwise judge using a chat language model to compare two model outputs.</p> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>LanguageModel</code>)           \u2013            <p>The language model to use for pairwise comparison.</p> </li> <li> <code>prompt_template</code>               (<code>PromptTemplate</code>)           \u2013            <p>The prompt template to embed the model outputs to be compared.</p> </li> <li> <code>system_message</code>               (<code>str | PromptTemplate | None</code>, default:                   <code>None</code> )           \u2013            <p>The system message to prepend to the chat messages.</p> </li> </ul> Source code in <code>flexeval/core/pairwise_comparison/judge/llm_judge.py</code> <pre><code>class ChatLLMPairwiseJudge(PairwiseJudge):\n    \"\"\"\n    Pairwise judge using a chat language model to compare two model outputs.\n\n    Args:\n        language_model: The language model to use for pairwise comparison.\n        prompt_template: The prompt template to embed the model outputs to be compared.\n        system_message: The system message to prepend to the chat messages.\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        system_message: str | PromptTemplate | None = None,\n    ) -&gt; None:\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.system_message = system_message\n\n    @staticmethod\n    def _parse_judge_output(judge_output: str) -&gt; tuple[Winner, str]:\n        \"\"\"Extract the last integer value from the judge output and return the\n        corresponding Winner and its rationale.\n\n        Return `Winner.DRAW` if parsing fails.\n        \"\"\"\n        try:\n            matched = re.findall(r\"(\\d+)\", judge_output)\n            value = int(matched[-1])\n            winner: Winner\n            rationale = judge_output\n            if value == 1:\n                winner = Winner.MODEL1\n            elif value == 2:\n                winner = Winner.MODEL2\n            elif value == 3:\n                winner = Winner.DRAW\n            else:\n                logger.warning(f\"Invalid number {value} was extracted:\\n\\n{judge_output}\")\n                winner = Winner.DRAW\n                rationale = f\"Invalid judge '{value}': {judge_output}\"\n        except (IndexError, ValueError):\n            logger.warning(f\"Failed to extract the judgment result:\\n\\n{judge_output}\")\n            return Winner.DRAW, f\"Parsing failure: {judge_output}\"\n        else:\n            return winner, rationale\n\n    def batch_judge(self, batch_model_items: list[tuple[dict[str, Any], dict[str, Any]]]) -&gt; list[tuple[Winner, str]]:\n        input_chat_messages_list: list[list[dict[str, str]]] = []\n        for model1_item, model2_item in batch_model_items:\n            references = model1_item[\"references\"]\n            prompt_inputs = {\n                \"model1_item\": model1_item,\n                \"model2_item\": model2_item,\n                \"references\": references,\n            }\n            self.prompt_template.embed_inputs(prompt_inputs)\n            judge_input = self.prompt_template.embed_inputs(prompt_inputs)\n            input_chat_messages = [{\"role\": \"user\", \"content\": judge_input}]\n            if self.system_message:\n                if isinstance(self.system_message, str):\n                    system_message = self.system_message\n                else:\n                    system_message = self.system_message.embed_inputs(prompt_inputs)\n                input_chat_messages.insert(\n                    0,\n                    {\"role\": \"system\", \"content\": system_message},\n                )\n            input_chat_messages_list.append(input_chat_messages)\n        judge_outputs = self.language_model.generate_chat_response(input_chat_messages_list)\n        return [self._parse_judge_output(output.text) for output in judge_outputs]\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.llm_judge.ChatLLMPairwiseJudge.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.llm_judge.ChatLLMPairwiseJudge.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.llm_judge.ChatLLMPairwiseJudge.system_message","title":"system_message  <code>instance-attribute</code>","text":"<pre><code>system_message = system_message\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.llm_judge.ChatLLMPairwiseJudge.__init__","title":"__init__","text":"<pre><code>__init__(\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    system_message: str | PromptTemplate | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/judge/llm_judge.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    system_message: str | PromptTemplate | None = None,\n) -&gt; None:\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.system_message = system_message\n</code></pre>"},{"location":"api_reference/PairwiseJudge/#flexeval.core.pairwise_comparison.judge.llm_judge.ChatLLMPairwiseJudge.batch_judge","title":"batch_judge","text":"<pre><code>batch_judge(\n    batch_model_items: list[\n        tuple[dict[str, Any], dict[str, Any]]\n    ],\n) -&gt; list[tuple[Winner, str]]\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/judge/llm_judge.py</code> <pre><code>def batch_judge(self, batch_model_items: list[tuple[dict[str, Any], dict[str, Any]]]) -&gt; list[tuple[Winner, str]]:\n    input_chat_messages_list: list[list[dict[str, str]]] = []\n    for model1_item, model2_item in batch_model_items:\n        references = model1_item[\"references\"]\n        prompt_inputs = {\n            \"model1_item\": model1_item,\n            \"model2_item\": model2_item,\n            \"references\": references,\n        }\n        self.prompt_template.embed_inputs(prompt_inputs)\n        judge_input = self.prompt_template.embed_inputs(prompt_inputs)\n        input_chat_messages = [{\"role\": \"user\", \"content\": judge_input}]\n        if self.system_message:\n            if isinstance(self.system_message, str):\n                system_message = self.system_message\n            else:\n                system_message = self.system_message.embed_inputs(prompt_inputs)\n            input_chat_messages.insert(\n                0,\n                {\"role\": \"system\", \"content\": system_message},\n            )\n        input_chat_messages_list.append(input_chat_messages)\n    judge_outputs = self.language_model.generate_chat_response(input_chat_messages_list)\n    return [self._parse_judge_output(output.text) for output in judge_outputs]\n</code></pre>"},{"location":"api_reference/PairwiseScorer/","title":"PairwiseScorer","text":""},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.base.PairwiseScorer","title":"PairwiseScorer","text":"<p>Compute scores for each model given the match results.</p> <p>Each match result is a triple of two model names and the winner.</p> Source code in <code>flexeval/core/pairwise_comparison/scorer/base.py</code> <pre><code>class PairwiseScorer(ABC):\n    \"\"\"Compute scores for each model given the match results.\n\n    Each match result is a triple of two model names and the winner.\n    \"\"\"\n\n    name: str = None\n\n    @abstractmethod\n    def compute_scores(\n        self: PairwiseScorer,\n        match_results: list[tuple[str, str, Winner]],\n    ) -&gt; dict[str, float]:\n        pass\n\n    @classmethod\n    def get_name(cls: type[PairwiseScorer]) -&gt; str:\n        return cls.name if cls.name else cls.__name__\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.base.PairwiseScorer.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = None\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.base.PairwiseScorer.compute_scores","title":"compute_scores  <code>abstractmethod</code>","text":"<pre><code>compute_scores(\n    match_results: list[tuple[str, str, Winner]],\n) -&gt; dict[str, float]\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/scorer/base.py</code> <pre><code>@abstractmethod\ndef compute_scores(\n    self: PairwiseScorer,\n    match_results: list[tuple[str, str, Winner]],\n) -&gt; dict[str, float]:\n    pass\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.base.PairwiseScorer.get_name","title":"get_name  <code>classmethod</code>","text":"<pre><code>get_name() -&gt; str\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/scorer/base.py</code> <pre><code>@classmethod\ndef get_name(cls: type[PairwiseScorer]) -&gt; str:\n    return cls.name if cls.name else cls.__name__\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer","title":"BradleyTerryScorer","text":"Source code in <code>flexeval/core/pairwise_comparison/scorer/bradley_terry.py</code> <pre><code>class BradleyTerryScorer(PairwiseScorer):\n    name: str = \"bradley_terry\"\n\n    def __init__(\n        self,\n        max_iters: int = 1000,\n        error_tol: float = 1e-3,\n        eps: float = 1e-8,\n        base: float = 10.0,\n        scale: float = 400.0,\n        init_rating: float = 1000.0,\n    ) -&gt; None:\n        self.max_iters = max_iters\n        self.error_tol = error_tol\n        self.eps = eps\n        self.base = base\n        self.scale = scale\n        self.init_rating = init_rating\n\n    def _gen_winloss_matrix(\n        self,\n        match_results: list[tuple[str, str, Winner]],\n    ) -&gt; dict[str, dict[str, float]]:\n        \"\"\"\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001 `matrix[\u30e2\u30c7\u30eb1][\u30e2\u30c7\u30eb2] = &lt;\u30e2\u30c7\u30eb1\u304c\u30e2\u30c7\u30eb2\u306b\u52dd\u3063\u305f\u56de\u6570&gt;` \u3068\u306a\u308b\u3088\u3046\u306a\u8f9e\u66f8\u3092\u8fd4\u3059\"\"\"\n        matrix = defaultdict(lambda: defaultdict(float))\n\n        for model1, model2, winner in match_results:\n            if winner == Winner.MODEL1:\n                matrix[model1][model2] += 1.0\n            elif winner == Winner.MODEL2:\n                matrix[model2][model1] += 1.0\n            elif winner == Winner.DRAW:\n                matrix[model1][model2] += 0.5\n                matrix[model2][model1] += 0.5\n\n        return matrix\n\n    def compute_scores(\n        self,\n        match_results: list[tuple[str, str, Winner]],\n    ) -&gt; dict[str, float]:\n        \"\"\"\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001Bradley-Terry model (MLE) \u3067\u63a8\u5b9a\u3057\u305f\u5404\u30e2\u30c7\u30eb\u306e\u30b9\u30b3\u30a2\u3092\u8fd4\u3059\u3002\"\"\"\n        model_names = sorted(\n            {m[0] for m in match_results} | {m[1] for m in match_results},\n        )\n        winloss_matrix = self._gen_winloss_matrix(match_results)\n\n        # https://jmlr.org/papers/volume24/22-1086/22-1086.pdf#page=5.50 (12)\n        scores = pd.Series(np.ones(len(model_names)), index=model_names)\n        for iters in range(self.max_iters):\n            old_scores = scores.copy()\n            for target_model in scores.keys():  # noqa: SIM118\n                numer = sum(\n                    [\n                        (winloss_matrix[target_model][other_model] * scores[other_model])\n                        / (scores[target_model] + scores[other_model])\n                        for other_model in winloss_matrix[target_model]\n                    ],\n                )\n                denom = sum(\n                    [\n                        (winloss_matrix[other_model][target_model]) / (scores[target_model] + scores[other_model])\n                        for other_model in winloss_matrix[target_model]\n                    ],\n                )\n\n                scores[target_model] = numer / (denom + self.eps)\n\n            scores /= np.exp(np.log(scores).sum()) ** (1 / len(scores))\n\n            if (scores - old_scores).abs().sum() &lt; self.error_tol:\n                logger.info(f\" * Converged after {iters} iterations.\")\n                break\n        else:\n            logger.info(\n                f\" * Max iterations reached ({self.max_iters} iters).\",\n            )\n\n        return (\n            scores.apply(\n                lambda x: self.scale / np.log(self.base) * np.log(x) + self.init_rating,\n            )\n            .sort_values(ascending=False)\n            .to_dict()\n        )\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = 'bradley_terry'\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.max_iters","title":"max_iters  <code>instance-attribute</code>","text":"<pre><code>max_iters = max_iters\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.error_tol","title":"error_tol  <code>instance-attribute</code>","text":"<pre><code>error_tol = error_tol\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps = eps\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.base","title":"base  <code>instance-attribute</code>","text":"<pre><code>base = base\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = scale\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.init_rating","title":"init_rating  <code>instance-attribute</code>","text":"<pre><code>init_rating = init_rating\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.__init__","title":"__init__","text":"<pre><code>__init__(\n    max_iters: int = 1000,\n    error_tol: float = 0.001,\n    eps: float = 1e-08,\n    base: float = 10.0,\n    scale: float = 400.0,\n    init_rating: float = 1000.0,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/pairwise_comparison/scorer/bradley_terry.py</code> <pre><code>def __init__(\n    self,\n    max_iters: int = 1000,\n    error_tol: float = 1e-3,\n    eps: float = 1e-8,\n    base: float = 10.0,\n    scale: float = 400.0,\n    init_rating: float = 1000.0,\n) -&gt; None:\n    self.max_iters = max_iters\n    self.error_tol = error_tol\n    self.eps = eps\n    self.base = base\n    self.scale = scale\n    self.init_rating = init_rating\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.bradley_terry.BradleyTerryScorer.compute_scores","title":"compute_scores","text":"<pre><code>compute_scores(\n    match_results: list[tuple[str, str, Winner]],\n) -&gt; dict[str, float]\n</code></pre> <p>\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001Bradley-Terry model (MLE) \u3067\u63a8\u5b9a\u3057\u305f\u5404\u30e2\u30c7\u30eb\u306e\u30b9\u30b3\u30a2\u3092\u8fd4\u3059\u3002</p> Source code in <code>flexeval/core/pairwise_comparison/scorer/bradley_terry.py</code> <pre><code>def compute_scores(\n    self,\n    match_results: list[tuple[str, str, Winner]],\n) -&gt; dict[str, float]:\n    \"\"\"\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001Bradley-Terry model (MLE) \u3067\u63a8\u5b9a\u3057\u305f\u5404\u30e2\u30c7\u30eb\u306e\u30b9\u30b3\u30a2\u3092\u8fd4\u3059\u3002\"\"\"\n    model_names = sorted(\n        {m[0] for m in match_results} | {m[1] for m in match_results},\n    )\n    winloss_matrix = self._gen_winloss_matrix(match_results)\n\n    # https://jmlr.org/papers/volume24/22-1086/22-1086.pdf#page=5.50 (12)\n    scores = pd.Series(np.ones(len(model_names)), index=model_names)\n    for iters in range(self.max_iters):\n        old_scores = scores.copy()\n        for target_model in scores.keys():  # noqa: SIM118\n            numer = sum(\n                [\n                    (winloss_matrix[target_model][other_model] * scores[other_model])\n                    / (scores[target_model] + scores[other_model])\n                    for other_model in winloss_matrix[target_model]\n                ],\n            )\n            denom = sum(\n                [\n                    (winloss_matrix[other_model][target_model]) / (scores[target_model] + scores[other_model])\n                    for other_model in winloss_matrix[target_model]\n                ],\n            )\n\n            scores[target_model] = numer / (denom + self.eps)\n\n        scores /= np.exp(np.log(scores).sum()) ** (1 / len(scores))\n\n        if (scores - old_scores).abs().sum() &lt; self.error_tol:\n            logger.info(f\" * Converged after {iters} iterations.\")\n            break\n    else:\n        logger.info(\n            f\" * Max iterations reached ({self.max_iters} iters).\",\n        )\n\n    return (\n        scores.apply(\n            lambda x: self.scale / np.log(self.base) * np.log(x) + self.init_rating,\n        )\n        .sort_values(ascending=False)\n        .to_dict()\n    )\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.win_rate.WinRateScorer","title":"WinRateScorer","text":"Source code in <code>flexeval/core/pairwise_comparison/scorer/win_rate.py</code> <pre><code>class WinRateScorer(PairwiseScorer):\n    name: str = \"win_rate\"\n\n    def compute_scores(\n        self,\n        match_results: list[tuple[str, str, Winner]],\n    ) -&gt; dict[str, float]:\n        \"\"\"\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001\u5404\u30e2\u30c7\u30eb\u306e\u52dd\u7387\u3092\u8fd4\u3059\u3002\"\"\"\n        match_count_dict: dict[str, float] = defaultdict(float)\n        win_count_dict: dict[str, float] = defaultdict(float)\n\n        for model1, model2, winner in match_results:\n            match_count_dict[model1] += 1\n            match_count_dict[model2] += 1\n            if winner == Winner.MODEL1:\n                win_count_dict[model1] += 1\n            elif winner == Winner.MODEL2:\n                win_count_dict[model2] += 1\n            elif winner == Winner.DRAW:\n                win_count_dict[model1] += 0.5\n                win_count_dict[model2] += 0.5\n\n        win_rate_dict = {}\n        for model in match_count_dict:\n            win_rate_dict[model] = 100 * win_count_dict.get(model, 0.0) / match_count_dict[model]\n\n        return dict(sorted(win_rate_dict.items(), key=lambda x: -x[1]))\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.win_rate.WinRateScorer.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = 'win_rate'\n</code></pre>"},{"location":"api_reference/PairwiseScorer/#flexeval.core.pairwise_comparison.scorer.win_rate.WinRateScorer.compute_scores","title":"compute_scores","text":"<pre><code>compute_scores(\n    match_results: list[tuple[str, str, Winner]],\n) -&gt; dict[str, float]\n</code></pre> <p>\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001\u5404\u30e2\u30c7\u30eb\u306e\u52dd\u7387\u3092\u8fd4\u3059\u3002</p> Source code in <code>flexeval/core/pairwise_comparison/scorer/win_rate.py</code> <pre><code>def compute_scores(\n    self,\n    match_results: list[tuple[str, str, Winner]],\n) -&gt; dict[str, float]:\n    \"\"\"\u6226\u7e3e\u3092\u53d7\u3051\u53d6\u308a\u3001\u5404\u30e2\u30c7\u30eb\u306e\u52dd\u7387\u3092\u8fd4\u3059\u3002\"\"\"\n    match_count_dict: dict[str, float] = defaultdict(float)\n    win_count_dict: dict[str, float] = defaultdict(float)\n\n    for model1, model2, winner in match_results:\n        match_count_dict[model1] += 1\n        match_count_dict[model2] += 1\n        if winner == Winner.MODEL1:\n            win_count_dict[model1] += 1\n        elif winner == Winner.MODEL2:\n            win_count_dict[model2] += 1\n        elif winner == Winner.DRAW:\n            win_count_dict[model1] += 0.5\n            win_count_dict[model2] += 0.5\n\n    win_rate_dict = {}\n    for model in match_count_dict:\n        win_rate_dict[model] = 100 * win_count_dict.get(model, 0.0) / match_count_dict[model]\n\n    return dict(sorted(win_rate_dict.items(), key=lambda x: -x[1]))\n</code></pre>"},{"location":"api_reference/PromptTemplate/","title":"PromptTemplate","text":""},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.base.PromptTemplate","title":"PromptTemplate","text":"<p>This class embeds task inputs <code>GenerationInstance</code> or <code>MultipleChoiceInstance</code> into a text that can be used as a prompt for <code>LanguageModel</code>.</p> Source code in <code>flexeval/core/prompt_template/base.py</code> <pre><code>class PromptTemplate(ABC):\n    \"\"\"\n    This class embeds task inputs `GenerationInstance` or `MultipleChoiceInstance` into a text that can be used\n    as a prompt for `LanguageModel`.\n    \"\"\"\n\n    @abstractmethod\n    def embed_inputs(self, input_dict: dict[str, Any]) -&gt; str:\n        \"\"\"\n        Embeds the input into a prompt template.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.base.PromptTemplate.embed_inputs","title":"embed_inputs  <code>abstractmethod</code>","text":"<pre><code>embed_inputs(input_dict: dict[str, Any]) -&gt; str\n</code></pre> <p>Embeds the input into a prompt template.</p> Source code in <code>flexeval/core/prompt_template/base.py</code> <pre><code>@abstractmethod\ndef embed_inputs(self, input_dict: dict[str, Any]) -&gt; str:\n    \"\"\"\n    Embeds the input into a prompt template.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.jinja2.Jinja2PromptTemplate","title":"Jinja2PromptTemplate","text":"<p>Embed task inputs using Jinja2 template engine.</p> <p>Parameters:</p> <ul> <li> <code>template</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The Jinja2 template to use.</p> </li> <li> <code>template_path</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The path to a file with the Jinja2 template to use.</p> </li> </ul> Source code in <code>flexeval/core/prompt_template/jinja2.py</code> <pre><code>class Jinja2PromptTemplate(PromptTemplate):\n    \"\"\"\n    Embed task inputs using Jinja2 template engine.\n\n    Args:\n        template: The Jinja2 template to use.\n        template_path: The path to a file with the Jinja2 template to use.\n    \"\"\"\n\n    def __init__(self, template: str | None = None, template_path: str | None = None) -&gt; None:\n        if template is None and template_path is None:\n            msg = \"Either template or template_path must be provided\"\n            raise ValueError(msg)\n        if template is not None and template_path is not None:\n            msg = \"Only one of template or template_path can be provided\"\n            raise ValueError(msg)\n\n        if template_path is not None:\n            with open(template_path) as f:\n                self.template = f.read()\n        else:\n            self.template = template\n\n        self.compiled_template = JINJA2_ENV.from_string(self.template)\n\n    def embed_inputs(self, input_dict: dict[str, Any]) -&gt; str:\n        return self.compiled_template.render(input_dict)\n\n    def __repr__(self) -&gt; str:\n        return f\"Jinja2PromptTemplate(template={self.template!r})\"\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.jinja2.Jinja2PromptTemplate.template","title":"template  <code>instance-attribute</code>","text":"<pre><code>template = read()\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.jinja2.Jinja2PromptTemplate.compiled_template","title":"compiled_template  <code>instance-attribute</code>","text":"<pre><code>compiled_template = from_string(template)\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.jinja2.Jinja2PromptTemplate.__init__","title":"__init__","text":"<pre><code>__init__(\n    template: str | None = None,\n    template_path: str | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/prompt_template/jinja2.py</code> <pre><code>def __init__(self, template: str | None = None, template_path: str | None = None) -&gt; None:\n    if template is None and template_path is None:\n        msg = \"Either template or template_path must be provided\"\n        raise ValueError(msg)\n    if template is not None and template_path is not None:\n        msg = \"Only one of template or template_path can be provided\"\n        raise ValueError(msg)\n\n    if template_path is not None:\n        with open(template_path) as f:\n            self.template = f.read()\n    else:\n        self.template = template\n\n    self.compiled_template = JINJA2_ENV.from_string(self.template)\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.jinja2.Jinja2PromptTemplate.embed_inputs","title":"embed_inputs","text":"<pre><code>embed_inputs(input_dict: dict[str, Any]) -&gt; str\n</code></pre> Source code in <code>flexeval/core/prompt_template/jinja2.py</code> <pre><code>def embed_inputs(self, input_dict: dict[str, Any]) -&gt; str:\n    return self.compiled_template.render(input_dict)\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.jinja2.Jinja2PromptTemplate.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/prompt_template/jinja2.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"Jinja2PromptTemplate(template={self.template!r})\"\n</code></pre>"},{"location":"api_reference/PromptTemplate/#flexeval.core.prompt_template.jinja2.instantiate_prompt_template_from_string","title":"instantiate_prompt_template_from_string","text":"<pre><code>instantiate_prompt_template_from_string(\n    template_or_path: str,\n) -&gt; Jinja2PromptTemplate\n</code></pre> Source code in <code>flexeval/core/prompt_template/jinja2.py</code> <pre><code>def instantiate_prompt_template_from_string(template_or_path: str) -&gt; Jinja2PromptTemplate:\n    # Use `os.path.isfile` instead of `Path.is_file()` to avoid \"OSError: [Errno 36] File name too long\"\n    if os.path.isfile(template_or_path):  # noqa: PTH113\n        return Jinja2PromptTemplate(template_path=template_or_path)\n    return Jinja2PromptTemplate(template=template_or_path)\n</code></pre>"},{"location":"api_reference/ResultRecorder/","title":"ResultRecorder","text":""},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.base.ResultRecorder","title":"ResultRecorder","text":"<p>An abstract base class for recording experiment results, including configuration, metrics, and model outputs.</p> <p>This class defines the interface for different result recording implementations, such as saving to a local directory, uploading to wandb, or integrating with MLflow.</p> Source code in <code>flexeval/core/result_recorder/base.py</code> <pre><code>class ResultRecorder(ABC):\n    \"\"\"\n    An abstract base class for recording experiment results, including configuration,\n    metrics, and model outputs.\n\n    This class defines the interface for different result recording implementations,\n    such as saving to a local directory, uploading to wandb, or integrating with MLflow.\n    \"\"\"\n\n    @abstractmethod\n    def record_config(self, config: dict[str, Any], group: str | None = None) -&gt; None:\n        \"\"\"\n        Record the configuration parameters of the experiment.\n\n        Args:\n            config: A dictionary containing the configuration\n                parameters of the evaluation.\n            group: An optional group name to organize the configuration.\n        \"\"\"\n\n    @abstractmethod\n    def record_metrics(self, metrics: dict[str, Any], group: str | None = None) -&gt; None:\n        \"\"\"\n        Record the evaluation metrics of the experiment.\n\n        Args:\n            metrics: A dictionary containing the evaluation metrics,\n                where keys are metric names and values are the corresponding results.\n            group: An optional group name to organize the metrics.\n        \"\"\"\n\n    @abstractmethod\n    def record_model_outputs(self, model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None:\n        \"\"\"\n        Record the outputs generated by the model during evaluation.\n\n        Args:\n            model_outputs: A list of dictionaries, where each\n                dictionary represents a single model output. The structure of these\n                dictionaries may vary depending on the specific model and task.\n            group: An optional group name to organize the model outputs.\n        \"\"\"\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.base.ResultRecorder.record_config","title":"record_config  <code>abstractmethod</code>","text":"<pre><code>record_config(\n    config: dict[str, Any], group: str | None = None\n) -&gt; None\n</code></pre> <p>Record the configuration parameters of the experiment.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, Any]</code>)           \u2013            <p>A dictionary containing the configuration parameters of the evaluation.</p> </li> <li> <code>group</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional group name to organize the configuration.</p> </li> </ul> Source code in <code>flexeval/core/result_recorder/base.py</code> <pre><code>@abstractmethod\ndef record_config(self, config: dict[str, Any], group: str | None = None) -&gt; None:\n    \"\"\"\n    Record the configuration parameters of the experiment.\n\n    Args:\n        config: A dictionary containing the configuration\n            parameters of the evaluation.\n        group: An optional group name to organize the configuration.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.base.ResultRecorder.record_metrics","title":"record_metrics  <code>abstractmethod</code>","text":"<pre><code>record_metrics(\n    metrics: dict[str, Any], group: str | None = None\n) -&gt; None\n</code></pre> <p>Record the evaluation metrics of the experiment.</p> <p>Parameters:</p> <ul> <li> <code>metrics</code>               (<code>dict[str, Any]</code>)           \u2013            <p>A dictionary containing the evaluation metrics, where keys are metric names and values are the corresponding results.</p> </li> <li> <code>group</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional group name to organize the metrics.</p> </li> </ul> Source code in <code>flexeval/core/result_recorder/base.py</code> <pre><code>@abstractmethod\ndef record_metrics(self, metrics: dict[str, Any], group: str | None = None) -&gt; None:\n    \"\"\"\n    Record the evaluation metrics of the experiment.\n\n    Args:\n        metrics: A dictionary containing the evaluation metrics,\n            where keys are metric names and values are the corresponding results.\n        group: An optional group name to organize the metrics.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.base.ResultRecorder.record_model_outputs","title":"record_model_outputs  <code>abstractmethod</code>","text":"<pre><code>record_model_outputs(\n    model_outputs: list[dict[str, Any]],\n    group: str | None = None,\n) -&gt; None\n</code></pre> <p>Record the outputs generated by the model during evaluation.</p> <p>Parameters:</p> <ul> <li> <code>model_outputs</code>               (<code>list[dict[str, Any]]</code>)           \u2013            <p>A list of dictionaries, where each dictionary represents a single model output. The structure of these dictionaries may vary depending on the specific model and task.</p> </li> <li> <code>group</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>An optional group name to organize the model outputs.</p> </li> </ul> Source code in <code>flexeval/core/result_recorder/base.py</code> <pre><code>@abstractmethod\ndef record_model_outputs(self, model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None:\n    \"\"\"\n    Record the outputs generated by the model during evaluation.\n\n    Args:\n        model_outputs: A list of dictionaries, where each\n            dictionary represents a single model output. The structure of these\n            dictionaries may vary depending on the specific model and task.\n        group: An optional group name to organize the model outputs.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder","title":"LocalRecorder","text":"<p>A class to record the results in JSON format.</p> <p>Parameters:</p> <ul> <li> <code>output_dir</code>               (<code>str</code>)           \u2013            <p>The directory to save the results.</p> </li> </ul> Source code in <code>flexeval/core/result_recorder/local_recorder.py</code> <pre><code>class LocalRecorder(ResultRecorder):\n    \"\"\"\n    A class to record the results in JSON format.\n\n    Args:\n        output_dir: The directory to save the results.\n    \"\"\"\n\n    def __init__(self, output_dir: str, force: bool = False) -&gt; None:\n        self.output_dir = Path(output_dir)\n        self.force = force\n\n    @staticmethod\n    def _check_output_dir_exists(output_dir: str | PathLike[str], checked_files: list[str]) -&gt; None:\n        output_dir = Path(output_dir)\n        for file_name in checked_files:\n            if (output_dir / file_name).exists():\n                msg = (\n                    f\"`{output_dir / file_name}` already exists. If you want to overwrite it, \"\n                    f\"please specify `--force true` from CLI or `force=True` when initializing the recorder.\"\n                )\n                raise FileExistsError(msg)\n\n    def record_config(self, config: dict[str, Any], group: str | None = None) -&gt; None:\n        output_dir = self.output_dir\n        if group is not None:\n            output_dir = self.output_dir / group\n\n        if not self.force:\n            self._check_output_dir_exists(output_dir, [CONFIG_FILE_NAME])\n\n        save_json(config, output_dir / CONFIG_FILE_NAME)\n        logger.info(f\"Saved the config to {output_dir / CONFIG_FILE_NAME}\")\n\n    def record_metrics(self, metrics: dict[str, Any], group: str | None = None) -&gt; None:\n        output_dir = self.output_dir\n        if group is not None:\n            output_dir = self.output_dir / group\n\n        if not self.force:\n            self._check_output_dir_exists(output_dir, [METRIC_FILE_NAME])\n\n        save_json(metrics, output_dir / METRIC_FILE_NAME)\n        logger.info(f\"Saved the metrics to {output_dir / METRIC_FILE_NAME}\")\n\n    def record_model_outputs(self, model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None:\n        output_dir = self.output_dir\n        if group is not None:\n            output_dir = output_dir / group\n\n        if not self.force:\n            self._check_output_dir_exists(output_dir, [OUTPUTS_FILE_NAME])\n\n        save_jsonl(model_outputs, output_dir / OUTPUTS_FILE_NAME)\n        logger.info(f\"Saved the outputs to {output_dir / OUTPUTS_FILE_NAME}\")\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder.output_dir","title":"output_dir  <code>instance-attribute</code>","text":"<pre><code>output_dir = Path(output_dir)\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder.force","title":"force  <code>instance-attribute</code>","text":"<pre><code>force = force\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder.__init__","title":"__init__","text":"<pre><code>__init__(output_dir: str, force: bool = False) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/local_recorder.py</code> <pre><code>def __init__(self, output_dir: str, force: bool = False) -&gt; None:\n    self.output_dir = Path(output_dir)\n    self.force = force\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder.record_config","title":"record_config","text":"<pre><code>record_config(\n    config: dict[str, Any], group: str | None = None\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/local_recorder.py</code> <pre><code>def record_config(self, config: dict[str, Any], group: str | None = None) -&gt; None:\n    output_dir = self.output_dir\n    if group is not None:\n        output_dir = self.output_dir / group\n\n    if not self.force:\n        self._check_output_dir_exists(output_dir, [CONFIG_FILE_NAME])\n\n    save_json(config, output_dir / CONFIG_FILE_NAME)\n    logger.info(f\"Saved the config to {output_dir / CONFIG_FILE_NAME}\")\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder.record_metrics","title":"record_metrics","text":"<pre><code>record_metrics(\n    metrics: dict[str, Any], group: str | None = None\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/local_recorder.py</code> <pre><code>def record_metrics(self, metrics: dict[str, Any], group: str | None = None) -&gt; None:\n    output_dir = self.output_dir\n    if group is not None:\n        output_dir = self.output_dir / group\n\n    if not self.force:\n        self._check_output_dir_exists(output_dir, [METRIC_FILE_NAME])\n\n    save_json(metrics, output_dir / METRIC_FILE_NAME)\n    logger.info(f\"Saved the metrics to {output_dir / METRIC_FILE_NAME}\")\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.local_recorder.LocalRecorder.record_model_outputs","title":"record_model_outputs","text":"<pre><code>record_model_outputs(\n    model_outputs: list[dict[str, Any]],\n    group: str | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/local_recorder.py</code> <pre><code>def record_model_outputs(self, model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None:\n    output_dir = self.output_dir\n    if group is not None:\n        output_dir = output_dir / group\n\n    if not self.force:\n        self._check_output_dir_exists(output_dir, [OUTPUTS_FILE_NAME])\n\n    save_jsonl(model_outputs, output_dir / OUTPUTS_FILE_NAME)\n    logger.info(f\"Saved the outputs to {output_dir / OUTPUTS_FILE_NAME}\")\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.wandb_recorder.WandBRecorder","title":"WandBRecorder","text":"<p>A class to record the results to Weights &amp; Biases.</p> <p>Parameters:</p> <ul> <li> <code>init_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>The arguments for the <code>wandb.init</code> function. Please refer to the official documentation for the details.</p> </li> </ul> Source code in <code>flexeval/core/result_recorder/wandb_recorder.py</code> <pre><code>class WandBRecorder(ResultRecorder):\n    \"\"\"\n    A class to record the results to Weights &amp; Biases.\n\n    Args:\n        init_kwargs: The arguments for the `wandb.init` function.\n            Please refer to [the official documentation](https://docs.wandb.ai/ref/python/init) for the details.\n    \"\"\"\n\n    def __init__(\n        self,\n        init_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        import wandb\n\n        self._wandb = wandb\n        init_kwargs = init_kwargs or {}\n        self._wandb.init(**init_kwargs)\n\n    def record_config(self, config: dict[str, Any], group: str | None = None) -&gt; None:\n        if group:\n            self._wandb.config.update({group: config})\n        else:\n            self._wandb.config.update(config)\n\n    def record_metrics(self, metrics: dict[str, Any], group: str | None = None) -&gt; None:\n        if group:\n            self._wandb.summary.update({group: metrics})\n        else:\n            self._wandb.summary.update(metrics)\n\n    def record_model_outputs(self, model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None:\n        table = self._wandb.Table(columns=list(model_outputs[0].keys()))\n\n        for output in model_outputs:\n            table.add_data(*output.values())\n\n        table_name = \"model_outputs\" if group is None else f\"{group}/model_outputs\"\n        self._wandb.log({table_name: table})\n\n    def __del__(self) -&gt; None:\n        self._wandb.finish()\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.wandb_recorder.WandBRecorder.__init__","title":"__init__","text":"<pre><code>__init__(init_kwargs: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/wandb_recorder.py</code> <pre><code>def __init__(\n    self,\n    init_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    import wandb\n\n    self._wandb = wandb\n    init_kwargs = init_kwargs or {}\n    self._wandb.init(**init_kwargs)\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.wandb_recorder.WandBRecorder.record_config","title":"record_config","text":"<pre><code>record_config(\n    config: dict[str, Any], group: str | None = None\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/wandb_recorder.py</code> <pre><code>def record_config(self, config: dict[str, Any], group: str | None = None) -&gt; None:\n    if group:\n        self._wandb.config.update({group: config})\n    else:\n        self._wandb.config.update(config)\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.wandb_recorder.WandBRecorder.record_metrics","title":"record_metrics","text":"<pre><code>record_metrics(\n    metrics: dict[str, Any], group: str | None = None\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/wandb_recorder.py</code> <pre><code>def record_metrics(self, metrics: dict[str, Any], group: str | None = None) -&gt; None:\n    if group:\n        self._wandb.summary.update({group: metrics})\n    else:\n        self._wandb.summary.update(metrics)\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.wandb_recorder.WandBRecorder.record_model_outputs","title":"record_model_outputs","text":"<pre><code>record_model_outputs(\n    model_outputs: list[dict[str, Any]],\n    group: str | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/wandb_recorder.py</code> <pre><code>def record_model_outputs(self, model_outputs: list[dict[str, Any]], group: str | None = None) -&gt; None:\n    table = self._wandb.Table(columns=list(model_outputs[0].keys()))\n\n    for output in model_outputs:\n        table.add_data(*output.values())\n\n    table_name = \"model_outputs\" if group is None else f\"{group}/model_outputs\"\n    self._wandb.log({table_name: table})\n</code></pre>"},{"location":"api_reference/ResultRecorder/#flexeval.core.result_recorder.wandb_recorder.WandBRecorder.__del__","title":"__del__","text":"<pre><code>__del__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/result_recorder/wandb_recorder.py</code> <pre><code>def __del__(self) -&gt; None:\n    self._wandb.finish()\n</code></pre>"},{"location":"api_reference/RewardModel/","title":"RewardModel","text":""},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.base.RewardModel","title":"RewardModel","text":"<p>Base class for reward models.</p> Source code in <code>flexeval/core/reward_model/base.py</code> <pre><code>class RewardModel(ABC):\n    \"\"\"Base class for reward models.\"\"\"\n\n    @abstractmethod\n    def batch_judge(\n        self,\n        batch_reward_bench_instances: list[RewardBenchInstance],\n    ) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n        \"\"\"Judge a batch of reward bench instances.\n\n        Args:\n            batch_reward_bench_instances (list[RewardBenchInstance]): A list of tuples, each containing two model items.\n\n        Returns:\n            tuple[list[bool], list[Any]]: A tuple with the following elements:\n                - chosen_is_betters: Indicating whether each `chosen` item is considered better by the model.\n                - judge_outputs: A list of outputs (rationale, score, etc....) from the model.\n        \"\"\"\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.base.RewardModel.batch_judge","title":"batch_judge  <code>abstractmethod</code>","text":"<pre><code>batch_judge(\n    batch_reward_bench_instances: list[RewardBenchInstance],\n) -&gt; tuple[list[bool], list[dict[str, Any]]]\n</code></pre> <p>Judge a batch of reward bench instances.</p> <p>Parameters:</p> <ul> <li> <code>batch_reward_bench_instances</code>               (<code>list[RewardBenchInstance]</code>)           \u2013            <p>A list of tuples, each containing two model items.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[bool], list[dict[str, Any]]]</code>           \u2013            <p>tuple[list[bool], list[Any]]: A tuple with the following elements: - chosen_is_betters: Indicating whether each <code>chosen</code> item is considered better by the model. - judge_outputs: A list of outputs (rationale, score, etc....) from the model.</p> </li> </ul> Source code in <code>flexeval/core/reward_model/base.py</code> <pre><code>@abstractmethod\ndef batch_judge(\n    self,\n    batch_reward_bench_instances: list[RewardBenchInstance],\n) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n    \"\"\"Judge a batch of reward bench instances.\n\n    Args:\n        batch_reward_bench_instances (list[RewardBenchInstance]): A list of tuples, each containing two model items.\n\n    Returns:\n        tuple[list[bool], list[Any]]: A tuple with the following elements:\n            - chosen_is_betters: Indicating whether each `chosen` item is considered better by the model.\n            - judge_outputs: A list of outputs (rationale, score, etc....) from the model.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.log_prob.LogProbRewardModel","title":"LogProbRewardModel","text":"<p>A reward model that judges the quality of a response based on the log probability computed by the auto-regressive language model.</p> Source code in <code>flexeval/core/reward_model/log_prob.py</code> <pre><code>class LogProbRewardModel(RewardModel):\n    \"\"\"\n    A reward model that judges the quality of a response\n    based on the log probability computed by the auto-regressive language model.\n    \"\"\"\n\n    def __init__(self, language_model: LanguageModel) -&gt; None:\n        self.language_model = language_model\n\n    def batch_judge(\n        self,\n        batch_reward_bench_instances: list[RewardBenchInstance],\n    ) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n        if not all(len(instance.chosen) == 1 for instance in batch_reward_bench_instances):\n            msg = \"`chosen` field must have exactly one element.\"\n            raise ValueError(msg)\n        if not all(len(instance.rejected) == 1 for instance in batch_reward_bench_instances):\n            msg = \"`rejected` field must have exactly one element.\"\n            raise ValueError(msg)\n\n        chosen_log_probs = self.language_model.compute_chat_log_probs(\n            prompt=[instance.prompt for instance in batch_reward_bench_instances],\n            response=[instance.chosen[0] for instance in batch_reward_bench_instances],\n        )\n        rejected_log_probs = self.language_model.compute_chat_log_probs(\n            prompt=[instance.prompt for instance in batch_reward_bench_instances],\n            response=[instance.rejected[0] for instance in batch_reward_bench_instances],\n        )\n        chosen_is_better = [\n            chosen_log_prob &gt; rejected_log_prob\n            for chosen_log_prob, rejected_log_prob in zip(chosen_log_probs, rejected_log_probs)\n        ]\n        outputs = [\n            {\n                \"chosen_log_prob\": chosen_log_prob,\n                \"rejected_log_prob\": rejected_log_prob,\n            }\n            for chosen_log_prob, rejected_log_prob in zip(chosen_log_probs, rejected_log_probs)\n        ]\n        return chosen_is_better, outputs\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.log_prob.LogProbRewardModel.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.log_prob.LogProbRewardModel.__init__","title":"__init__","text":"<pre><code>__init__(language_model: LanguageModel) -&gt; None\n</code></pre> Source code in <code>flexeval/core/reward_model/log_prob.py</code> <pre><code>def __init__(self, language_model: LanguageModel) -&gt; None:\n    self.language_model = language_model\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.log_prob.LogProbRewardModel.batch_judge","title":"batch_judge","text":"<pre><code>batch_judge(\n    batch_reward_bench_instances: list[RewardBenchInstance],\n) -&gt; tuple[list[bool], list[dict[str, Any]]]\n</code></pre> Source code in <code>flexeval/core/reward_model/log_prob.py</code> <pre><code>def batch_judge(\n    self,\n    batch_reward_bench_instances: list[RewardBenchInstance],\n) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n    if not all(len(instance.chosen) == 1 for instance in batch_reward_bench_instances):\n        msg = \"`chosen` field must have exactly one element.\"\n        raise ValueError(msg)\n    if not all(len(instance.rejected) == 1 for instance in batch_reward_bench_instances):\n        msg = \"`rejected` field must have exactly one element.\"\n        raise ValueError(msg)\n\n    chosen_log_probs = self.language_model.compute_chat_log_probs(\n        prompt=[instance.prompt for instance in batch_reward_bench_instances],\n        response=[instance.chosen[0] for instance in batch_reward_bench_instances],\n    )\n    rejected_log_probs = self.language_model.compute_chat_log_probs(\n        prompt=[instance.prompt for instance in batch_reward_bench_instances],\n        response=[instance.rejected[0] for instance in batch_reward_bench_instances],\n    )\n    chosen_is_better = [\n        chosen_log_prob &gt; rejected_log_prob\n        for chosen_log_prob, rejected_log_prob in zip(chosen_log_probs, rejected_log_probs)\n    ]\n    outputs = [\n        {\n            \"chosen_log_prob\": chosen_log_prob,\n            \"rejected_log_prob\": rejected_log_prob,\n        }\n        for chosen_log_prob, rejected_log_prob in zip(chosen_log_probs, rejected_log_probs)\n    ]\n    return chosen_is_better, outputs\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel","title":"PairwiseJudgeRewardModel","text":"<p>Pairwise judge using a chat language model to compare two model or human outputs. The reward model\u2019s judgment is counted as correct only if it is order\u2011invariant: when given (A = chosen, B = rejected) it prefers A, and when the inputs are swapped (A = rejected, B = chosen) it prefers B.</p> <p>Examples:</p> <ul> <li>\u2705 Correct (order\u2011invariant):</li> <li>judge(prompt, A=chosen, B=rejected) \u2192 A</li> <li>judge(prompt, A=rejected, B=chosen) \u2192 B</li> <li>\u274c Incorrect (position bias; same answer regardless of order):</li> <li>judge(prompt, A=chosen, B=rejected) \u2192 A</li> <li>judge(prompt, A=rejected, B=chosen) \u2192 A</li> <li>\u274c Incorrect (both wrong):</li> <li>judge(prompt, A=chosen, B=rejected) \u2192 B</li> <li>judge(prompt, A=rejected, B=chosen) \u2192 A</li> </ul> <p>Parameters:</p> <ul> <li> <code>language_model</code>               (<code>LanguageModel</code>)           \u2013            <p>The language model to use for pairwise comparison.             This model is expected to output PairwiseChoice.</p> </li> <li> <code>prompt_template</code>               (<code>PromptTemplate</code>)           \u2013            <p>The prompt template to embed the model outputs to be compared.              Be sure to include {{prompt}}, {{answer_a}}, and {{answer_b}}.</p> </li> <li> <code>system_message</code>               (<code>str | PromptTemplate | None</code>, default:                   <code>None</code> )           \u2013            <p>The system message to prepend to the chat messages.</p> </li> <li> <code>gen_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Generation kwargs for the language model.</p> </li> </ul> Source code in <code>flexeval/core/reward_model/pairwise_judge_reward_model.py</code> <pre><code>class PairwiseJudgeRewardModel(RewardModel):\n    \"\"\"Pairwise judge using a chat language model to compare two model or human\n    outputs.\n    The reward model\u2019s judgment is counted as **correct only if it is order\u2011invariant**:\n    when given (A = chosen, B = rejected) it prefers **A**, and when the inputs are swapped\n    (A = rejected, B = chosen) it prefers **B**.\n\n    Examples:\n    - \u2705 Correct (order\u2011invariant):\n      - judge(prompt, A=chosen, B=rejected) \u2192 **A**\n      - judge(prompt, A=rejected, B=chosen) \u2192 **B**\n    - \u274c Incorrect (position bias; same answer regardless of order):\n      - judge(prompt, A=chosen, B=rejected) \u2192 **A**\n      - judge(prompt, A=rejected, B=chosen) \u2192 **A**\n    - \u274c Incorrect (both wrong):\n      - judge(prompt, A=chosen, B=rejected) \u2192 **B**\n      - judge(prompt, A=rejected, B=chosen) \u2192 **A**\n\n    Args:\n        language_model: The language model to use for pairwise comparison.\n                        This model is expected to output PairwiseChoice.\n        prompt_template: The prompt template to embed the model outputs to be compared.\n                         Be sure to include {{prompt}}, {{answer_a}}, and {{answer_b}}.\n        system_message: The system message to prepend to the chat messages.\n        gen_kwargs: Generation kwargs for the language model.\n    \"\"\"\n\n    def __init__(\n        self,\n        language_model: LanguageModel,\n        prompt_template: PromptTemplate,\n        system_message: str | PromptTemplate | None = None,\n        gen_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        if gen_kwargs is None:\n            gen_kwargs = {}\n        self.language_model = language_model\n        self.prompt_template = prompt_template\n        self.system_message = system_message\n        self.gen_kwargs = gen_kwargs\n\n    def _create_input_chat_messages_list(self, pairwise_instance: PairwiseInstance) -&gt; list[dict[str, str]]:\n        pairwise_instance_asdict = asdict(pairwise_instance)\n        judge_input = self.prompt_template.embed_inputs(pairwise_instance_asdict)\n        input_chat_messages = [{\"role\": \"user\", \"content\": judge_input}]\n        if self.system_message:\n            if isinstance(self.system_message, str):\n                system_message = self.system_message\n            elif isinstance(self.system_message, PromptTemplate):\n                system_message = self.system_message.embed_inputs(pairwise_instance_asdict)\n            else:\n                msg = \"system_message should be str or PromptTemplate.\"\n                raise ValueError(msg)\n            input_chat_messages.insert(\n                0,\n                {\"role\": \"system\", \"content\": system_message},\n            )\n        return input_chat_messages\n\n    def batch_judge(\n        self,\n        batch_reward_bench_instances: list[RewardBenchInstance],\n    ) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n        input_chat_messages_list: list[list[dict[str, str]]] = []\n        all_pairwise_instances: list[PairwiseInstance] = []\n        outputs: list[dict[str, Any]] = []\n        for reward_bench_instance in batch_reward_bench_instances:\n            # to address position biases, create two inputs by swapping chosen/rejected orderings\n            pairwise_instance_answer_a_is_chosen = PairwiseInstance(\n                prompt=reward_bench_instance.prompt,\n                answer_a=reward_bench_instance.chosen,\n                answer_b=reward_bench_instance.rejected,\n                answer_label=PairwiseChoice.A,\n            )\n            input_chat_messages_a_is_chosen = self._create_input_chat_messages_list(\n                pairwise_instance_answer_a_is_chosen\n            )\n            input_chat_messages_list.append(input_chat_messages_a_is_chosen)\n\n            pairwise_instance_answer_b_is_chosen = PairwiseInstance(\n                prompt=reward_bench_instance.prompt,\n                answer_a=reward_bench_instance.rejected,\n                answer_b=reward_bench_instance.chosen,\n                answer_label=PairwiseChoice.B,\n            )\n            input_chat_messages_b_is_chosen = self._create_input_chat_messages_list(\n                pairwise_instance_answer_b_is_chosen\n            )\n            input_chat_messages_list.append(input_chat_messages_b_is_chosen)\n            all_pairwise_instances += [pairwise_instance_answer_a_is_chosen, pairwise_instance_answer_b_is_chosen]\n\n            output = {\n                \"llm_inputs\": [input_chat_messages_a_is_chosen, input_chat_messages_b_is_chosen],\n            }\n            outputs.append(output)\n        judge_outputs = self.language_model.generate_chat_response(input_chat_messages_list, **self.gen_kwargs)\n        chosen_is_better_list: list[bool] = [\n            evaluate_model_output(judge_output.text, pairwise_instance.answer_label)\n            for judge_output, pairwise_instance in zip(judge_outputs, all_pairwise_instances)\n        ]\n\n        if len(outputs) * 2 != len(chosen_is_better_list):\n            msg = \"The number of outputs should be twice the number of inputs.\"\n            raise ValueError(msg)\n\n        aggregated_results, aggregated_outputs = aggregate_judge_results(outputs, judge_outputs, chosen_is_better_list)\n\n        return aggregated_results, aggregated_outputs\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel.language_model","title":"language_model  <code>instance-attribute</code>","text":"<pre><code>language_model = language_model\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel.prompt_template","title":"prompt_template  <code>instance-attribute</code>","text":"<pre><code>prompt_template = prompt_template\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel.system_message","title":"system_message  <code>instance-attribute</code>","text":"<pre><code>system_message = system_message\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel.gen_kwargs","title":"gen_kwargs  <code>instance-attribute</code>","text":"<pre><code>gen_kwargs = gen_kwargs\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel.__init__","title":"__init__","text":"<pre><code>__init__(\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    system_message: str | PromptTemplate | None = None,\n    gen_kwargs: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/reward_model/pairwise_judge_reward_model.py</code> <pre><code>def __init__(\n    self,\n    language_model: LanguageModel,\n    prompt_template: PromptTemplate,\n    system_message: str | PromptTemplate | None = None,\n    gen_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    if gen_kwargs is None:\n        gen_kwargs = {}\n    self.language_model = language_model\n    self.prompt_template = prompt_template\n    self.system_message = system_message\n    self.gen_kwargs = gen_kwargs\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.pairwise_judge_reward_model.PairwiseJudgeRewardModel.batch_judge","title":"batch_judge","text":"<pre><code>batch_judge(\n    batch_reward_bench_instances: list[RewardBenchInstance],\n) -&gt; tuple[list[bool], list[dict[str, Any]]]\n</code></pre> Source code in <code>flexeval/core/reward_model/pairwise_judge_reward_model.py</code> <pre><code>def batch_judge(\n    self,\n    batch_reward_bench_instances: list[RewardBenchInstance],\n) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n    input_chat_messages_list: list[list[dict[str, str]]] = []\n    all_pairwise_instances: list[PairwiseInstance] = []\n    outputs: list[dict[str, Any]] = []\n    for reward_bench_instance in batch_reward_bench_instances:\n        # to address position biases, create two inputs by swapping chosen/rejected orderings\n        pairwise_instance_answer_a_is_chosen = PairwiseInstance(\n            prompt=reward_bench_instance.prompt,\n            answer_a=reward_bench_instance.chosen,\n            answer_b=reward_bench_instance.rejected,\n            answer_label=PairwiseChoice.A,\n        )\n        input_chat_messages_a_is_chosen = self._create_input_chat_messages_list(\n            pairwise_instance_answer_a_is_chosen\n        )\n        input_chat_messages_list.append(input_chat_messages_a_is_chosen)\n\n        pairwise_instance_answer_b_is_chosen = PairwiseInstance(\n            prompt=reward_bench_instance.prompt,\n            answer_a=reward_bench_instance.rejected,\n            answer_b=reward_bench_instance.chosen,\n            answer_label=PairwiseChoice.B,\n        )\n        input_chat_messages_b_is_chosen = self._create_input_chat_messages_list(\n            pairwise_instance_answer_b_is_chosen\n        )\n        input_chat_messages_list.append(input_chat_messages_b_is_chosen)\n        all_pairwise_instances += [pairwise_instance_answer_a_is_chosen, pairwise_instance_answer_b_is_chosen]\n\n        output = {\n            \"llm_inputs\": [input_chat_messages_a_is_chosen, input_chat_messages_b_is_chosen],\n        }\n        outputs.append(output)\n    judge_outputs = self.language_model.generate_chat_response(input_chat_messages_list, **self.gen_kwargs)\n    chosen_is_better_list: list[bool] = [\n        evaluate_model_output(judge_output.text, pairwise_instance.answer_label)\n        for judge_output, pairwise_instance in zip(judge_outputs, all_pairwise_instances)\n    ]\n\n    if len(outputs) * 2 != len(chosen_is_better_list):\n        msg = \"The number of outputs should be twice the number of inputs.\"\n        raise ValueError(msg)\n\n    aggregated_results, aggregated_outputs = aggregate_judge_results(outputs, judge_outputs, chosen_is_better_list)\n\n    return aggregated_results, aggregated_outputs\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.sequence_classification.SequenceClassificationRewardModel","title":"SequenceClassificationRewardModel","text":"<p>Pairwise judge using a chat language model to compare two model or human outputs.</p> Source code in <code>flexeval/core/reward_model/sequence_classification.py</code> <pre><code>class SequenceClassificationRewardModel(RewardModel):\n    \"\"\"Pairwise judge using a chat language model to compare two model or human\n    outputs.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        model_kwargs: dict[str, Any] | None = None,\n        tokenizer: str | None = None,\n        tokenizer_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        tokenizer = tokenizer if tokenizer else model\n        tokenizer_kwargs = tokenizer_kwargs or {}\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer, **tokenizer_kwargs)\n\n        model_kwargs = get_default_model_kwargs(model_kwargs)\n        self.model = AutoModelForSequenceClassification.from_pretrained(model, **model_kwargs)\n        # Set pad_token_id if not set\n        # to avoid \"ValueError: Cannot handle batch sizes &gt; 1 if no padding token is defined.\" in self.model()\n        if self.model.config.pad_token_id is None:\n            self.model.config.pad_token_id = self.tokenizer.pad_token_id\n        self.model.eval()\n\n    @torch.inference_mode()\n    def batch_judge(\n        self,\n        batch_reward_bench_instances: list[RewardBenchInstance],\n    ) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n        chosen_messages = [instance.prompt + instance.chosen for instance in batch_reward_bench_instances]\n        chosen_inputs = self.tokenizer.apply_chat_template(\n            chosen_messages, return_tensors=\"pt\", padding=True, return_dict=True\n        )\n        chosen_outputs = self.model(**{k: v.to(self.model.device) for k, v in chosen_inputs.items()})\n        chosen_rewards = chosen_outputs.logits[:, 0]\n\n        rejected_messages = [instance.prompt + instance.rejected for instance in batch_reward_bench_instances]\n        rejected_inputs = self.tokenizer.apply_chat_template(\n            rejected_messages, return_tensors=\"pt\", padding=True, return_dict=True\n        )\n        rejected_outputs = self.model(**{k: v.to(self.model.device) for k, v in rejected_inputs.items()})\n        rejected_rewards = rejected_outputs.logits[:, 0]\n\n        chosen_is_better = (chosen_rewards &gt; rejected_rewards).tolist()\n        outputs = [\n            {\n                \"chosen_reward\": chosen_reward.item(),\n                \"rejected_reward\": rejected_reward.item(),\n            }\n            for chosen_reward, rejected_reward in zip(chosen_rewards, rejected_rewards)\n        ]\n        return chosen_is_better, outputs\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.sequence_classification.SequenceClassificationRewardModel.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = from_pretrained(tokenizer, **tokenizer_kwargs)\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.sequence_classification.SequenceClassificationRewardModel.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = from_pretrained(model, **model_kwargs)\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.sequence_classification.SequenceClassificationRewardModel.__init__","title":"__init__","text":"<pre><code>__init__(\n    model: str,\n    model_kwargs: dict[str, Any] | None = None,\n    tokenizer: str | None = None,\n    tokenizer_kwargs: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/reward_model/sequence_classification.py</code> <pre><code>def __init__(\n    self,\n    model: str,\n    model_kwargs: dict[str, Any] | None = None,\n    tokenizer: str | None = None,\n    tokenizer_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    tokenizer = tokenizer if tokenizer else model\n    tokenizer_kwargs = tokenizer_kwargs or {}\n    self.tokenizer = AutoTokenizer.from_pretrained(tokenizer, **tokenizer_kwargs)\n\n    model_kwargs = get_default_model_kwargs(model_kwargs)\n    self.model = AutoModelForSequenceClassification.from_pretrained(model, **model_kwargs)\n    # Set pad_token_id if not set\n    # to avoid \"ValueError: Cannot handle batch sizes &gt; 1 if no padding token is defined.\" in self.model()\n    if self.model.config.pad_token_id is None:\n        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n    self.model.eval()\n</code></pre>"},{"location":"api_reference/RewardModel/#flexeval.core.reward_model.sequence_classification.SequenceClassificationRewardModel.batch_judge","title":"batch_judge","text":"<pre><code>batch_judge(\n    batch_reward_bench_instances: list[RewardBenchInstance],\n) -&gt; tuple[list[bool], list[dict[str, Any]]]\n</code></pre> Source code in <code>flexeval/core/reward_model/sequence_classification.py</code> <pre><code>@torch.inference_mode()\ndef batch_judge(\n    self,\n    batch_reward_bench_instances: list[RewardBenchInstance],\n) -&gt; tuple[list[bool], list[dict[str, Any]]]:\n    chosen_messages = [instance.prompt + instance.chosen for instance in batch_reward_bench_instances]\n    chosen_inputs = self.tokenizer.apply_chat_template(\n        chosen_messages, return_tensors=\"pt\", padding=True, return_dict=True\n    )\n    chosen_outputs = self.model(**{k: v.to(self.model.device) for k, v in chosen_inputs.items()})\n    chosen_rewards = chosen_outputs.logits[:, 0]\n\n    rejected_messages = [instance.prompt + instance.rejected for instance in batch_reward_bench_instances]\n    rejected_inputs = self.tokenizer.apply_chat_template(\n        rejected_messages, return_tensors=\"pt\", padding=True, return_dict=True\n    )\n    rejected_outputs = self.model(**{k: v.to(self.model.device) for k, v in rejected_inputs.items()})\n    rejected_rewards = rejected_outputs.logits[:, 0]\n\n    chosen_is_better = (chosen_rewards &gt; rejected_rewards).tolist()\n    outputs = [\n        {\n            \"chosen_reward\": chosen_reward.item(),\n            \"rejected_reward\": rejected_reward.item(),\n        }\n        for chosen_reward, rejected_reward in zip(chosen_rewards, rejected_rewards)\n    ]\n    return chosen_is_better, outputs\n</code></pre>"},{"location":"api_reference/StringProcessor/","title":"StringProcessor","text":""},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.base.StringProcessor","title":"StringProcessor","text":"<p>An interface class used to process the model's output before evaluation. Typically used in <code>Metric</code>.</p> Source code in <code>flexeval/core/string_processor/base.py</code> <pre><code>class StringProcessor(ABC):\n    \"\"\"An interface class used to process the model's output before evaluation.\n    Typically used in `Metric`.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, text: str) -&gt; str:\n        \"\"\"\n        Process the input text.\n\n        Args:\n            text: The text to process.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.base.StringProcessor.__call__","title":"__call__  <code>abstractmethod</code>","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> <p>Process the input text.</p> <p>Parameters:</p> <ul> <li> <code>text</code>               (<code>str</code>)           \u2013            <p>The text to process.</p> </li> </ul> Source code in <code>flexeval/core/string_processor/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, text: str) -&gt; str:\n    \"\"\"\n    Process the input text.\n\n    Args:\n        text: The text to process.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.aio.AIONormalizer","title":"AIONormalizer","text":"<p>StringProcessor used for AI\u738b (AI king) question answering task. This is adapted from the official script.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import AIONormalizer\n&gt;&gt;&gt; processor = AIONormalizer()\n&gt;&gt;&gt; text = \"\u300c\u86f9\u5316(\u3088\u3046\u304b)\u300d\"\n&gt;&gt;&gt; normalized_text = processor(text)\n&gt;&gt;&gt; print(normalized_text)\n\u86f9\u5316\n</code></pre> Source code in <code>flexeval/core/string_processor/aio.py</code> <pre><code>class AIONormalizer(StringProcessor):\n    \"\"\"StringProcessor used for AI\u738b (AI king) question answering task.\n    This is adapted from\n    [the official script](https://github.com/cl-tohoku/aio4-bpr-baseline/blob/c5a226296b5e1c403268016dc7136147bbb515fe/compute_score.py).\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import AIONormalizer\n        &gt;&gt;&gt; processor = AIONormalizer()\n        &gt;&gt;&gt; text = \"\u300c\u86f9\u5316(\u3088\u3046\u304b)\u300d\"\n        &gt;&gt;&gt; normalized_text = processor(text)\n        &gt;&gt;&gt; print(normalized_text)\n        \u86f9\u5316\n    \"\"\"\n\n    def __call__(self, text: str) -&gt; str:\n        # substitute some symbols that will not be replaced by unicode normalization\n        text = text.replace(\"\uff5e\", \"\u301c\")\n\n        # unicode normalization\n        text = unicodedata.normalize(\"NFKC\", text)\n\n        # lowercase alphabetical characters\n        text = text.lower()\n\n        # remove kagi-kakkos\n        text = re.sub(r\"\u300c(.*?)\u300d\", r\"\\1\", text)\n        text = re.sub(r\"\u300e(.*?)\u300f\", r\"\\1\", text)\n\n        # remove some punctuation marks\n        text = text.replace(\"\u30fb\", \"\")\n        text = text.replace(\"=\", \"\")\n        text = text.replace(\"-\", \"\")\n\n        # compress whitespaces\n        text = re.sub(r\"\\s+\", \"\", text).strip()\n\n        # remove parenthesis: \u86f9\u5316(\u3088\u3046\u304b)\u3000\u2192\u3000\u86f9\u5316\n        return re.sub(r\"\\((.*?)\\)\", \"\", text)\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.aio.AIONormalizer.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/string_processor/aio.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    # substitute some symbols that will not be replaced by unicode normalization\n    text = text.replace(\"\uff5e\", \"\u301c\")\n\n    # unicode normalization\n    text = unicodedata.normalize(\"NFKC\", text)\n\n    # lowercase alphabetical characters\n    text = text.lower()\n\n    # remove kagi-kakkos\n    text = re.sub(r\"\u300c(.*?)\u300d\", r\"\\1\", text)\n    text = re.sub(r\"\u300e(.*?)\u300f\", r\"\\1\", text)\n\n    # remove some punctuation marks\n    text = text.replace(\"\u30fb\", \"\")\n    text = text.replace(\"=\", \"\")\n    text = text.replace(\"-\", \"\")\n\n    # compress whitespaces\n    text = re.sub(r\"\\s+\", \"\", text).strip()\n\n    # remove parenthesis: \u86f9\u5316(\u3088\u3046\u304b)\u3000\u2192\u3000\u86f9\u5316\n    return re.sub(r\"\\((.*?)\\)\", \"\", text)\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.last_line.LastLineExtractor","title":"LastLineExtractor","text":"<p>Extract the last line from a string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import LastLineExtractor\n&gt;&gt;&gt; processor = LastLineExtractor()\n&gt;&gt;&gt; text = \"Answer\\nFUJI-YAMA\"\n&gt;&gt;&gt; print(processor(text))\nFUJI-YAMA\n</code></pre> Source code in <code>flexeval/core/string_processor/last_line.py</code> <pre><code>class LastLineExtractor(StringProcessor):\n    \"\"\"Extract the last line from a string.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import LastLineExtractor\n        &gt;&gt;&gt; processor = LastLineExtractor()\n        &gt;&gt;&gt; text = \"Answer\\\\nFUJI-YAMA\"\n        &gt;&gt;&gt; print(processor(text))\n        FUJI-YAMA\n    \"\"\"\n\n    def __call__(self, text: str) -&gt; str:\n        return text.split(\"\\n\")[-1]\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.last_line.LastLineExtractor.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/string_processor/last_line.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    return text.split(\"\\n\")[-1]\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.lower.StringLower","title":"StringLower","text":"<p>This processor returns a lowercased string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import StringLower\n&gt;&gt;&gt; processor = StringLower()\n&gt;&gt;&gt; text = \"ABCDefg\"\n&gt;&gt;&gt; normalized_text = processor(text)\n&gt;&gt;&gt; print(normalized_text)\nabcdefg\n</code></pre> Source code in <code>flexeval/core/string_processor/lower.py</code> <pre><code>class StringLower(StringProcessor):\n    \"\"\"This processor returns a lowercased string.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import StringLower\n        &gt;&gt;&gt; processor = StringLower()\n        &gt;&gt;&gt; text = \"ABCDefg\"\n        &gt;&gt;&gt; normalized_text = processor(text)\n        &gt;&gt;&gt; print(normalized_text)\n        abcdefg\n    \"\"\"\n\n    def __call__(self, text: str) -&gt; str:\n        return text.lower()\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.lower.StringLower.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/string_processor/lower.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    return text.lower()\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.mgsm.RemoveCommaProcessor","title":"RemoveCommaProcessor","text":"<p>StringProcessor that remove comma following https://github.com/openai/simple-evals/blob/main/mgsm_eval.py.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval.core.string_processor import RemoveCommaProcessor\n&gt;&gt;&gt; processor = RemoveCommaProcessor()\n&gt;&gt;&gt; text = \"3,000\"\n&gt;&gt;&gt; print(processor(text))\n3000\n</code></pre> Source code in <code>flexeval/core/string_processor/mgsm.py</code> <pre><code>class RemoveCommaProcessor(StringProcessor):\n    \"\"\"\n    StringProcessor that remove comma following https://github.com/openai/simple-evals/blob/main/mgsm_eval.py.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval.core.string_processor import RemoveCommaProcessor\n        &gt;&gt;&gt; processor = RemoveCommaProcessor()\n        &gt;&gt;&gt; text = \"3,000\"\n        &gt;&gt;&gt; print(processor(text))\n        3000\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        pass\n\n    def __call__(self, text: str) -&gt; str:\n        return text.replace(\",\", \"\")\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.mgsm.RemoveCommaProcessor.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/string_processor/mgsm.py</code> <pre><code>def __init__(self) -&gt; None:\n    pass\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.mgsm.RemoveCommaProcessor.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/string_processor/mgsm.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    return text.replace(\",\", \"\")\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.mgsm.SimpleEvalMGSMProcessor","title":"SimpleEvalMGSMProcessor","text":"<p>StringProcessor that extracts and normalize numerical expression following https://github.com/openai/simple-evals/blob/main/mgsm_eval.py. This processor is slightly different from that of simple-evals' in that it includes an option that does not require the response to start with a specified string. This is because when model is not trained on instruction following dataset, it should be hard to follow instruction to starts with specified string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval.core.string_processor import SimpleEvalMGSMProcessor\n&gt;&gt;&gt; processor = SimpleEvalMGSMProcessor()\n&gt;&gt;&gt; text = \"Step 1: 30.0 + 20.0 = 50.0\\nStep 2: 50.0 \u00d7 40.0 = 2,000.0\\nAnswer: 2,000.0\"\n&gt;&gt;&gt; print(processor(text))\n2000\n</code></pre> Source code in <code>flexeval/core/string_processor/mgsm.py</code> <pre><code>class SimpleEvalMGSMProcessor(StringProcessor):\n    \"\"\"\n    StringProcessor that extracts and normalize numerical expression following\n    https://github.com/openai/simple-evals/blob/main/mgsm_eval.py.\n    This processor is slightly different from that of simple-evals' in that it includes\n    an option that does not require the response to start with a specified string.\n    This is because when model is not trained on instruction following dataset, it should\n    be hard to follow instruction to starts with specified string.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval.core.string_processor import SimpleEvalMGSMProcessor\n        &gt;&gt;&gt; processor = SimpleEvalMGSMProcessor()\n        &gt;&gt;&gt; text = \"Step 1: 30.0 + 20.0 = 50.0\\\\nStep 2: 50.0 \u00d7 40.0 = 2,000.0\\\\nAnswer: 2,000.0\"\n        &gt;&gt;&gt; print(processor(text))\n        2000\n    \"\"\"\n\n    def __init__(self, answer_prefix: str | None = None) -&gt; None:\n        self.answer_prefix = answer_prefix\n\n    def __call__(self, text: str) -&gt; str:\n        if self.answer_prefix is not None:\n            text = text.split(self.answer_prefix)[-1].strip()\n\n        # find all the numbers (including decimals) in the string\n        numbers = re.findall(r\"-?\\d+\\.?\\d*\", text.replace(\",\", \"\"))\n\n        prediction = numbers[-1].rstrip(\".\") if numbers else \"\"\n\n        if \".\" in prediction:\n            prediction = prediction.rstrip(\"0\").rstrip(\".\")\n        return prediction.replace(\",\", \"\")\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.mgsm.SimpleEvalMGSMProcessor.answer_prefix","title":"answer_prefix  <code>instance-attribute</code>","text":"<pre><code>answer_prefix = answer_prefix\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.mgsm.SimpleEvalMGSMProcessor.__init__","title":"__init__","text":"<pre><code>__init__(answer_prefix: str | None = None) -&gt; None\n</code></pre> Source code in <code>flexeval/core/string_processor/mgsm.py</code> <pre><code>def __init__(self, answer_prefix: str | None = None) -&gt; None:\n    self.answer_prefix = answer_prefix\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.mgsm.SimpleEvalMGSMProcessor.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/string_processor/mgsm.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    if self.answer_prefix is not None:\n        text = text.split(self.answer_prefix)[-1].strip()\n\n    # find all the numbers (including decimals) in the string\n    numbers = re.findall(r\"-?\\d+\\.?\\d*\", text.replace(\",\", \"\"))\n\n    prediction = numbers[-1].rstrip(\".\") if numbers else \"\"\n\n    if \".\" in prediction:\n        prediction = prediction.rstrip(\"0\").rstrip(\".\")\n    return prediction.replace(\",\", \"\")\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.nfkc.NFKCNormalizer","title":"NFKCNormalizer","text":"<p>This processor returns a NFKC normalized string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import NFKCNormalizer\n&gt;&gt;&gt; processor = NFKCNormalizer()\n&gt;&gt;&gt; text = \"\uff10\uff11\uff12\uff13\uff21\uff22\uff23\"\n&gt;&gt;&gt; normalized_text = processor(text)\n&gt;&gt;&gt; print(normalized_text)\n0123ABC\n</code></pre> Source code in <code>flexeval/core/string_processor/nfkc.py</code> <pre><code>class NFKCNormalizer(StringProcessor):\n    \"\"\"This processor returns a NFKC normalized string.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import NFKCNormalizer\n        &gt;&gt;&gt; processor = NFKCNormalizer()\n        &gt;&gt;&gt; text = \"\uff10\uff11\uff12\uff13\uff21\uff22\uff23\"\n        &gt;&gt;&gt; normalized_text = processor(text)\n        &gt;&gt;&gt; print(normalized_text)\n        0123ABC\n    \"\"\"\n\n    def __call__(self, text: str) -&gt; str:\n        return unicodedata.normalize(\"NFKC\", text)\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.nfkc.NFKCNormalizer.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/string_processor/nfkc.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    return unicodedata.normalize(\"NFKC\", text)\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.regex.RegexExtractor","title":"RegexExtractor","text":"<p>StringProcessor that extracts the last match of a regex pattern. Useful to extract an answer after a step-by-step derivation.</p> <p>Parameters:</p> <ul> <li> <code>pattern</code>               (<code>str</code>)           \u2013            <p>The regex pattern to extract.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import RegexExtractor\n&gt;&gt;&gt; processor = RegexExtractor(r\"Answer: (.*)\")\n&gt;&gt;&gt; text = \"Step 1: 3 + 2 = 5\\nStep 2: 5 \u00d7 4 = 20\\nAnswer: 20\"\n&gt;&gt;&gt; print(processor(text))\n20\n</code></pre> Source code in <code>flexeval/core/string_processor/regex.py</code> <pre><code>class RegexExtractor(StringProcessor):\n    \"\"\"\n    StringProcessor that extracts the last match of a regex pattern.\n    Useful to extract an answer after a step-by-step derivation.\n\n    Args:\n        pattern: The regex pattern to extract.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import RegexExtractor\n        &gt;&gt;&gt; processor = RegexExtractor(r\"Answer: (.*)\")\n        &gt;&gt;&gt; text = \"Step 1: 3 + 2 = 5\\\\nStep 2: 5 \u00d7 4 = 20\\\\nAnswer: 20\"\n        &gt;&gt;&gt; print(processor(text))\n        20\n    \"\"\"\n\n    def __init__(self, pattern: str) -&gt; None:\n        self._pattern = re.compile(pattern, flags=re.DOTALL)\n\n    def __call__(self, text: str) -&gt; str:\n        found = self._pattern.findall(text)\n        if not found:\n            return \"\"\n        return found[-1]\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.regex.RegexExtractor.__init__","title":"__init__","text":"<pre><code>__init__(pattern: str) -&gt; None\n</code></pre> Source code in <code>flexeval/core/string_processor/regex.py</code> <pre><code>def __init__(self, pattern: str) -&gt; None:\n    self._pattern = re.compile(pattern, flags=re.DOTALL)\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.regex.RegexExtractor.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/string_processor/regex.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    found = self._pattern.findall(text)\n    if not found:\n        return \"\"\n    return found[-1]\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.string_strip.StringStrip","title":"StringStrip","text":"<p>Strip leading and trailing whitespaces from a string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import StringStrip\n&gt;&gt;&gt; processor = StringStrip()\n&gt;&gt;&gt; text = \" ABC\"\n&gt;&gt;&gt; normalized_text = processor(text)\n&gt;&gt;&gt; print(normalized_text)\nABC\n</code></pre> Source code in <code>flexeval/core/string_processor/string_strip.py</code> <pre><code>class StringStrip(StringProcessor):\n    \"\"\"Strip leading and trailing whitespaces from a string.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import StringStrip\n        &gt;&gt;&gt; processor = StringStrip()\n        &gt;&gt;&gt; text = \" ABC\"\n        &gt;&gt;&gt; normalized_text = processor(text)\n        &gt;&gt;&gt; print(normalized_text)\n        ABC\n    \"\"\"\n\n    def __call__(self, text: str) -&gt; str:\n        return text.strip()\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.string_strip.StringStrip.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/string_processor/string_strip.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    return text.strip()\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.template.TemplateRenderer","title":"TemplateRenderer","text":"<p>Render a jinja2 template with a given string</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import TemplateRenderer\n&gt;&gt;&gt; processor = TemplateRenderer(\"This is a {{text}}\")\n&gt;&gt;&gt; text = \"ABC\"\n&gt;&gt;&gt; normalized_text = processor(text)\n&gt;&gt;&gt; print(normalized_text)\nThis is a ABC\n</code></pre> Source code in <code>flexeval/core/string_processor/template.py</code> <pre><code>class TemplateRenderer(StringProcessor):\n    \"\"\"Render a jinja2 template with a given string\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import TemplateRenderer\n        &gt;&gt;&gt; processor = TemplateRenderer(\"This is a {{text}}\")\n        &gt;&gt;&gt; text = \"ABC\"\n        &gt;&gt;&gt; normalized_text = processor(text)\n        &gt;&gt;&gt; print(normalized_text)\n        This is a ABC\n    \"\"\"\n\n    def __init__(self, template: str) -&gt; None:\n        self._template = JINJA2_ENV.from_string(template)\n\n    def __call__(self, text: str) -&gt; str:\n        return self._template.render(text=text)\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.template.TemplateRenderer.__init__","title":"__init__","text":"<pre><code>__init__(template: str) -&gt; None\n</code></pre> Source code in <code>flexeval/core/string_processor/template.py</code> <pre><code>def __init__(self, template: str) -&gt; None:\n    self._template = JINJA2_ENV.from_string(template)\n</code></pre>"},{"location":"api_reference/StringProcessor/#flexeval.core.string_processor.template.TemplateRenderer.__call__","title":"__call__","text":"<pre><code>__call__(text: str) -&gt; str\n</code></pre> Source code in <code>flexeval/core/string_processor/template.py</code> <pre><code>def __call__(self, text: str) -&gt; str:\n    return self._template.render(text=text)\n</code></pre>"},{"location":"api_reference/TextDataset/","title":"TextDataset","text":""},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.base.TextDataset","title":"TextDataset","text":"<p>This class represents a dataset of text examples.</p> Source code in <code>flexeval/core/text_dataset/base.py</code> <pre><code>class TextDataset(Sequence[TextInstance], ABC):\n    \"\"\"\n    This class represents a dataset of text examples.\n    \"\"\"\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        pass\n\n    @abstractmethod\n    def __getitem__(self, item: int) -&gt; TextInstance:\n        pass\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.base.TextDataset.__len__","title":"__len__  <code>abstractmethod</code>","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/text_dataset/base.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    pass\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.base.TextDataset.__getitem__","title":"__getitem__  <code>abstractmethod</code>","text":"<pre><code>__getitem__(item: int) -&gt; TextInstance\n</code></pre> Source code in <code>flexeval/core/text_dataset/base.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, item: int) -&gt; TextInstance:\n    pass\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.base.TextDataset.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> Source code in <code>flexeval/core/text_dataset/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    return f\"{self.__class__.__name__}(num_instances={len(self)})\"\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.base.TextInstance","title":"TextInstance  <code>dataclass</code>","text":"Source code in <code>flexeval/core/text_dataset/base.py</code> <pre><code>@dataclass\nclass TextInstance:\n    text: str\n    prefix: str = \"\"\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.base.TextInstance.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.base.TextInstance.prefix","title":"prefix  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prefix: str = ''\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.base.TextInstance.__init__","title":"__init__","text":"<pre><code>__init__(text: str, prefix: str = '') -&gt; None\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.hf.HFTextDataset","title":"HFTextDataset","text":"<p>This class represents a dataset of text examples loaded from Hugging Face datasets.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str</code>)           \u2013            <p>The name of the dataset to load.</p> </li> <li> <code>split</code>               (<code>str</code>)           \u2013            <p>The split of the dataset to load.</p> </li> <li> <code>text_template</code>               (<code>str</code>)           \u2013            <p>A Jinja2 template for the text.</p> </li> <li> <code>subset</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>The subset of the dataset to load.</p> </li> <li> <code>keep_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to filter certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.</p> </li> <li> <code>remove_conditions</code>               (<code>dict[str, str] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary to indicate the condition to remove certain items. The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.</p> </li> <li> <code>dataset_kwargs</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Additional keyword arguments for <code>datasets.load_dataset</code>.</p> </li> </ul> Source code in <code>flexeval/core/text_dataset/hf.py</code> <pre><code>class HFTextDataset(TextDataset):\n    \"\"\"\n    This class represents a dataset of text examples loaded from Hugging Face datasets.\n\n    Args:\n        path: The name of the dataset to load.\n        split: The split of the dataset to load.\n        text_template: A Jinja2 template for the text.\n        subset: The subset of the dataset to load.\n        keep_conditions: A dictionary to indicate the condition to filter certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to keep.\n        remove_conditions: A dictionary to indicate the condition to remove certain items.\n            The key is a Jinja2 template string to embed the item into a string, and the value is the value to remove.\n        dataset_kwargs: Additional keyword arguments for `datasets.load_dataset`.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        split: str,\n        text_template: str,\n        prefix_template: str | None = None,\n        subset: str | None = None,\n        keep_conditions: dict[str, str] | None = None,\n        remove_conditions: dict[str, str] | None = None,\n        dataset_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        dataset_kwargs = dataset_kwargs or {}\n        self.dataset = datasets.load_dataset(path, split=split, name=subset, **dataset_kwargs)\n\n        keep_conditions = keep_conditions or {}\n        for template_str, value_to_keep in keep_conditions.items():\n            filter_template = JINJA2_ENV.from_string(template_str)\n            self.dataset = self.dataset.filter(lambda x, t=filter_template, v=value_to_keep: t.render(**x) == v)\n        remove_conditions = remove_conditions or {}\n        for template_str, value_to_remove in remove_conditions.items():\n            filter_template = JINJA2_ENV.from_string(template_str)\n            self.dataset = self.dataset.filter(lambda x, t=filter_template, v=value_to_remove: t.render(**x) != v)\n\n        self.text_template = JINJA2_ENV.from_string(text_template)\n        self.prefix_template = None\n        if prefix_template:\n            self.prefix_template = JINJA2_ENV.from_string(prefix_template)\n\n    def __len__(self) -&gt; int:\n        return len(self.dataset)\n\n    def __getitem__(self, i: int) -&gt; TextInstance:\n        item = self.dataset[i]\n        text = self.text_template.render(**item)\n        prefix = \"\"\n        if self.prefix_template:\n            prefix = self.prefix_template.render(**item)\n        return TextInstance(text=text, prefix=prefix)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.hf.HFTextDataset.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset = filter(\n    lambda x, t=filter_template, v=value_to_remove: render(\n        **x\n    )\n    != v\n)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.hf.HFTextDataset.text_template","title":"text_template  <code>instance-attribute</code>","text":"<pre><code>text_template = from_string(text_template)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.hf.HFTextDataset.prefix_template","title":"prefix_template  <code>instance-attribute</code>","text":"<pre><code>prefix_template = None\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.hf.HFTextDataset.__init__","title":"__init__","text":"<pre><code>__init__(\n    path: str,\n    split: str,\n    text_template: str,\n    prefix_template: str | None = None,\n    subset: str | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/text_dataset/hf.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    split: str,\n    text_template: str,\n    prefix_template: str | None = None,\n    subset: str | None = None,\n    keep_conditions: dict[str, str] | None = None,\n    remove_conditions: dict[str, str] | None = None,\n    dataset_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    dataset_kwargs = dataset_kwargs or {}\n    self.dataset = datasets.load_dataset(path, split=split, name=subset, **dataset_kwargs)\n\n    keep_conditions = keep_conditions or {}\n    for template_str, value_to_keep in keep_conditions.items():\n        filter_template = JINJA2_ENV.from_string(template_str)\n        self.dataset = self.dataset.filter(lambda x, t=filter_template, v=value_to_keep: t.render(**x) == v)\n    remove_conditions = remove_conditions or {}\n    for template_str, value_to_remove in remove_conditions.items():\n        filter_template = JINJA2_ENV.from_string(template_str)\n        self.dataset = self.dataset.filter(lambda x, t=filter_template, v=value_to_remove: t.render(**x) != v)\n\n    self.text_template = JINJA2_ENV.from_string(text_template)\n    self.prefix_template = None\n    if prefix_template:\n        self.prefix_template = JINJA2_ENV.from_string(prefix_template)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.hf.HFTextDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/text_dataset/hf.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self.dataset)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.hf.HFTextDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(i: int) -&gt; TextInstance\n</code></pre> Source code in <code>flexeval/core/text_dataset/hf.py</code> <pre><code>def __getitem__(self, i: int) -&gt; TextInstance:\n    item = self.dataset[i]\n    text = self.text_template.render(**item)\n    prefix = \"\"\n    if self.prefix_template:\n        prefix = self.prefix_template.render(**item)\n    return TextInstance(text=text, prefix=prefix)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.jsonl.JsonlTextDataset","title":"JsonlTextDataset","text":"<p>This class represents a dataset of text examples loaded from a JSONL file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>str | PathLike[str]</code>)           \u2013            <p>The path to the JSONL file.</p> </li> <li> <code>field</code>               (<code>str</code>)           \u2013            <p>The field to extract from the JSONL file.</p> </li> </ul> Source code in <code>flexeval/core/text_dataset/jsonl.py</code> <pre><code>class JsonlTextDataset(TextDataset):\n    \"\"\"\n    This class represents a dataset of text examples loaded from a JSONL file.\n\n    Args:\n        path: The path to the JSONL file.\n        field: The field to extract from the JSONL file.\n    \"\"\"\n\n    def __init__(self, path: str | PathLike[str], field: str) -&gt; None:\n        self._text_list: list[str] = []\n        with open(path) as f:\n            for line in f:\n                item = json.loads(line)\n                self._text_list.append(item[field])\n\n    def __len__(self) -&gt; int:\n        return len(self._text_list)\n\n    def __getitem__(self, item: int) -&gt; TextInstance:\n        return TextInstance(self._text_list[item])\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.jsonl.JsonlTextDataset.__init__","title":"__init__","text":"<pre><code>__init__(path: str | PathLike[str], field: str) -&gt; None\n</code></pre> Source code in <code>flexeval/core/text_dataset/jsonl.py</code> <pre><code>def __init__(self, path: str | PathLike[str], field: str) -&gt; None:\n    self._text_list: list[str] = []\n    with open(path) as f:\n        for line in f:\n            item = json.loads(line)\n            self._text_list.append(item[field])\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.jsonl.JsonlTextDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> Source code in <code>flexeval/core/text_dataset/jsonl.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._text_list)\n</code></pre>"},{"location":"api_reference/TextDataset/#flexeval.core.text_dataset.jsonl.JsonlTextDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(item: int) -&gt; TextInstance\n</code></pre> Source code in <code>flexeval/core/text_dataset/jsonl.py</code> <pre><code>def __getitem__(self, item: int) -&gt; TextInstance:\n    return TextInstance(self._text_list[item])\n</code></pre>"},{"location":"api_reference/Tokenizer/","title":"Tokenizer","text":""},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.base.Tokenizer","title":"Tokenizer","text":"<p>Tokenizer interface.</p> <p>Tokenizers are used to split text into tokens. Typically, this is used in <code>Metric</code> that requires word-level statistics.</p> Source code in <code>flexeval/core/tokenizer/base.py</code> <pre><code>class Tokenizer(ABC):\n    \"\"\"\n    Tokenizer interface.\n\n    Tokenizers are used to split text into tokens.\n    Typically, this is used in `Metric` that requires word-level statistics.\n    \"\"\"\n\n    @abstractmethod\n    def tokenize(self, text: str) -&gt; list[str]:\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.base.Tokenizer.tokenize","title":"tokenize  <code>abstractmethod</code>","text":"<pre><code>tokenize(text: str) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/tokenizer/base.py</code> <pre><code>@abstractmethod\ndef tokenize(self, text: str) -&gt; list[str]:\n    raise NotImplementedError\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.mecab.MecabTokenizer","title":"MecabTokenizer","text":"<p>MeCab tokenizer for Japanese text.</p> Source code in <code>flexeval/core/tokenizer/mecab.py</code> <pre><code>class MecabTokenizer(Tokenizer):\n    \"\"\"\n    MeCab tokenizer for Japanese text.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        import fugashi\n\n        self._tagger = fugashi.Tagger(\"-Owakati\")\n\n    def tokenize(self, text: str) -&gt; list[str]:\n        tokens = self._tagger(text)\n        return [token.surface for token in tokens]\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.mecab.MecabTokenizer.__init__","title":"__init__","text":"<pre><code>__init__() -&gt; None\n</code></pre> Source code in <code>flexeval/core/tokenizer/mecab.py</code> <pre><code>def __init__(self) -&gt; None:\n    import fugashi\n\n    self._tagger = fugashi.Tagger(\"-Owakati\")\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.mecab.MecabTokenizer.tokenize","title":"tokenize","text":"<pre><code>tokenize(text: str) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/tokenizer/mecab.py</code> <pre><code>def tokenize(self, text: str) -&gt; list[str]:\n    tokens = self._tagger(text)\n    return [token.surface for token in tokens]\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.sacrebleu_tokenizer.SacreBleuTokenizer","title":"SacreBleuTokenizer","text":"<p>A tokenizer imported from uses the sacrebleu library.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>The name of the tokenizer.</p> </li> </ul> Source code in <code>flexeval/core/tokenizer/sacrebleu_tokenizer.py</code> <pre><code>class SacreBleuTokenizer(Tokenizer):\n    \"\"\"\n    A tokenizer imported from uses the sacrebleu library.\n\n    Args:\n        name: The name of the tokenizer.\n    \"\"\"\n\n    def __init__(self, name: str) -&gt; None:\n        self.tokenizer = _get_tokenizer(name)()\n\n    def tokenize(self, text: str) -&gt; list[str]:\n        return self.tokenizer(text).split(\" \")\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.sacrebleu_tokenizer.SacreBleuTokenizer.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = _get_tokenizer(name)()\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.sacrebleu_tokenizer.SacreBleuTokenizer.__init__","title":"__init__","text":"<pre><code>__init__(name: str) -&gt; None\n</code></pre> Source code in <code>flexeval/core/tokenizer/sacrebleu_tokenizer.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    self.tokenizer = _get_tokenizer(name)()\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.sacrebleu_tokenizer.SacreBleuTokenizer.tokenize","title":"tokenize","text":"<pre><code>tokenize(text: str) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/tokenizer/sacrebleu_tokenizer.py</code> <pre><code>def tokenize(self, text: str) -&gt; list[str]:\n    return self.tokenizer(text).split(\" \")\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.tiktoken_tokenizer.TiktokenTokenizer","title":"TiktokenTokenizer","text":"Source code in <code>flexeval/core/tokenizer/tiktoken_tokenizer.py</code> <pre><code>class TiktokenTokenizer(Tokenizer):\n    def __init__(self, tokenizer_name: str | None = None, model_name: str | None = None) -&gt; None:\n        # raise error, if both tokenizer_name and model_name are provided\n        if tokenizer_name is not None and model_name is not None:\n            msg = \"Only one of tokenizer_name or model_name must be provided.\"\n            raise ValueError(msg)\n\n        if tokenizer_name:\n            self.encoding = tiktoken.get_encoding(tokenizer_name)\n        elif model_name:\n            self.encoding = tiktoken.encoding_for_model(model_name)\n        else:\n            msg = \"Either tokenizer_name or model_name must be provided\"\n            raise ValueError(msg)\n\n    def tokenize(self, text: str) -&gt; list[str]:\n        token_ids = self.encoding.encode(text)\n        return [self.encoding.decode([token_id]) for token_id in token_ids]\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.tiktoken_tokenizer.TiktokenTokenizer.encoding","title":"encoding  <code>instance-attribute</code>","text":"<pre><code>encoding = get_encoding(tokenizer_name)\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.tiktoken_tokenizer.TiktokenTokenizer.__init__","title":"__init__","text":"<pre><code>__init__(\n    tokenizer_name: str | None = None,\n    model_name: str | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/tokenizer/tiktoken_tokenizer.py</code> <pre><code>def __init__(self, tokenizer_name: str | None = None, model_name: str | None = None) -&gt; None:\n    # raise error, if both tokenizer_name and model_name are provided\n    if tokenizer_name is not None and model_name is not None:\n        msg = \"Only one of tokenizer_name or model_name must be provided.\"\n        raise ValueError(msg)\n\n    if tokenizer_name:\n        self.encoding = tiktoken.get_encoding(tokenizer_name)\n    elif model_name:\n        self.encoding = tiktoken.encoding_for_model(model_name)\n    else:\n        msg = \"Either tokenizer_name or model_name must be provided\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.tiktoken_tokenizer.TiktokenTokenizer.tokenize","title":"tokenize","text":"<pre><code>tokenize(text: str) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/tokenizer/tiktoken_tokenizer.py</code> <pre><code>def tokenize(self, text: str) -&gt; list[str]:\n    token_ids = self.encoding.encode(text)\n    return [self.encoding.decode([token_id]) for token_id in token_ids]\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.transformers_tokenizer.TransformersTokenizer","title":"TransformersTokenizer","text":"Source code in <code>flexeval/core/tokenizer/transformers_tokenizer.py</code> <pre><code>class TransformersTokenizer(Tokenizer):\n    def __init__(\n        self,\n        path: str,\n        init_kwargs: dict[str, Any] | None = None,\n        tokenize_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        init_kwargs = init_kwargs or {}\n        self.tokenizer = AutoTokenizer.from_pretrained(path, **init_kwargs)\n        self.tokenize_kwargs = tokenize_kwargs or {}\n\n    def tokenize(self, text: str) -&gt; list[str]:\n        return self.tokenizer.tokenize(text, **self.tokenize_kwargs)\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.transformers_tokenizer.TransformersTokenizer.tokenizer","title":"tokenizer  <code>instance-attribute</code>","text":"<pre><code>tokenizer = from_pretrained(path, **init_kwargs)\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.transformers_tokenizer.TransformersTokenizer.tokenize_kwargs","title":"tokenize_kwargs  <code>instance-attribute</code>","text":"<pre><code>tokenize_kwargs = tokenize_kwargs or {}\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.transformers_tokenizer.TransformersTokenizer.__init__","title":"__init__","text":"<pre><code>__init__(\n    path: str,\n    init_kwargs: dict[str, Any] | None = None,\n    tokenize_kwargs: dict[str, Any] | None = None,\n) -&gt; None\n</code></pre> Source code in <code>flexeval/core/tokenizer/transformers_tokenizer.py</code> <pre><code>def __init__(\n    self,\n    path: str,\n    init_kwargs: dict[str, Any] | None = None,\n    tokenize_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    init_kwargs = init_kwargs or {}\n    self.tokenizer = AutoTokenizer.from_pretrained(path, **init_kwargs)\n    self.tokenize_kwargs = tokenize_kwargs or {}\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.transformers_tokenizer.TransformersTokenizer.tokenize","title":"tokenize","text":"<pre><code>tokenize(text: str) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/tokenizer/transformers_tokenizer.py</code> <pre><code>def tokenize(self, text: str) -&gt; list[str]:\n    return self.tokenizer.tokenize(text, **self.tokenize_kwargs)\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.whitespace.WhitespaceTokenizer","title":"WhitespaceTokenizer","text":"<p>A simple whitespace tokenizer.</p> Source code in <code>flexeval/core/tokenizer/whitespace.py</code> <pre><code>class WhitespaceTokenizer(Tokenizer):\n    \"\"\"\n    A simple whitespace tokenizer.\n    \"\"\"\n\n    def tokenize(self, text: str) -&gt; list[str]:\n        return text.split()\n</code></pre>"},{"location":"api_reference/Tokenizer/#flexeval.core.tokenizer.whitespace.WhitespaceTokenizer.tokenize","title":"tokenize","text":"<pre><code>tokenize(text: str) -&gt; list[str]\n</code></pre> Source code in <code>flexeval/core/tokenizer/whitespace.py</code> <pre><code>def tokenize(self, text: str) -&gt; list[str]:\n    return text.split()\n</code></pre>"},{"location":"api_reference/utils/","title":"Utils","text":""},{"location":"api_reference/utils/#flexeval.utils.module_utils.instantiate_from_config","title":"instantiate_from_config","text":"<pre><code>instantiate_from_config(\n    config_path: str,\n    overrides: dict[str, Any] | None = None,\n) -&gt; Module\n</code></pre> <p>Instantiates a module from a jsonnet config file.</p> <p>Parameters:</p> <ul> <li> <code>config_path</code>               (<code>str</code>)           \u2013            <p>The path to the jsonnet config file.</p> </li> <li> <code>overrides</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary of overrides to apply to the config.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>           \u2013            <p>The instantiated module.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from flexeval import instantiate_from_config\n&gt;&gt;&gt; eval_setup = instantiate_from_config(\"aio\")\n</code></pre> Source code in <code>flexeval/utils/module_utils.py</code> <pre><code>def instantiate_from_config(\n    config_path: str,\n    overrides: dict[str, Any] | None = None,\n) -&gt; Module:\n    \"\"\"\n    Instantiates a module from a jsonnet config file.\n\n    Args:\n        config_path: The path to the jsonnet config file.\n        overrides: A dictionary of overrides to apply to the config.\n\n    Returns:\n        The instantiated module.\n\n    Examples:\n        &gt;&gt;&gt; from flexeval import instantiate_from_config\n        &gt;&gt;&gt; eval_setup = instantiate_from_config(\"aio\")\n    \"\"\"\n    resolved_config_path = ConfigNameResolver()(config_path)\n\n    if resolved_config_path is None:\n        msg = f'Config name \"{config_path}\" not found in the specified path nor in the preset config directories.'\n        raise ValueError(msg)\n\n    config = json.loads(_jsonnet.evaluate_file(resolved_config_path))\n    module_class = getattr(flexeval, config[\"class_path\"])\n\n    parser = ArgumentParser(parser_mode=\"jsonnet\")\n    parser.add_argument(\"--module\", type=module_class, required=True, enable_path=True)\n\n    args_to_parse = [\"--module\", resolved_config_path]\n    overrides = overrides or {}\n    for key, value in overrides.items():\n        args_to_parse += [f\"--module.{key}\", str(value)]\n\n    args = parser.parse_args(args_to_parse)\n    instantiated_config = parser.instantiate_classes(args)\n    return instantiated_config.module\n</code></pre>"},{"location":"how_to/","title":"How to","text":"<ul> <li>Configure Few-Shot Examples</li> <li>Evaluate with LLM Judges</li> <li>Implement Your Own Module</li> </ul>"},{"location":"how_to/configure_few_shot_examples/","title":"Configure Few-Shot Examples","text":"<p>The logic to configure few-shot examples is implemented in the <code>FewShotGenerator</code> classes.</p>"},{"location":"how_to/configure_few_shot_examples/#change-the-number-of-shots","title":"Change the number of shots","text":""},{"location":"how_to/configure_few_shot_examples/#overriding-the-arguments","title":"Overriding the arguments","text":"<p>Most presets use <code>RandomFewShotGenerator</code> to generate few-shot examples. From command line, the number of shots can be changed using the <code>--eval_setup.few_shot_generator.num_shots</code> argument.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa\" \\\n  --eval_setup.few_shot_generator.num_shots 3\n</code></pre>"},{"location":"how_to/configure_few_shot_examples/#editing-the-configuration-file","title":"Editing the configuration file","text":"<p>First, save the configuration file to a local file.</p> <pre><code>flexeval_presets commonsense_qa &gt; commonsense_qa_custom.jsonnet\n</code></pre> <p>Then, edit the <code>num_shots</code> field in the <code>few_shot_generator</code> section.</p> <p>Finally, run the evaluation with the custom configuration file.</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa_custom.jsonnet\" \n</code></pre>"},{"location":"how_to/configure_few_shot_examples/#change-the-sampling-method","title":"Change the sampling method","text":"<p>Sometime, you may want to change the sampling method for few-shot examples. In that case, you can change the <code>FewShotGenerator</code> class.</p> <p>For example, see the config file from <code>flexeval_presets twitter_sentiment</code>. It uses <code>BalancedFewShotGenerator</code> to generate few-shot examples. This classes samples examples so that the number of labels (the first element of the <code>references</code> field) is balanced.</p> <p>See API Reference for other available classes.</p>"},{"location":"how_to/evaluate_with_llm_judges/","title":"Evaluate with LLM Judges","text":"<p>Evaluation of chat model is difficult since the response is open-ended and manually evaluating the responses is not scalable. One solution is to use a LLM as an evaluator.</p>"},{"location":"how_to/evaluate_with_llm_judges/#single-judge-evaluation","title":"Single Judge Evaluation","text":"<p>First, we need to generate responses from the chat model. In this example, we use ChatGPT with the following command:</p> <pre><code>export OPENAI_API_KEY=\"YOUR_API_KEY\"\n\nflexeval_lm \\\n  --language_model OpenAIChatAPI \\\n  --language_model.model \"gpt-4o-mini\" \\\n  --eval_setup \"mt-en\" \\\n  --save_dir \"results/mt-en_gpt3.5-turbo\"\n</code></pre> <p>Now you have the model outputs in <code>results/mt-en-gpt3.5-turbo/outputs.jsonl</code>.</p> <p>Let's evaluate the responses with GPT4. The LLM evaluation is implemented as a <code>Metric</code> class and we will use a preset metric named <code>assistant_eval_en_single_turn</code>. You can check its configuration with the following command:</p> <pre><code>flexeval_presets assistant_eval_en_single_turn\n</code></pre> <p>In this metric, GPT4 is asked to rate the responses with the score from 1 to 10. The score is extracted as the last digit found in the evaluator's output.</p> <p>Tip</p> <p>To take a closer look at the prompt template, combine pipeline with <code>jsonnet</code> and <code>jq</code>:</p> <pre><code>flexeval_presets assistant_eval_en_single_turn | jsonnet - | jq -r \".init_args.prompt_template.init_args.template\"\n</code></pre> <p>Perform automatic evaluation with GPT4 with the following command:</p> <pre><code>flexeval_file \\\n   --eval_file \"results/mt-en-gpt3.5-turbo/outputs.jsonl\" \\\n   --metrics \"assistant_eval_en_single_turn\" \\\n   --save_dir \"results/mt-en_gpt3.5-turbo/eval_by_gpt\"\n</code></pre> <p>\u2615\ufe0f It may take a while to finish the evaluation...</p> <p>By hitting <code>cat results/mt-en-gpt3.5-turbo/eval_by_gpt/metrics.json</code>, you can see the evaluation result like <code>{\"llm_score\": 7.795}</code>. The evaluation for each response is stored in <code>results/mt-en-gpt3.5-turbo/eval_by_gpt/outputs.jsonl</code>.</p> <p>You can check the output of the evaluator LLM in the <code>llm_score_output</code> field.</p> <pre><code>head -n 1 results/mt-en-gpt3.5-turbo/eval_by_gpt/outputs.jsonl | jq -r \".llm_output\"\n</code></pre> <p>Info</p> <p><code>flexeval_file</code> just runs the same evaluation as <code>flexeval_lm</code> but with the given file. So, theoretically, you can perform the same evaluation with <code>flexeval_lm</code> in one go:</p> <pre><code>flexeval_lm \\\n  --language_model OpenAIChatAPI \\\n  --language_model.model \"gpt-4o-mini\" \\\n  --eval_setup \"mt-en\" \\\n  --metrics+=\"assistant_eval_en_single_turn\" \\\n  --save_dir \"results/mt-en_gpt3.5-turbo\"\n</code></pre> <p>Yet, we recommend separate the response generation (<code>flexeval_lm</code>) and evaluation (<code>flexeval_file</code>) so that you don't lost the response by some errors in the evaluation process.</p>"},{"location":"how_to/evaluate_with_llm_judges/#pairwise-judge-evaluation","title":"Pairwise Judge Evaluation","text":"<p>Sometimes, evaluating chat models individually cannot capture a subtle difference between models. In such cases, pairwise evaluation is useful.</p> <p>The overview of process is generating the responses using <code>flexeval_lm</code> and evaluating them with <code>flexeval_pairwise</code>.</p> <p>In this example, we will compare the responses from GPT3.5 and GPT-4o.</p> <p>First, generate the responses with GPT3.5. You can skip this if you have already generated the responses.</p> <pre><code>export OPENAI_API_KEY=\"YOUR_API_KEY\"\n\nflexeval_lm \\\n  --language_model OpenAIChatAPI \\\n  --language_model.model \"gpt-4o-mini\" \\\n  --eval_setup \"mt-en\" \\\n  --save_dir \"results/mt-en_gpt3.5-turbo\"\n</code></pre> <p>Generate the responses with GPT-4o.</p> <pre><code>flexeval_lm \\\n  --language_model OpenAIChatAPI \\\n  --language_model.model \"gpt-4o\" \\\n  --eval_setup \"mt-en\" \\\n  --save_dir \"results/mt-en_gpt-4o\"\n</code></pre> <p>Now, compare the responses with GPT-4.</p> <pre><code>flexeval_pairwise \\\n  --lm_output_paths.gpt_3_5 \"results/mt-en_gpt3.5-turbo/outputs.jsonl\"  \\\n  --lm_output_paths.gpt_4o \"results/mt-en_gpt-4o/outputs.jsonl\"  \\\n  --judge \"assistant_judge_en_single_turn\" \\\n  --save_dir \"results/mt-en_gpt3.5_vs_gpt4o\"\n</code></pre> <p>\u2615\ufe0f It may take a while to finish the evaluation...</p> <p>You can see the result in <code>results/mt-en_gpt3.5_vs_gpt4o/scores.json</code>.</p> <pre><code>{\n    \"win_rate\": {\n        \"gpt_4o\": 85.3125,\n        \"gpt_3_5\": 14.6875\n    },\n    \"bradley_terry\": {\n        \"gpt_4o\": 1152.8129577165919,\n        \"gpt_3_5\": 847.1870422834081\n    }\n}\n</code></pre> <p>The <code>win_rate</code> shows the percentage of wins of each model. The <code>bradley_terry</code> shows the Bradley-Terry score of each model.</p>"},{"location":"how_to/evaluate_with_llm_judges/#whats-next","title":"What's Next?","text":"<ul> <li>To define your own LLM evaluator, see Configuration Guide.</li> </ul>"},{"location":"how_to/implement_your_own_module/","title":"Implement Your Own Module","text":"<p>FlexEval is designed to be highly extensible, allowing you to implement your own modules without changing the core codebase. Whether you want to add new language models, evaluation setups, prompt templates, or metrics, FlexEval provides a flexible framework for you to do so.</p> <p>In this guide, we'll walk through the process of implementing your own module in FlexEval.</p>"},{"location":"how_to/implement_your_own_module/#step-1-identify-the-module-type","title":"Step 1: Identify the Module Type","text":"<p>First, determine which type of module you want to implement. FlexEval supports the modules listed in the API Reference.</p> <p>For this guide, we'll focus on implementing a new <code>Metric</code>.</p>"},{"location":"how_to/implement_your_own_module/#step-2-create-your-module","title":"Step 2: Create Your Module","text":"<p>Create a new Python file and inherit the appropriate base class provided by <code>flexeval</code>.</p> <pre><code>mkdir custom_modules\ntouch custom_modules/__init__.py\ntouch custom_modules/my_custom_metric.py\n</code></pre> <p>Note</p> <p>Make sure the module is importable from your program by adding an <code>__init__.py</code> file in the directory.</p> <p>All you need to do is implement the required methods based on the module type. Let's create a simple custom metric that calculates the length ratio of the generated text to the reference text.</p> <p><code>custom_modules/my_custom_metric.py</code>:</p> <pre><code>from flexeval import Metric, MetricResult\n\n\nclass MyCustomMetric(Metric):\n    \"\"\"\n    My custom metric implementation.\n    This class reports the length ratio of the generated text to the reference text.\n    \"\"\"\n    def evaluate(\n        self,\n        lm_outputs: list[str],\n        extra_info_list: list[dict[str, str]],\n        references_list: list[list[str]],\n    ) -&gt; MetricResult:\n        length_ratios = [\n            len(lm_output) / len(references[0])  # Assuming a single reference\n            for lm_output, references in zip(lm_outputs, references_list)\n        ]\n\n        return MetricResult(\n            {\"length_ratio\": sum(length_ratios) / len(length_ratios)},\n            instance_details=[{\"length_ratio\": ratio} for ratio in length_ratios],\n        )\n</code></pre>"},{"location":"how_to/implement_your_own_module/#step-3-specify-the-module-in-the-configuration","title":"Step 3: Specify the Module in the Configuration","text":"<p>Now you can use your custom metric to run evaluations.</p> <p>It can be specified in the configuration file as follows:</p> <pre><code>{\n  class_path: 'Generation',\n  init_args: {\n    metrics: [\n      {class_path: \"custom_modules.my_custom_metric.MyCustomMetric\" }\n    ]\n  }\n}\n</code></pre> <p>Note</p> <p>Make sure the <code>class_path</code> is a full import path to the module. Unlike the core modules, the program cannot locate your custom module without the full path.</p> <p>Or add it from the command line:</p> <pre><code>flexeval_lm \\\n  --language_model HuggingFaceLM \\\n  --language_model.model \"sbintuitions/tiny-lm\" \\\n  --eval_setup \"commonsense_qa\" \\\n  --eval_setup.metrics+=\"custom_modules.my_custom_metric.MyCustomMetric\"\n</code></pre> <p>Info</p> <p>The argument <code>--eval_setup.metrics</code> can take a list of metric classes. You can use <code>+=</code> to add your custom metric to the existing metrics in the config.</p> <p>You will see the new metric <code>length_ratio</code> in the results.</p> <p>Now you've successfully implemented your own module \ud83c\udf89.</p> <p>If you believe your module would be useful for others, consider contributing it to the official repository.</p>"},{"location":"preset_configs/","title":"Preset Configs","text":"<p>You can check the config using the following command: <pre><code>flexeval_presets &lt;config_name&gt;\n</code></pre></p>"},{"location":"preset_configs/#evalsetup","title":"EvalSetup","text":""},{"location":"preset_configs/#code_chat","title":"code_chat","text":"<ul> <li>mbpp_chat</li> </ul>"},{"location":"preset_configs/#code_generation","title":"code_generation","text":"<ul> <li>jhumaneval</li> <li>jhumaneval_tab_indent</li> <li>mbpp</li> <li>mbpp_tab_indent</li> <li>openai_humaneval</li> <li>openai_humaneval_tab_indent</li> </ul>"},{"location":"preset_configs/#en_chat","title":"en_chat","text":"<ul> <li>mt-en</li> <li>vicuna-en</li> </ul>"},{"location":"preset_configs/#en_generation","title":"en_generation","text":"<ul> <li>babi</li> <li>commonsense_qa</li> <li>gsm8k</li> <li>squad_v1</li> <li>trivia_qa</li> <li>twitter_sentiment</li> </ul>"},{"location":"preset_configs/#en_multiple_choice","title":"en_multiple_choice","text":"<ul> <li>arc_challenge</li> <li>arc_easy</li> <li>commonsense_qa_mc</li> <li>hellaswag</li> <li>openbookqa</li> <li>piqa</li> <li>xwinograd_en</li> </ul>"},{"location":"preset_configs/#en_perplexity","title":"en_perplexity","text":"<ul> <li>tiny_shakespeare</li> </ul>"},{"location":"preset_configs/#ja_chat","title":"ja_chat","text":"<ul> <li>aio_chat</li> <li>elyza_tasks_100</li> <li>mgsm_ja_chat</li> <li>mt-ja</li> <li>rakuda-v2-ja</li> <li>vicuna-ja</li> </ul>"},{"location":"preset_configs/#ja_generation","title":"ja_generation","text":"<ul> <li>aio</li> <li>jamcqa</li> <li>jcommonsenseqa</li> <li>jnli</li> <li>jsquad</li> <li>mgsm_ja</li> <li>wrime_pos_neg</li> <li>xlsum_ja</li> </ul>"},{"location":"preset_configs/#ja_multiple_choice","title":"ja_multiple_choice","text":"<ul> <li>jcommonsenseqa_mc</li> <li>xwinograd_ja</li> </ul>"},{"location":"preset_configs/#translation","title":"translation","text":"<ul> <li>wmt20_en_ja</li> <li>wmt20_ja_en</li> </ul>"},{"location":"preset_configs/#translation_chat","title":"translation_chat","text":"<ul> <li>wmt20_en_ja_chat</li> <li>wmt20_ja_en_chat</li> </ul>"},{"location":"preset_configs/#metric","title":"Metric","text":"<ul> <li>assistant_eval_en_single_turn</li> <li>assistant_eval_ja_single_turn</li> <li>elyza_tasks_100_eval</li> </ul>"},{"location":"preset_configs/#pairwisejudge","title":"PairwiseJudge","text":"<ul> <li>assistant_judge_en_single_turn</li> <li>assistant_judge_ja_single_turn</li> </ul>"},{"location":"preset_configs/EvalSetup/","title":"EvalSetup","text":""},{"location":"preset_configs/EvalSetup/#code_chat","title":"code_chat","text":"<ul> <li>mbpp_chat</li> </ul>"},{"location":"preset_configs/EvalSetup/#code_generation","title":"code_generation","text":"<ul> <li>jhumaneval</li> <li>jhumaneval_tab_indent</li> <li>mbpp</li> <li>mbpp_tab_indent</li> <li>openai_humaneval</li> <li>openai_humaneval_tab_indent</li> </ul>"},{"location":"preset_configs/EvalSetup/#en_chat","title":"en_chat","text":"<ul> <li>mt-en</li> <li>vicuna-en</li> </ul>"},{"location":"preset_configs/EvalSetup/#en_generation","title":"en_generation","text":"<ul> <li>babi</li> <li>commonsense_qa</li> <li>gsm8k</li> <li>squad_v1</li> <li>trivia_qa</li> <li>twitter_sentiment</li> </ul>"},{"location":"preset_configs/EvalSetup/#en_multiple_choice","title":"en_multiple_choice","text":"<ul> <li>arc_challenge</li> <li>arc_easy</li> <li>commonsense_qa_mc</li> <li>hellaswag</li> <li>openbookqa</li> <li>piqa</li> <li>xwinograd_en</li> </ul>"},{"location":"preset_configs/EvalSetup/#en_perplexity","title":"en_perplexity","text":"<ul> <li>tiny_shakespeare</li> </ul>"},{"location":"preset_configs/EvalSetup/#ja_chat","title":"ja_chat","text":"<ul> <li>aio_chat</li> <li>elyza_tasks_100</li> <li>mgsm_ja_chat</li> <li>mt-ja</li> <li>rakuda-v2-ja</li> <li>vicuna-ja</li> </ul>"},{"location":"preset_configs/EvalSetup/#ja_generation","title":"ja_generation","text":"<ul> <li>aio</li> <li>jamcqa</li> <li>jcommonsenseqa</li> <li>jnli</li> <li>jsquad</li> <li>mgsm_ja</li> <li>wrime_pos_neg</li> <li>xlsum_ja</li> </ul>"},{"location":"preset_configs/EvalSetup/#ja_multiple_choice","title":"ja_multiple_choice","text":"<ul> <li>jcommonsenseqa_mc</li> <li>xwinograd_ja</li> </ul>"},{"location":"preset_configs/EvalSetup/#translation","title":"translation","text":"<ul> <li>wmt20_en_ja</li> <li>wmt20_ja_en</li> </ul>"},{"location":"preset_configs/EvalSetup/#translation_chat","title":"translation_chat","text":"<ul> <li>wmt20_en_ja_chat</li> <li>wmt20_ja_en_chat</li> </ul>"},{"location":"preset_configs/EvalSetup/code_chat/","title":"Code chat","text":""},{"location":"preset_configs/EvalSetup/code_chat/#mbpp_chat","title":"mbpp_chat","text":"<p>Mostly Basic Python Problems (MBPP) is a dataset of crowd-sourced programming problems. This is a evaluation setup for chat LLMs.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Program Synthesis with Large Language Models <pre><code>local dataset_base_args = {\n  class_path: 'HFChatDataset',\n  init_args: {\n    path: 'mbpp',\n    subset: 'sanitized',\n    input_template: std.stripChars(|||\n      Generate a Python function that satisfies the following question and test cases.\n      ## Question\n      {{ prompt }}\n      ## Test cases\n      ```python\n      {{ test_list | join('\\n') }}\n      ```\n    |||, '\\n'),\n  },\n};\n\n{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'test', reference_list_template: '{{ test_list | join(\"\\n\") }}' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'prompt', reference_template: '```python\\n{{ code }}\\n```' } },\n        num_shots: 3,\n      },\n    },\n    metrics: [\n      { class_path: 'CodeEval', init_args: { lm_output_processor: { class_path: 'RegexExtractor', init_args: { pattern: '```python\\n(.*?)\\n```' } } } },\n    ],\n    gen_kwargs: { max_new_tokens: 512 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/code_generation/","title":"Code generation","text":""},{"location":"preset_configs/EvalSetup/code_generation/#jhumaneval","title":"jhumaneval","text":"<p>Zero-shot Python code generation task in Japanese.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>LLM \u306f\u65e5\u672c\u8a9e\u8ffd\u52a0\u5b66\u7fd2\u306b\u3088\u308a\u8a00\u8a9e\u9593\u77e5\u8b58\u8ee2\u79fb\u3092\u8d77\u3053\u3059\u306e\u304b\uff1f <pre><code>{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFGenerationDataset',\n      init_args: {\n        path: 'kogi-jwu/jhumaneval',\n        split: 'test',\n        reference_template: '{{ test }}\\n\\ncheck({{ entry_point }})\\n',\n      },\n    },\n    prompt_template: '{{ prompt }}',\n    metrics: [\n      { class_path: 'CodeEval', init_args: { code_template: '{{ prompt }}{{ lm_output }}' } },\n    ],\n    gen_kwargs: { max_new_tokens: 512, stop_sequences: ['\\nclass', '\\ndef', '\\n#', '\\n@', '\\nprint', '\\nif', '\\n```'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/code_generation/#jhumaneval_tab_indent","title":"jhumaneval_tab_indent","text":"<p>Zero-shot Python code generation task in Japanese.</p> <p>This is a version of jhumaneval preprocessed to replace indentation spaces with tabs. Some models (e.g., Llama) seems to have trouble with spaces in the prompt. <pre><code>local original_config = import './jhumaneval.jsonnet';\n\noriginal_config {\n  init_args+: {\n    eval_dataset+: {\n      init_args+: {\n        reference_template: '{{ test | replace(\"    \", \"\\t\") }}\\n\\ncheck({{ entry_point }})\\n',\n      },\n    },\n    prompt_template: \"{{ prompt | replace('    ', '\\t') }}\",\n    metrics: [\n      { class_path: 'CodeEval', init_args: { code_template: '{{ prompt | replace(\"    \", \"\\t\") }}{{ lm_output }}' } },\n    ],\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/EvalSetup/code_generation/#mbpp","title":"mbpp","text":"<p>Mostly Basic Python Problems (MBPP) is a dataset of crowd-sourced programming problems.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Program Synthesis with Large Language Models <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'mbpp',\n    subset: 'sanitized',\n    reference_list_template: '{{ test_list }}',\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'test' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'prompt' } },\n        num_shots: 3,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      ## Question\n      {{ item.prompt }}\n      ## Test cases\n      ```python\n      {{ item.test_list | join('\\n') }}\n      ```\n      ## Code\n      ```python\n      {{ item.code }}\n      ```\n      {% endfor %}\n      ## Question\n      {{ prompt }}\n      ## Test cases\n      ```python\n      {{ test_list | join('\\n') }}\n      ```\n      ## Code\n      ```python\n    |||,\n    metrics: [\n      { class_path: 'CodeEval' },\n    ],\n    gen_kwargs: { max_new_tokens: 512, stop_sequences: ['```'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/code_generation/#mbpp_tab_indent","title":"mbpp_tab_indent","text":"<p>Mostly Basic Python Problems (MBPP) is a dataset of crowd-sourced programming problems.</p> <p>This is a version of openai_humaneval preprocessed to replace indentation spaces with tabs. Some models (e.g., Llama) seems to have trouble with spaces in the prompt. <pre><code>local original_config = import './mbpp.jsonnet';\n\noriginal_config {\n  init_args+: {\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      ## Question\n      {{ item.prompt }}\n      ## Test cases\n      ```python\n      {{ item.test_list | join('\\n') }}\n      ```\n      ## Code\n      ```python\n      {{ item.code | replace('    ', '\\t') }}\n      ```\n      {% endfor %}\n      ## Question\n      {{ prompt }}\n      ## Test cases\n      ```python\n      {{ test_list | join('\\n') }}\n      ```\n      ## Code\n      ```python\n    |||,\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/EvalSetup/code_generation/#openai_humaneval","title":"openai_humaneval","text":"<p>Zero-shot Python code generation task developed by OpenAI.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Evaluating Large Language Models Trained on Code <pre><code>{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFGenerationDataset',\n      init_args: {\n        path: 'openai_humaneval',\n        split: 'test',\n        reference_template: '{{ test }}\\n\\ncheck({{ entry_point }})\\n',\n      },\n    },\n    prompt_template: '{{ prompt }}',\n    metrics: [\n      { class_path: 'CodeEval', init_args: { code_template: '{{ prompt }}{{ lm_output }}' } },\n    ],\n    gen_kwargs: { max_new_tokens: 512, stop_sequences: ['\\nclass', '\\ndef', '\\n#', '\\n@', '\\nprint', '\\nif', '\\n```'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/code_generation/#openai_humaneval_tab_indent","title":"openai_humaneval_tab_indent","text":"<p>Zero-shot Python code generation task developed by OpenAI.</p> <p>This is a version of openai_humaneval preprocessed to replace indentation spaces with tabs. Some models (e.g., Llama) seems to have trouble with spaces in the prompt. <pre><code>local original_config = import './openai_humaneval.jsonnet';\n\noriginal_config {\n  init_args+: {\n    eval_dataset+: {\n      init_args+: {\n        reference_template: '{{ test | replace(\"    \", \"\\t\") }}\\n\\ncheck({{ entry_point }})\\n',\n      },\n    },\n    prompt_template: '{{ prompt | replace(\"    \", \"\\t\") }}',\n    metrics: [\n      { class_path: 'CodeEval', init_args: { code_template: '{{ prompt | replace(\"    \", \"\\t\") }}{{ lm_output }}' } },\n    ],\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/EvalSetup/en_chat/","title":"En chat","text":""},{"location":"preset_configs/EvalSetup/en_chat/#mt-en","title":"mt-en","text":"<p>Multi-Turn Benchmark for large language models.</p> <p>References:</p> <ul> <li>Data Source</li> <li>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena <pre><code>{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: {\n      class_path: 'ChatbotBench',\n      init_args: {\n        path_or_name: 'mt-en',\n        ref_path_or_name: 'mt-en-ref-gpt4',\n      },\n    },\n    metrics: [\n      { class_path: 'OutputLengthStats' },\n    ],\n    gen_kwargs: { max_new_tokens: 1024 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_chat/#vicuna-en","title":"vicuna-en","text":"<p>Vicuna Benchmark for large language models.</p> <p>References:</p> <ul> <li>Data Source <pre><code>{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: {\n      class_path: 'ChatbotBench',\n      init_args: {\n        path_or_name: 'vicuna-en',\n        ref_path_or_name: 'vicuna-en-ref-gpt4',\n      },\n    },\n    metrics: [\n      { class_path: 'OutputLengthStats' },\n    ],\n    gen_kwargs: { max_new_tokens: 1024 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_generation/","title":"En generation","text":""},{"location":"preset_configs/EvalSetup/en_generation/#babi","title":"babi","text":"<p>Synthetic question answering dataset with reasoning questions.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'Muennighoff/babi',\n    reference_template: '{{ answer }}',\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 3,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      Passage: {{ item.passage | trim }}\n      Question: {{ item.question }}\n      Answer: \"{{ item.references[0] }}\"\n      {% endfor %}\n      Passage: {{ passage | trim }}\n      Question: {{ question }}\n    ||| + 'Answer: \"',\n    metrics: [\n      { class_path: 'CharF1' },\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 32, stop_sequences: ['\"'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_generation/#commonsense_qa","title":"commonsense_qa","text":"<p>CommonsenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers. This is a setup for generating answers based on the choices provided.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'tau/commonsense_qa',\n    reference_template: '{% set answer_index = choices.label.index(answerKey) %}{{ choices.text[answer_index] }}',\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 2,\n      },\n    },\n    prompt_template: |||\n      Choose the correct answer from the choices.\n      {% for item in few_shot_data %}\n      Choices:\n      0. \"{{ item.choices.text[0] }}\"\n      1. \"{{ item.choices.text[1] }}\"\n      2. \"{{ item.choices.text[2] }}\"\n      3. \"{{ item.choices.text[3] }}\"\n      4. \"{{ item.choices.text[4] }}\"\n      Question: {{ item.question }}\n      Answer: \"{{ item.references[0] }}\"\n      {% endfor %}\n      Choices:\n      0. \"{{ choices.text[0] }}\"\n      1. \"{{ choices.text[1] }}\"\n      2. \"{{ choices.text[2] }}\"\n      3. \"{{ choices.text[3] }}\"\n      4. \"{{ choices.text[4] }}\"\n      Question: {{question}}\n    ||| + 'Answer: \"',\n    metrics: [\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 40, stop_sequences: ['\"'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_generation/#gsm8k","title":"gsm8k","text":"<p>GSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.</p> <p>References:</p> <ul> <li>[Hugging Face Dataset](https://huggingface.co/datasets/gsm8k]</li> <li>Training Verifiers to Solve Math Word Problems <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'gsm8k',\n    subset: 'main',\n    reference_template: '{{ answer | regex_replace(\"&lt;&lt;.*?&gt;&gt;\", \"\") }}',\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'test' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 4,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      Q: {{ item.question }}\n      A: {{ item.references[0] }}\n      {% endfor %}\n      Q: {{ question }}\n    ||| + 'A:',\n    metrics: [\n      {\n        class_path: 'ExactMatch',\n        init_args: {\n          lm_output_processor: { class_path: 'RegexExtractor', init_args: { pattern: '-?[0-9.,]+' } },\n          reference_processor: { class_path: 'RegexExtractor', init_args: { pattern: '-?[0-9.,]+' } },\n        },\n      },\n    ],\n    gen_kwargs: { max_new_tokens: 256, stop_sequences: ['Q:'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_generation/#squad_v1","title":"squad_v1","text":"<p>Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>SQuAD: 100,000+ Questions for Machine Comprehension of Text <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'rajpurkar/squad',\n    reference_list_template: '{{ answers.text }}',\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 2,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      Context: {{ item.context | trim }}\n      Question: {{ item.question }}\n      Answer: \"{{ item.references[0] }}\"\n      {% endfor %}\n      Context: {{ context | trim }}\n      Question: {{ question }}\n    ||| + 'Answer: \"',\n    metrics: [\n      { class_path: 'CharF1' },\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 32, stop_sequences: ['\"'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_generation/#trivia_qa","title":"trivia_qa","text":"<p>TriviaqQA is a reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaqQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'trivia_qa',\n    subset: 'rc.nocontext',\n    reference_list_template: '{{ answer.aliases }}',\n  },\n};\n\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 0,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      Question: {{ item.question }}\n      Answer: \"{{ item.references[0] }}\"\n      {% endfor %}\n      Question: {{ question }}\n    ||| + 'Answer: \"',\n    metrics: [\n      { class_path: 'CharF1' },\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 32, stop_sequences: ['\"'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_generation/#twitter_sentiment","title":"twitter_sentiment","text":"<p>TSATC: Twitter Sentiment Analysis Training Corpus. This dataset is a preprocessed version of the original dataset. See the hugging face dataset page for more information.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Twitter Sentiment Analysis Training Corpus (Dataset) <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'carblacac/twitter-sentiment-analysis',\n    reference_template: \"{{ ['Positive', 'Negative'][feeling] }}\",\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'test' } },\n    few_shot_generator: {\n      class_path: 'BalancedFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 4,\n      },\n    },\n    prompt_template: |||\n      Classify the sentiment of the following tweet.\n      {% for item in few_shot_data %}\n      Tweet: {{ item.text }}\n      Sentiment: `{{ item.references[0] }}`\n      {% endfor %}\n      Tweet: {{ text }}\n    ||| + 'Sentiment: `',\n    metrics: [\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 8, stop_sequences: ['`'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/","title":"En multiple choice","text":""},{"location":"preset_configs/EvalSetup/en_multiple_choice/#arc_challenge","title":"arc_challenge","text":"<p>The ARC dataset contains 7,787 genuine grade-school level, multiple-choice science questions, assembled to encourage research in advanced question-answering. The dataset is partitioned into a Challenge Set and an Easy Set, and this is the Challenge Set.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge <pre><code>local dataset_base_args = {\n  path: 'allenai/ai2_arc',\n  subset: 'ARC-Challenge',\n  choices_templates: [\n    '{% if choices.text | length &gt; 0 %}{{ choices.text[0] }}{% endif %}',\n    '{% if choices.text | length &gt; 1 %}{{ choices.text[1] }}{% endif %}',\n    '{% if choices.text | length &gt; 2 %}{{ choices.text[2] }}{% endif %}',\n    '{% if choices.text | length &gt; 3 %}{{ choices.text[3] }}{% endif %}',\n    '{% if choices.text | length &gt; 4 %}{{ choices.text[4] }}{% endif %}',\n  ],\n  // answerKey is one of A, B, C, D, E, 1, 2, 3, 4\n  answer_index_template: '{% if answerKey == \"A\" %}0{% elif answerKey == \"B\" %}1{% elif answerKey == \"C\" %}2{% elif answerKey == \"D\" %}3{% elif answerKey == \"E\" %}3{% else %}{{ answerKey | int - 1 }}{% endif %}',\n  whitespace_before_choices: true,\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'test' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 4,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      Question: {{ item.question }}\n      Answer:{{ item.choices[item.answer_index] }}\n      {% endfor %}\n      Question: {{ question }}\n    ||| + 'Answer:',\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/#arc_easy","title":"arc_easy","text":"<p>The ARC dataset contains 7,787 genuine grade-school level, multiple-choice science questions, assembled to encourage research in advanced question-answering. The dataset is partitioned into a Challenge Set and an Easy Set, and this is the Easy Set.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge <pre><code>local dataset_base_args = {\n  path: 'allenai/ai2_arc',\n  subset: 'ARC-Easy',\n  choices_templates: [\n    '{% if choices.text | length &gt; 0 %}{{ choices.text[0] }}{% endif %}',\n    '{% if choices.text | length &gt; 1 %}{{ choices.text[1] }}{% endif %}',\n    '{% if choices.text | length &gt; 2 %}{{ choices.text[2] }}{% endif %}',\n    '{% if choices.text | length &gt; 3 %}{{ choices.text[3] }}{% endif %}',\n    '{% if choices.text | length &gt; 4 %}{{ choices.text[4] }}{% endif %}',\n  ],\n  // answerKey is one of A, B, C, D, E, 1, 2, 3, 4\n  answer_index_template: '{% if answerKey == \"A\" %}0{% elif answerKey == \"B\" %}1{% elif answerKey == \"C\" %}2{% elif answerKey == \"D\" %}3{% elif answerKey == \"E\" %}3{% else %}{{ answerKey | int - 1 }}{% endif %}',\n  whitespace_before_choices: true,\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'test' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 4,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      Question: {{ item.question }}\n      Answer:{{ item.choices[item.answer_index] }}\n      {% endfor %}\n      Question: {{ question }}\n    ||| + 'Answer:',\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/#commonsense_qa_mc","title":"commonsense_qa_mc","text":"<p>CommonsenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers. This is a setup for multiple choice where the model chooses the correct answer based on the log-probabilities of the choices.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge <pre><code>local dataset_base_args = {\n  path: 'tau/commonsense_qa',\n  choices_templates: ['{{ choices.text[0] }}', '{{ choices.text[1] }}', '{{ choices.text[2] }}', '{{ choices.text[3] }}', '{{ choices.text[4] }}'],\n  answer_index_template: '{% if answerKey == \"A\" %}0{% elif answerKey == \"B\" %}1{% elif answerKey == \"C\" %}2{% elif answerKey == \"D\" %}3{% elif answerKey == \"E\" %}4{% endif %}',\n  whitespace_before_choices: true,\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'validation' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 4,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      Question: {{ item.question }}\n      Answer:{{ item.choices[item.answer_index] }}\n      {% endfor %}\n      Question: {{ question }}\n    ||| + 'Answer:',\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/#hellaswag","title":"hellaswag","text":"<p>Hellaswag is a dataset for physically situated commonsense reasoning. The dataset is constructed through adversarial filtering to make it challenging for models to perform well.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>HellaSwag: Can a Machine Really Finish Your Sentence? <pre><code>local dataset_base_args = {\n  path: 'Rowan/hellaswag',\n  choices_templates: ['{{ endings[0] }}', '{{ endings[1] }}', '{{ endings[2] }}', '{{ endings[3] }}'],\n  answer_index_template: '{{ label }}',\n  whitespace_before_choices: true,\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'validation' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 4,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      {{ item.ctx }}{{ item.choices[item.answer_index] }}\n      {% endfor %}\n    ||| + '{{ ctx }}',\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/#openbookqa","title":"openbookqa","text":"<p>OpenBookQA contains questions that require multi-step reasoning, use of additional common and commonsense knowledge, and rich text comprehension.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering <pre><code>local dataset_base_args = {\n  path: 'allenai/openbookqa',\n  subset: 'main',\n  choices_templates: ['{{ choices.text[0] }}', '{{ choices.text[1] }}', '{{ choices.text[2] }}', '{{ choices.text[3] }}'],\n  answer_index_template: '{% if answerKey == \"A\" %}0{% elif answerKey == \"B\" %}1{% elif answerKey == \"C\" %}2{% elif answerKey == \"D\" %}3{% endif %}',\n  whitespace_before_choices: true,\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'test' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 4,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      Question: {{ item.question_stem }}\n      Answer:{{ item.choices[item.answer_index] }}\n      {% endfor %}\n      Question: {{ question_stem }}\n    ||| + 'Answer:',\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/#piqa","title":"piqa","text":"<p>The PIQA dataset introduces the task of physical commonsense reasoning and a corresponding benchmark dataset</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>PIQA: Reasoning about Physical Commonsense in Natural Language <pre><code>local dataset_base_args = {\n  path: 'ybisk/piqa',\n  choices_templates: ['{{ sol1 }}', '{{ sol2 }}'],\n  answer_index_template: '{{ label }}',\n  whitespace_before_choices: true,\n  dataset_kwargs: { trust_remote_code: true },\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'validation' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 4,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      {{ item.goal }}{{ item.choices[item.answer_index] }}\n      {% endfor %}\n    ||| + '{{ goal }}',\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_multiple_choice/#xwinograd_en","title":"xwinograd_en","text":"<p>XWinograd is a multilingual collection of Winograd Schemas in six languages that can be used for evaluation of cross-lingual commonsense reasoning capabilities. This is an English subset of the dataset.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>It\u2019s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning <pre><code>{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: {\n        path: 'Muennighoff/xwinograd',\n        subset: 'en',\n        split: 'test',\n        choices_templates: [\n          '{{ option1 }}{{ sentence.split(\"_\")[1] }}',\n          '{{ option2 }}{{ sentence.split(\"_\")[1] }}',\n        ],\n        answer_index_template: '{{ answer | int - 1 }}',\n        input_templates: { context: '{{ sentence.split(\"_\")[0] }}' },\n      },\n    },\n    prompt_template: '{{ context }}',\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/en_perplexity/","title":"En perplexity","text":""},{"location":"preset_configs/EvalSetup/en_perplexity/#tiny_shakespeare","title":"tiny_shakespeare","text":"<p>40,000 lines of Shakespeare from a variety of Shakespeare's plays. Featured in Andrej Karpathy's blog post 'The Unreasonable Effectiveness of Recurrent Neural Networks'.</p> <p>References:</p> <ul> <li>Hugging Face Dataset <pre><code>{\n  class_path: 'Perplexity',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFTextDataset',\n      init_args: {\n        path: 'karpathy/tiny_shakespeare',\n        split: 'test',\n        text_template: '{{ text }}',\n      },\n    },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_chat/","title":"Ja chat","text":""},{"location":"preset_configs/EvalSetup/ja_chat/#aio_chat","title":"aio_chat","text":"<p>AI\u738b (AI king) is a Japanese quiz dataset developed for research and competition purposes. This is a evaluation setup for chat LLMs.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>AI\u738b \u301c\u30af\u30a4\u30baAI\u65e5\u672c\u4e00\u6c7a\u5b9a\u6226\u301c</li> <li>JAQKET: \u30af\u30a4\u30ba\u3092\u984c\u6750\u306b\u3057\u305f\u65e5\u672c\u8a9e QA \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u69cb\u7bc9 <pre><code>local dataset_base_args = {\n  class_path: 'HFChatDataset',\n  init_args: {\n    path: 'llm-book/aio',\n    input_template: '{{ question }}',\n    reference_list_template: '{{ answers }}',\n    dataset_kwargs: { trust_remote_code: true },\n  },\n};\n\n{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 4,\n      },\n    },\n    metrics: [\n      {\n        class_path: 'CharF1',\n        init_args: {\n          lm_output_processor: { class_path: 'AIONormalizer' },\n          reference_processor: { class_path: 'AIONormalizer' },\n        },\n      },\n      {\n        class_path: 'ExactMatch',\n        init_args: {\n          lm_output_processor: { class_path: 'AIONormalizer' },\n          reference_processor: { class_path: 'AIONormalizer' },\n        },\n      },\n    ],\n    gen_kwargs: { max_new_tokens: 32 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_chat/#elyza_tasks_100","title":"elyza_tasks_100","text":"<p>A dataset for evaluating instruction-tuned models developed by ELYZA Inc.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>\u516c\u5f0f\u30d6\u30ed\u30b0 <pre><code>{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFChatDataset',\n      init_args: {\n        path: 'elyza/ELYZA-tasks-100',\n        split: 'test',\n        input_template: '{{ input }}',\n        reference_template: '{{ output }}',\n        extra_info_templates: { eval_aspect: '{{ eval_aspect }}' },\n      },\n    },\n    metrics: [\n      { class_path: 'OutputLengthStats' },\n    ],\n    gen_kwargs: { max_new_tokens: 1024 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_chat/#mgsm_ja_chat","title":"mgsm_ja_chat","text":"<p>Multilingual Grade School Math Benchmark (MGSM) is a benchmark of grade-school math problems. This is a Japanese subset of the benchmark. This is a evaluation setup for chat LLMs.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Language Models are Multilingual Chain-of-Thought Reasoners <pre><code>local dataset_base_args = {\n  class_path: 'HFChatDataset',\n  init_args: {\n    path: 'juletxara/mgsm',\n    subset: 'ja',\n    reference_template: '{{ answer }}',\n  },\n};\n\n{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'test', input_template: '\u554f\u984c: {{ question }}' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train', input_template: '{{ question }}' } },\n        num_shots: 4,\n      },\n    },\n    metrics: [\n      { class_path: 'ExactMatch', init_args: { lm_output_processor: { class_path: 'RegexExtractor', init_args: { pattern: '-?[0-9.,]+' } } } },\n    ],\n    gen_kwargs: { max_new_tokens: 256 },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_chat/#mt-ja","title":"mt-ja","text":"<p>Multi-Turn Benchmark for large language models in Japanese.</p> <p>References:</p> <ul> <li>Data Source <pre><code>{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: {\n      class_path: 'ChatbotBench',\n      init_args: {\n        path_or_name: 'mt-ja',\n        ref_path_or_name: 'mt-ja-ref-gpt4',\n      },\n    },\n    metrics: [\n      { class_path: 'OutputLengthStats' },\n    ],\n    gen_kwargs: { max_new_tokens: 1024 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_chat/#rakuda-v2-ja","title":"rakuda-v2-ja","text":"<p>Rakuda benckmark concists of a set of 40 questions in Japanese about Japanese-specific topics designed to evaluate the capabilities of AI Assistants in Japanese.</p> <p>References:</p> <ul> <li>Original Repository</li> <li>Hugging Face Dataset <pre><code>{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: {\n      class_path: 'ChatbotBench',\n      init_args: {\n        path_or_name: 'rakuda-v2-ja',\n      },\n    },\n    metrics: [\n      { class_path: 'OutputLengthStats' },\n    ],\n    gen_kwargs: { max_new_tokens: 1024 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_chat/#vicuna-ja","title":"vicuna-ja","text":"<p>Vicuna Benchmark for large language models in Japanese.</p> <p>References:</p> <ul> <li>Data Source <pre><code>{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: {\n      class_path: 'ChatbotBench',\n      init_args: {\n        path_or_name: 'vicuna-ja',\n        ref_path_or_name: 'vicuna-ja-ref-gpt4',\n      },\n    },\n    metrics: [\n      { class_path: 'OutputLengthStats' },\n    ],\n    gen_kwargs: { max_new_tokens: 1024 },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/","title":"Ja generation","text":""},{"location":"preset_configs/EvalSetup/ja_generation/#aio","title":"aio","text":"<p>AI\u738b (AI king) is a Japanese quiz dataset developed for research and competition purposes.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>AI\u738b \u301c\u30af\u30a4\u30baAI\u65e5\u672c\u4e00\u6c7a\u5b9a\u6226\u301c</li> <li>JAQKET: \u30af\u30a4\u30ba\u3092\u984c\u6750\u306b\u3057\u305f\u65e5\u672c\u8a9e QA \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u69cb\u7bc9 <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'sbintuitions/aio-extended-answers',\n    split: 'validation',\n    reference_list_template: '{{ answers }}',\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args,\n    prompt_template: '{{ question }}\u7b54\u3048\u306f\u300c',\n    metrics: [\n      {\n        class_path: 'CharF1',\n        init_args: {\n          lm_output_processor: { class_path: 'AIONormalizer' },\n          reference_processor: { class_path: 'AIONormalizer' },\n        },\n      },\n      {\n        class_path: 'ExactMatch',\n        init_args: {\n          lm_output_processor: { class_path: 'AIONormalizer' },\n          reference_processor: { class_path: 'AIONormalizer' },\n        },\n      },\n    ],\n    gen_kwargs: { max_new_tokens: 64, stop_sequences: ['\u300d'] },\n    batch_size: 1,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/#jamcqa","title":"jamcqa","text":""},{"location":"preset_configs/EvalSetup/ja_generation/#jamc-qa","title":"JamC-QA","text":"<p>This benchmark evaluates knowledge specific to Japan through multiple-choice questions. It covers eight categories: culture, custom, regional_identity, geography, history, government, law, and healthcare. Achieving high performance requires broad and detailed understanding of Japan across these categories.</p> <p>References:</p> <ul> <li>Hugging Face Dataset <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'sbintuitions/JamC-QA',\n    subset: 'v1.0',\n    split: 'test',\n    reference_template: '{% set choices = [choice0, choice1, choice2, choice3] %}{{ choices[answer_index] }}',\n  },\n};\n\nlocal template_blank = |||\n  \u8cea\u554f:: {{ question }}, \u9078\u629e\u80a2::\n   {{ choice0 }}\n   {{ choice1 }}\n   {{ choice2 }}\n||| + ' {{ choice3 }}, \u56de\u7b54::';\n\nlocal template_culture = |||\n  \u4ee5\u4e0b\u306f\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3068\u3001\u8ffd\u52a0\u306e\u80cc\u666f\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u5165\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u8981\u6c42\u3092\u9069\u5207\u306b\u6e80\u305f\u3059\u56de\u7b54\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n  \u6307\u793a:: \u8cea\u554f\u3068\u56de\u7b54\u306e\u9078\u629e\u80a2\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308a\u3001\u9078\u629e\u80a2\u304b\u3089\u56de\u7b54\u3092\u9078\u629e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u4ed6\u306b\u306f\u4f55\u3082\u542b\u3081\u306a\u3044\u3053\u3068\u3092\u53b3\u5b88\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n  \u8cea\u554f:: \u79cb\u306e\u5b63\u8a9e\u3067\u306a\u3044\u3082\u306e\u3092\u9078\u629e\u80a2\u306e\u4e2d\u304b\u3089\u9078\u3079, \u9078\u629e\u80a2::\n   \u6708\u898b\n   \u7d05\u8449\n   \u9e97\u304b\n   \u5c71\u7ca7\u3046, \u56de\u7b54:: \u9e97\u304b\n  \u8cea\u554f:: \u4e03\u6bb5\u98fe\u308a\u306e\u96db\u4eba\u5f62\u3092\u98fe\u308b\u969b\u30015\u6bb5\u76ee\u306b\u4e26\u3079\u308b\u3082\u306e\u306f\u3069\u308c?, \u9078\u629e\u80a2::\n   \u968f\u8eab\n   \u5fa1\u99d5\u7bed\n   \u7baa\u7b25\n   \u4ed5\u4e01, \u56de\u7b54:: \u4ed5\u4e01\n  \u8cea\u554f:: \u842c\u6b73\u697d\u30fb\u65b0\u5e74\u30fb\u6771\u904a\u30fb\u8d8a\u5929\u697d\u306e\u3046\u3061\u3001\u50ac\u99ac\u697d\u306b\u5f53\u305f\u308b\u3082\u306e\u306f\u3069\u308c, \u9078\u629e\u80a2::\n   \u842c\u6b73\u697d\n   \u65b0\u5e74\n   \u8d8a\u5929\u697d\n   \u6771\u904a, \u56de\u7b54:: \u65b0\u5e74\n  \u8cea\u554f:: \u9905\u3064\u304d\u3067\u6775\u3092\u632f\u308b\u4eba\u306f\u3064\u304d\u624b\u3068\u547c\u3076\u304c\u3001\u3082\u3061\u7c73\u3092\u3072\u3063\u304f\u308a\u8fd4\u3059\u4eba\u306f\u4f55\u3068\u547c\u3076\u304b\u9078\u629e\u80a2\u304b\u3089\u9078\u3079, \u9078\u629e\u80a2::\n   \u8fd4\u3057\u624b\n   \u62bc\u3057\u624b\n   \u3053\u306d\u624b\n   \u3082\u307f\u624b, \u56de\u7b54:: \u8fd4\u3057\u624b\n||| + template_blank;\n\nlocal template_custom = |||\n  \u4ee5\u4e0b\u306f\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3068\u3001\u8ffd\u52a0\u306e\u80cc\u666f\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u5165\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u8981\u6c42\u3092\u9069\u5207\u306b\u6e80\u305f\u3059\u56de\u7b54\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n  \u6307\u793a:: \u8cea\u554f\u3068\u56de\u7b54\u306e\u9078\u629e\u80a2\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308a\u3001\u9078\u629e\u80a2\u304b\u3089\u56de\u7b54\u3092\u9078\u629e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u4ed6\u306b\u306f\u4f55\u3082\u542b\u3081\u306a\u3044\u3053\u3068\u3092\u53b3\u5b88\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n  \u8cea\u554f:: \u5730\u93ae\u796d\u3092\u5b9f\u65bd\u3059\u308b\u306e\u306b\u826f\u3044\u3068\u3055\u308c\u3066\u3044\u308b\u65e5\u306f\u3069\u308c, \u9078\u629e\u80a2::\n   \u5148\u8ca0\n   \u53cb\u5f15\n   \u4ecf\u6ec5\n   \u8d64\u53e3, \u56de\u7b54:: \u53cb\u5f15\n  \u8cea\u554f:: \u4ecf\u6559\u306e\u304a\u846c\u5f0f\u3084\u6cd5\u4e8b\u306e\u304a\u713c\u9999\u3067\u7528\u3044\u3089\u308c\u308b\u7d30\u304b\u3044\u9999\u6728\u306f\u3069\u308c?, \u9078\u629e\u80a2::\n   \u62b9\u9999\n   \u7dda\u9999\n   \u9999\u7089\n   \u6570\u73e0, \u56de\u7b54:: \u62b9\u9999\n  \u8cea\u554f:: \u304a\u6094\u3084\u307f\u306e\u8a00\u8449\u3067\u53e3\u982d\u3067\u4f7f\u3046\u306b\u306f\u3075\u3055\u308f\u3057\u304f\u306a\u3044\u3082\u306e\u3092\u9078\u3079, \u9078\u629e\u80a2::\n   \u6b8b\u5ff5\u3067\u306a\u308a\u307e\u305b\u3093\n   \u304a\u6094\u3084\u307f\u7533\u3057\u4e0a\u3052\u307e\u3059\n   \u3054\u6101\u50b7\u69d8\u3067\u3059\n   \u3054\u51a5\u798f\u3092\u304a\u7948\u308a\u3057\u307e\u3059, \u56de\u7b54:: \u3054\u51a5\u798f\u3092\u304a\u7948\u308a\u3057\u307e\u3059\n  \u8cea\u554f:: \u306e\u3057\u888b\u306e\u6e21\u3057\u65b9\u3068\u3057\u3066\u3001\u6b63\u3057\u304f\u306a\u3044\u8aac\u660e\u3092\u9078\u3079, \u9078\u629e\u80a2::\n   \u3075\u304f\u3055\u306b\u5305\u3093\u3067\u6301\u53c2\u3059\u308b\n   \u4e21\u624b\u3067\u6301\u3063\u3066\u6e21\u3059\n   \u7384\u95a2\u5148\u3067\u771f\u3063\u5148\u306b\u6e21\u3059\n   \u8868\u66f8\u304d\u304c\u76f8\u624b\u306b\u898b\u3048\u308b\u3088\u3046\u306b\u3057\u3066\u6e21\u3059, \u56de\u7b54:: \u7384\u95a2\u5148\u3067\u771f\u3063\u5148\u306b\u6e21\u3059\n||| + template_blank;\n\nlocal template_regional_identity = |||\n  \u4ee5\u4e0b\u306f\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3068\u3001\u8ffd\u52a0\u306e\u80cc\u666f\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u5165\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u8981\u6c42\u3092\u9069\u5207\u306b\u6e80\u305f\u3059\u56de\u7b54\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n  \u6307\u793a:: \u8cea\u554f\u3068\u56de\u7b54\u306e\u9078\u629e\u80a2\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308a\u3001\u9078\u629e\u80a2\u304b\u3089\u56de\u7b54\u3092\u9078\u629e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u4ed6\u306b\u306f\u4f55\u3082\u542b\u3081\u306a\u3044\u3053\u3068\u3092\u53b3\u5b88\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n  \u8cea\u554f:: \u65e5\u672c\u3067\u521d\u3081\u3066\u5c0f\u5b66\u6821\u304c\u3067\u304d\u305f\u306e\u306f\u3069\u3053\u304b\u9078\u3079, \u9078\u629e\u80a2::\n   \u5927\u962a\n   \u6a2a\u6d5c\n   \u4eac\u90fd\n   \u6771\u4eac, \u56de\u7b54:: \u4eac\u90fd\n  \u8cea\u554f:: \u5409\u6c34\u795e\u793e\u306e\u540d\u52dd\u3068\u3057\u3066\u77e5\u3089\u308c\u308b\u685c\u306e\u98a8\u666f\u306e\u5225\u540d\u306f\u3069\u308c?, \u9078\u629e\u80a2::\n   \u4e00\u76ee\u5341\u884c\n   \u4e00\u76ee\u5341\u5e74\n   \u4e00\u76ee\u5343\u672c\n   \u4e00\u76ee\u516b\u666f, \u56de\u7b54:: \u4e00\u76ee\u5341\u5e74\n  \u8cea\u554f:: \u5b87\u90fd\u5bae\u304c\u300c\u9903\u5b50\u306e\u307e\u3061\u300d\u3068\u547c\u3070\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u7406\u7531\u306b\u3064\u3044\u3066\u3001\u8aa4\u3063\u3066\u3044\u308b\u3082\u306e\u3092\u9078\u3079, \u9078\u629e\u80a2::\n   \u6c17\u5019\u306e\u5dee\u304c\u6fc0\u3057\u3044\u5b87\u90fd\u5bae\u5468\u8fba\u306f\u3001\u5c0f\u9ea6\u3068\u767d\u83dc\u3092\u4f5c\u308b\u306e\u306b\u9069\u3057\u3066\u3044\u305f\n   \u590f\u304c\u6691\u3044\u5b87\u90fd\u5bae\u3067\u3001\u30b9\u30bf\u30df\u30ca\u3092\u4ed8\u3051\u308b\u305f\u3081\u306b\u4eba\u6c17\u304c\u9ad8\u307e\u3063\u305f\n   \u6226\u6642\u4e2d\u3001\u5175\u968a\u304c\u4e2d\u56fd\u3067\u98df\u3079\u3066\u3044\u305f\u9903\u5b50\u3092\u3001\u6226\u5f8c\u306b\u306a\u3063\u3066\u5b87\u90fd\u5bae\u3067\u4f5c\u308a\u59cb\u3081\u305f\n   \u6614\u304b\u3089\u30ad\u30e3\u30d9\u30c4\u3084\u7389\u306d\u304e\u306e\u751f\u7523\u3084\u990a\u8c5a\u304c\u76db\u3093\u3060\u3063\u305f, \u56de\u7b54:: \u6614\u304b\u3089\u30ad\u30e3\u30d9\u30c4\u3084\u7389\u306d\u304e\u306e\u751f\u7523\u3084\u990a\u8c5a\u304c\u76db\u3093\u3060\u3063\u305f\n  \u8cea\u554f:: \u56db\u56fd\u516b\u5341\u516b\u304b\u6240\u306e\u4e00\u756a\u672d\u6240\u306f\u3069\u308c?, \u9078\u629e\u80a2::\n   \u970a\u5c71\u5bfa\n   \u6975\u697d\u5bfa\n   \u5927\u65e5\u5bfa\n   \u91d1\u6cc9\u5bfa, \u56de\u7b54:: \u970a\u5c71\u5bfa\n||| + template_blank;\n\nlocal template_geography = |||\n  \u4ee5\u4e0b\u306f\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3068\u3001\u8ffd\u52a0\u306e\u80cc\u666f\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u5165\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u8981\u6c42\u3092\u9069\u5207\u306b\u6e80\u305f\u3059\u56de\u7b54\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n  \u6307\u793a:: \u8cea\u554f\u3068\u56de\u7b54\u306e\u9078\u629e\u80a2\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308a\u3001\u9078\u629e\u80a2\u304b\u3089\u56de\u7b54\u3092\u9078\u629e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u4ed6\u306b\u306f\u4f55\u3082\u542b\u3081\u306a\u3044\u3053\u3068\u3092\u53b3\u5b88\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n  \u8cea\u554f:: \u65e5\u672c\u306e\u4eba\u5de5\u5cf6\u7a7a\u6e2f\u30672\u756a\u76ee\u306b\u53e4\u3044\u7a7a\u6e2f\u306f\u3069\u308c, \u9078\u629e\u80a2::\n   \u95a2\u897f\u56fd\u969b\u7a7a\u6e2f\n   \u4e2d\u90e8\u56fd\u969b\u7a7a\u6e2f\n   \u795e\u6238\u7a7a\u6e2f\n   \u7fbd\u7530\u7a7a\u6e2f, \u56de\u7b54:: \u4e2d\u90e8\u56fd\u969b\u7a7a\u6e2f\n  \u8cea\u554f:: 2024\u5e74\u73fe\u5728\u3001\u65e5\u672c\u3067\u4e8c\u756a\u76ee\u306b\u5927\u304d\u3044\u53e4\u58b3\u306f\u3069\u308c?, \u9078\u629e\u80a2::\n   \u5fdc\u795e\u5929\u7687\u9675\u53e4\u58b3\n   \u9020\u5c71\u53e4\u58b3\n   \u4ec1\u5fb3\u5929\u7687\u9675\u53e4\u58b3\n   \u7bb8\u5893\u53e4\u58b3, \u56de\u7b54:: \u5fdc\u795e\u5929\u7687\u9675\u53e4\u58b3\n  \u8cea\u554f:: \u6771\u4eac23\u533a\u306e\u3046\u3061\u30012024\u5e741\u6708\u6642\u70b9\u30672\u756a\u76ee\u306b\u4eba\u53e3\u304c\u591a\u3044\u533a\u3092\u9078\u629e\u80a2\u304b\u3089\u9078\u3079, \u9078\u629e\u80a2::\n   \u4e16\u7530\u8c37\u533a\n   \u5927\u7530\u533a\n   \u8db3\u7acb\u533a\n   \u7df4\u99ac\u533a, \u56de\u7b54:: \u7df4\u99ac\u533a\n  \u8cea\u554f:: 2024\u5e74\u73fe\u5728\u3001\u6700\u3082\u6a19\u9ad8\u304c\u4f4e\u3044\u5c71\u306f\u3069\u308c, \u9078\u629e\u80a2::\n   \u4fe1\u592b\u5c71\n   \u5bcc\u5c71\n   \u4e45\u80fd\u5c71\n   \u4e8c\u4e0a\u5c71, \u56de\u7b54:: \u4e45\u80fd\u5c71\n||| + template_blank;\n\nlocal template_history = |||\n  \u4ee5\u4e0b\u306f\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3068\u3001\u8ffd\u52a0\u306e\u80cc\u666f\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u5165\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u8981\u6c42\u3092\u9069\u5207\u306b\u6e80\u305f\u3059\u56de\u7b54\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n  \u6307\u793a:: \u8cea\u554f\u3068\u56de\u7b54\u306e\u9078\u629e\u80a2\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308a\u3001\u9078\u629e\u80a2\u304b\u3089\u56de\u7b54\u3092\u9078\u629e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u4ed6\u306b\u306f\u4f55\u3082\u542b\u3081\u306a\u3044\u3053\u3068\u3092\u53b3\u5b88\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n  \u8cea\u554f:: \u65e5\u672c\u6a4b\u306b\u3042\u308b\u3001\u65e5\u672c\u306e\u56fd\u9053\u306e\u8d77\u70b9\u3092\u793a\u3057\u305f\u300c\u65e5\u672c\u56fd\u9053\u8def\u5143\u6a19\u300d\u306e\u6587\u5b57\u3092\u66f8\u3044\u305f\u306e\u306f\u8ab0\u304b\u9078\u3079, \u9078\u629e\u80a2::\n   \u5fb3\u5ddd\u6176\u559c\n   \u4f0a\u85e4\u535a\u6587\n   \u7530\u4e2d\u89d2\u6804\n   \u4f50\u85e4\u6804\u4f5c, \u56de\u7b54:: \u4f50\u85e4\u6804\u4f5c\n  \u8cea\u554f:: \u6c5f\u6238\u6642\u4ee3\u306b\u8077\u696d\u3068\u3057\u3066\u5b58\u5728\u3057\u306a\u304b\u3063\u305f\u3082\u306e\u306f\u3069\u308c?, \u9078\u629e\u80a2::\n   \u8abf\u5f8b\u5e2b\n   \u5c0f\u4fbf\u4ef2\u9593\n   \u5c41\u8ca0\u6bd4\u4e18\u5c3c\n   \u3051\u3060\u3082\u306e\u5c4b, \u56de\u7b54:: \u8abf\u5f8b\u5e2b\n  \u8cea\u554f:: \u5e73\u5b89\u6642\u4ee3\u3001\u6d44\u571f\u6559\u3092\u4fe1\u4ef0\u3057\u305f\u85e4\u539f\u983c\u9053\u306b\u3088\u3063\u3066\u5efa\u3066\u3089\u308c\u305f\u4e16\u754c\u907a\u7523\u306f\u3069\u308c?, \u9078\u629e\u80a2::\n   \u4e2d\u5c0a\u5bfa\n   \u5e73\u7b49\u9662\n   \u5510\u62db\u63d0\u5bfa\n   \u91d1\u525b\u5bfa, \u56de\u7b54:: \u5e73\u7b49\u9662\n  \u8cea\u554f:: \u521d\u3081\u3066\u306e\u65e5\u672c\u90f5\u4fbf\u5207\u624b\u306b\u4f7f\u308f\u308c\u305f\u901a\u8ca8\u5358\u4f4d\u306f\u3069\u308c?, \u9078\u629e\u80a2::\n   \u5186\n   \u4e21\n   \u6587\n   \u92ad, \u56de\u7b54:: \u6587\n||| + template_blank;\n\nlocal template_government = |||\n  \u4ee5\u4e0b\u306f\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3068\u3001\u8ffd\u52a0\u306e\u80cc\u666f\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u5165\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u8981\u6c42\u3092\u9069\u5207\u306b\u6e80\u305f\u3059\u56de\u7b54\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n  \u6307\u793a:: \u8cea\u554f\u3068\u56de\u7b54\u306e\u9078\u629e\u80a2\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308a\u3001\u9078\u629e\u80a2\u304b\u3089\u56de\u7b54\u3092\u9078\u629e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u4ed6\u306b\u306f\u4f55\u3082\u542b\u3081\u306a\u3044\u3053\u3068\u3092\u53b3\u5b88\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n  \u8cea\u554f:: 2024\u5e7412\u6708\u73fe\u5728\u3001\u30de\u30a4\u30ca\u30f3\u30d0\u30fc\u30ab\u30fc\u30c9\u3092\u4f5c\u6210\u5f8c\u3001\u81ea\u4e3b\u7684\u306b\u8fd4\u7d0d\u3057\u3001\u518d\u5ea6\u4f5c\u6210\u3059\u308b\u5834\u5408(\u96fb\u5b50\u8a3c\u660e\u66f8\u306e\u767a\u884c\u542b\u3080)\u306b\u304b\u304b\u308b\u624b\u6570\u6599\u306f\u3044\u304f\u3089\u304b\u9078\u629e\u80a2\u306e\u4e2d\u304b\u3089\u9078\u3079, \u9078\u629e\u80a2::\n   500\u5186\n   800\u5186\n   1000\u5186\n   2000\u5186, \u56de\u7b54:: 1000\u5186\n  \u8cea\u554f:: 2024\u5e7412\u6708\u73fe\u5728\u306b\u5b58\u5728\u3059\u308b\u30018\u67081\u65e5\u304b\u3089\u7fcc\u5e747\u670831\u65e5\u306e\u533b\u7642\u4fdd\u967a\u3068\u4ecb\u8b77\u4fdd\u967a\u306e\u81ea\u5df1\u8ca0\u62c5\u984d\u304c\u9ad8\u984d\u306a\u5834\u5408\u3001\u81ea\u5df1\u8ca0\u62c5\u3092\u8efd\u6e1b\u3067\u304d\u308b\u5236\u5ea6\u306f\u3069\u308c?, \u9078\u629e\u80a2::\n   \u9ad8\u984d\u4ecb\u8b77\u5408\u7b97\u7642\u990a\u8cbb\u5236\u5ea6\n   \u9ad8\u984d\u7642\u990a\u8cbb\u5236\u5ea6\n   \u9ad8\u984d\u4ecb\u8b77\u30b5\u30fc\u30d3\u30b9\u8cbb\u5236\u5ea6\n   \u9ad8\u984d\u533b\u7642\u8cbb\u5236\u5ea6, \u56de\u7b54:: \u9ad8\u984d\u4ecb\u8b77\u5408\u7b97\u7642\u990a\u8cbb\u5236\u5ea6\n  \u8cea\u554f:: 2025\u5e742\u6708\u73fe\u5728\u3001\u56fd\u6c11\u751f\u6d3b\u30bb\u30f3\u30bf\u30fc\u306e\u554f\u3044\u5408\u308f\u305b\u5148\u306e\u96fb\u8a71\u756a\u53f7\u306f\u3069\u308c?, \u9078\u629e\u80a2::\n   03-5662-7637\n   03-3446-0999\n   188\n   03-3406-7644, \u56de\u7b54:: 03-3446-0999\n  \u8cea\u554f:: 2024\u5e7412\u6708\u73fe\u5728\u3001\u81ea\u52d5\u8eca\u3092\u89e3\u4f53\u51e6\u5206\u3057\u305f\u969b\u306b\u5fc5\u8981\u306a\u624b\u7d9a\u304d\u3092\u9078\u3079, \u9078\u629e\u80a2::\n   \u89e3\u4f53\u8a3c\u660e\u66f8\n   \u30ea\u30b5\u30a4\u30af\u30eb\u6cd5\n   \u4e00\u6642\u62b9\u6d88\u767b\u9332\n   \u6c38\u4e45\u62b9\u6d88\u767b\u9332, \u56de\u7b54:: \u6c38\u4e45\u62b9\u6d88\u767b\u9332\n||| + template_blank;\n\nlocal template_law = |||\n  \u4ee5\u4e0b\u306f\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3068\u3001\u8ffd\u52a0\u306e\u80cc\u666f\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u5165\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u8981\u6c42\u3092\u9069\u5207\u306b\u6e80\u305f\u3059\u56de\u7b54\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n  \u6307\u793a:: \u8cea\u554f\u3068\u56de\u7b54\u306e\u9078\u629e\u80a2\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308a\u3001\u9078\u629e\u80a2\u304b\u3089\u56de\u7b54\u3092\u9078\u629e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u4ed6\u306b\u306f\u4f55\u3082\u542b\u3081\u306a\u3044\u3053\u3068\u3092\u53b3\u5b88\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n  \u8cea\u554f:: 2024\u5e74\u73fe\u5728\u3001\u901f\u5ea6\u6307\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u9ad8\u901f\u9053\u8def\u3067\u306e\u6700\u4f4e\u6cd5\u5b9a\u901f\u5ea6\u306f\u3069\u308c, \u9078\u629e\u80a2::\n   \u6642\u901f50\u30ad\u30ed\u30e1\u30fc\u30c8\u30eb\n   \u6642\u901f30\u30ad\u30ed\u30e1\u30fc\u30c8\u30eb\n   \u6642\u901f40\u30ad\u30ed\u30e1\u30fc\u30c8\u30eb\n   \u6642\u901f60\u30ad\u30ed\u30e1\u30fc\u30c8\u30eb, \u56de\u7b54:: \u6642\u901f50\u30ad\u30ed\u30e1\u30fc\u30c8\u30eb\n  \u8cea\u554f:: 2024\u5e74\u73fe\u5728\u3001\u30c9\u30ed\u30fc\u30f3\u8996\u70b9\u306e\u30ab\u30e1\u30e9\u6620\u50cf\u304c\u4f7f\u7528\u3067\u304d\u308bFPV\u30c9\u30ed\u30fc\u30f3(5.8GHz)\u306e\u64cd\u4f5c\u304c\u51fa\u6765\u306a\u3044\u8cc7\u683c\u306f\u3069\u308c, \u9078\u629e\u80a2::\n   \u7b2c2\u7d1a\u9678\u4e0a\u7279\u6b8a\u7121\u7dda\u6280\u58eb\n   \u7b2c3\u7d1a\u30a2\u30de\u30c1\u30e5\u30a2\u7121\u7dda\u6280\u58eb\n   \u7b2c2\u7d1a\u30a2\u30de\u30c1\u30e5\u30a2\u7121\u7dda\u6280\u58eb\n   \u7b2c1\u7d1a\u6d77\u4e0a\u7279\u6b8a\u7121\u7dda\u6280\u58eb, \u56de\u7b54:: \u7b2c1\u7d1a\u6d77\u4e0a\u7279\u6b8a\u7121\u7dda\u6280\u58eb\n  \u8cea\u554f:: \u9053\u8def\u4e0a\u306b\u63cf\u304b\u308c\u3066\u3044\u308b\u3072\u3057\u5f62\u306e\u30de\u30fc\u30af\u306e\u540d\u79f0\u306f\u3069\u308c?, \u9078\u629e\u80a2::\n   \u6a2a\u65ad\u6b69\u9053\u6ce8\u610f\u30de\u30fc\u30af\n   \u30c0\u30a4\u30e4\u30de\u30fc\u30af\n   \u81ea\u8ee2\u8eca\u6a2a\u65ad\u5e2f\u30de\u30fc\u30af\n   \u4e00\u6642\u505c\u6b62\u30de\u30fc\u30af, \u56de\u7b54:: \u30c0\u30a4\u30e4\u30de\u30fc\u30af\n  \u8cea\u554f:: \u9663\u4e2d\u898b\u821e\u3044\u3067\u7981\u6b62\u3055\u308c\u3066\u3044\u308b\u3082\u306e\u306f\u3069\u308c, \u9078\u629e\u80a2::\n   \u91d1\u92ad\u306e\u5bc4\u4ed8\n   \u304a\u83d3\u5b50\u30fb\u679c\u7269\n   \u5f01\u5f53\u30fb\u9152\u30fb\u30b8\u30e5\u30fc\u30b9\n   \u6709\u4fa1\u8a3c\u5238\u306e\u5bc4\u4ed8, \u56de\u7b54:: \u5f01\u5f53\u30fb\u9152\u30fb\u30b8\u30e5\u30fc\u30b9\n||| + template_blank;\n\nlocal template_healthcare = |||\n  \u4ee5\u4e0b\u306f\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3068\u3001\u8ffd\u52a0\u306e\u80cc\u666f\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u5165\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u8981\u6c42\u3092\u9069\u5207\u306b\u6e80\u305f\u3059\u56de\u7b54\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n  \u6307\u793a:: \u8cea\u554f\u3068\u56de\u7b54\u306e\u9078\u629e\u80a2\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308a\u3001\u9078\u629e\u80a2\u304b\u3089\u56de\u7b54\u3092\u9078\u629e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u4ed6\u306b\u306f\u4f55\u3082\u542b\u3081\u306a\u3044\u3053\u3068\u3092\u53b3\u5b88\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n  \u8cea\u554f:: 2024\u5e74\u73fe\u5728\u3001\u7279\u5b9a\u75be\u60a3\u7642\u990a\u7ba1\u7406\u6599\u306e\u7b97\u5b9a\u5bfe\u8c61\u5916\u3068\u306a\u308b\u75be\u60a3\u306f\u3069\u308c?, \u9078\u629e\u80a2::\n   \u80c3\u6f70\u760d\n   \u9ad8\u8840\u5727\u75c7\n   \u7d50\u6838\n   \u5598\u606f, \u56de\u7b54:: \u9ad8\u8840\u5727\u75c7\n  \u8cea\u554f:: 2024\u5e74\u73fe\u5728\u3001\u30de\u30a4\u30ca\u4fdd\u967a\u8a3c\u3092\u5229\u7528\u3057\u305f\u5834\u5408\u521d\u8a3a\u6599\u306f\u3044\u304f\u3089\u304b\u9078\u3079, \u9078\u629e\u80a2::\n   18\u5186\n   9\u5186\n   3\u5186\n   6\u5186, \u56de\u7b54:: 6\u5186\n  \u8cea\u554f:: 2024\u5e74\u73fe\u5728\u3001\u51e6\u65b9\u7b8b\u306e\u4f7f\u7528\u671f\u9593\u306f\u3001\u4ea4\u4ed8\u306e\u65e5\u3092\u542b\u3081\u3066\u4f55\u65e5\u4ee5\u5185\u304b\u9078\u3079, \u9078\u629e\u80a2::\n   4\u65e5\u4ee5\u5185\n   3\u65e5\u4ee5\u5185\n   \u5f53\u65e5\u4ee5\u5185\n   1\u9031\u9593\u4ee5\u5185, \u56de\u7b54:: 4\u65e5\u4ee5\u5185\n  \u8cea\u554f:: \u8a2a\u65e5\u5916\u56fd\u4eba\u65c5\u884c\u8005\u306e\u533b\u7642\u8cbb\u306f\u3001\u4e00\u822c\u7684\u306b\u3069\u306e\u3088\u3046\u306a\u6271\u3044\u306b\u306a\u308b\u304b\u9078\u3079, \u9078\u629e\u80a2::\n   \u751f\u6d3b\u4fdd\u8b77\u306b\u3088\u308b\u8a3a\u7642\n   \u7121\u6599\u8a3a\u7642\n   \u81ea\u8cbb\u8a3a\u7642\n   \u516c\u7684\u533b\u7642\u4fdd\u967a\u8a3a\u7642, \u56de\u7b54:: \u81ea\u8cbb\u8a3a\u7642\n||| + template_blank;\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args,\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: '{% if category == \"culture\" %}'+ template_culture + '{% elif category == \"custom\" %}'+ template_custom + '{% elif category == \"regional_identity\" %}'+ template_regional_identity + '{% elif category == \"geography\" %}'+ template_geography + '{% elif category == \"history\" %}'+ template_history + '{% elif category == \"government\" %}'+ template_government + '{% elif category == \"law\" %}'+ template_law + '{% elif category == \"healthcare\" %}' + template_healthcare + '{% endif %}',\n      },\n    },\n    metrics: [\n      { class_path: 'ExactMatch',\n        init_args: {\n          lm_output_processor: [ \n            { class_path: 'NFKCNormalizer'},\n            { class_path: 'StringStrip', },\n          ],\n          reference_processor: [ \n            { class_path: 'NFKCNormalizer'},\n            { class_path: 'StringStrip', },\n          ],\n        },\n      },\n    ],\n    gen_kwargs: { max_new_tokens: 128, stop_sequences: ['\\n'], },\n    batch_size: 1,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/#jcommonsenseqa","title":"jcommonsenseqa","text":"<p>JCommonsenseQA is a Japanese version of CommonsenseQA, which is a multiple-choice question answering dataset that requires commonsense reasoning ability. The dataset is built using crowdsourcing with seeds extracted from the knowledge base ConceptNet. This is a setup for generating answers based on the choices provided.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Original Repository</li> <li>JGLUE: Japanese General Language Understanding Evaluation</li> <li>JGLUE: \u65e5\u672c\u8a9e\u8a00\u8a9e\u7406\u89e3\u30d9\u30f3\u30c1\u30de\u30fc\u30af <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'sbintuitions/JCommonsenseQA',\n    split: 'validation',\n    reference_template: '{% set choices = [choice0, choice1, choice2, choice3, choice4] %}{{ choices[label] }}',\n  },\n};\n\nlocal template_ = |||\n  \u4ee5\u4e0b\u306f\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3068\u3001\u8ffd\u52a0\u306e\u80cc\u666f\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u5165\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u8981\u6c42\u3092\u9069\u5207\u306b\u6e80\u305f\u3059\u56de\u7b54\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n  ### \u6307\u793a\n  \u8cea\u554f\u3068\u56de\u7b54\u306e\u9078\u629e\u80a2\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308a\u3001\u9078\u629e\u80a2\u304b\u3089\u56de\u7b54\u3092\u9078\u629e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u4ed6\u306b\u306f\u4f55\u3082\u542b\u3081\u306a\u3044\u3053\u3068\u3092\u53b3\u5b88\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n  ### \u5165\u529b\uff1a\n  \u8cea\u554f\uff1a\u4e3b\u306b\u5b50\u3069\u3082\u5411\u3051\u306e\u3082\u306e\u3067\u3001\u30a4\u30e9\u30b9\u30c8\u306e\u3064\u3044\u305f\u7269\u8a9e\u304c\u66f8\u304b\u308c\u3066\u3044\u308b\u3082\u306e\u306f\u3069\u308c\uff1f\n  \u9078\u629e\u80a2\uff1a\u4e16\u754c,\u5199\u771f\u96c6,\u7d75\u672c,\u8ad6\u6587,\u56f3\u9451\n  ### \u56de\u7b54\uff1a\n  \u7d75\u672c\n\n  ### \u5165\u529b\uff1a\n  \u8cea\u554f\uff1a\u672a\u6210\u5e74\u8005\u3092\u76e3\u8b77\u30fb\u6559\u80b2\u3057\uff0c\u5f7c\u3089\u3092\u76e3\u7763\u3057\uff0c\u5f7c\u3089\u306e\u8ca1\u7523\u4e0a\u306e\u5229\u76ca\u3092\u5b88\u308b\u6cd5\u5f8b\u4e0a\u306e\u7fa9\u52d9\u3092\u3082\u3064\u4eba\u306f\uff1f\n  \u9078\u629e\u80a2\uff1a\u6d6e\u6d6a\u8005,\u4fdd\u8b77\u8005,\u304a\u574a\u3055\u3093,\u5b97\u6559\u8005,\u9810\u8a00\u8005\n  ### \u56de\u7b54\uff1a\n  \u4fdd\u8b77\u8005\n\n  ### \u5165\u529b\uff1a\n  \u8cea\u554f\uff1a\u6570\u5b57\u306e\uff11\u3092\u8868\u3059\u3068\u304d\u306b\u4f7f\u3046\u4f53\u306f\uff1f\n  \u9078\u629e\u80a2\uff1a\u80f8,\u8089\u7403,\u80cc\u4e2d,\u4eba\u5dee\u3057\u6307,\u89aa\u6307\n  ### \u56de\u7b54\uff1a\n  \u4eba\u5dee\u3057\u6307\n\n  ### \u5165\u529b\uff1a\n  \u8cea\u554f\uff1a\u706b\u3092\u8d77\u3053\u3059\u3068\u3042\u3089\u308f\u308c\u308b\u3082\u304f\u3082\u304f\u3059\u308b\u3082\u306e\u306f\uff1f\n  \u9078\u629e\u80a2\uff1a\u6b6f\u306e\u5909\u8272,\u30ac\u30b9,\u4e2d\u6bd2,\u7206\u767a,\u7159\n  ### \u56de\u7b54\uff1a\n  \u7159\n\n  ### \u5165\u529b\uff1a\n  \u8cea\u554f\uff1a{{ question }}\n  \u9078\u629e\u80a2\uff1a{{ choice0 }},{{ choice1 }},{{ choice2 }},{{ choice3 }},{{ choice4 }}\n  ### \u56de\u7b54\uff1a\n|||;\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args,\n    prompt_template: template_,\n    metrics: [\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 64, stop_sequences: ['\\n\\n'] },\n    batch_size: 1,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/#jnli","title":"jnli","text":"<p>JNLI is a Japanese version of the NLI (Natural Language Inference) dataset. The sentence pairs are extracted from image captions and annotated by crowd workers.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Original Repository</li> <li>JGLUE: Japanese General Language Understanding Evaluation</li> <li>JGLUE: \u65e5\u672c\u8a9e\u8a00\u8a9e\u7406\u89e3\u30d9\u30f3\u30c1\u30de\u30fc\u30af <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'llm-book/JGLUE',\n    subset: 'JNLI',\n    reference_template: \"{{ ['\\\"\u542b\u610f\\\"', '\\\"\u77db\u76fe\\\"', '\\\"\u4e2d\u7acb\\\"'][label] }}\",\n    dataset_kwargs: { trust_remote_code: true },\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'BalancedFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 3,\n      },\n    },\n    prompt_template: |||\n      \u524d\u63d0\u3068\u4eee\u8aac\u306e\u95a2\u4fc2\u3092\u300c\u4e2d\u7acb\u300d\u3001\u300c\u542b\u610f\u300d\u3001\u300c\u77db\u76fe\u300d\u306e\u4e2d\u304b\u3089\u56de\u7b54\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n      {% for item in few_shot_data %}\n      \u524d\u63d0\uff1a\u300c{{ item.sentence1 }}\u300d\n      \u4eee\u8aac\uff1a\u300c{{ item.sentence2 }}\u300d\n      \u95a2\u4fc2\uff1a\u300c{{ item.references[0] }}\u300d\n      {% endfor %}\n      \u524d\u63d0\uff1a\u300c{{ sentence1 }}\u300d\n      \u4eee\u8aac\uff1a\u300c{{ sentence2 }}\u300d\n    ||| + '\u95a2\u4fc2\uff1a\u300c',\n    metrics: [\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 6, stop_sequences: ['\u524d\u63d0', '\u300d'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/#jsquad","title":"jsquad","text":"<p>JSQuAD is a Japanese version of SQuAD, one of the datasets of reading comprehension. The passages are extracted from Japanese Wikipedia, and the questions and answers are created by crowd workers.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Original Repository</li> <li>JGLUE: Japanese General Language Understanding Evaluation</li> <li>JGLUE: \u65e5\u672c\u8a9e\u8a00\u8a9e\u7406\u89e3\u30d9\u30f3\u30c1\u30de\u30fc\u30af <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'sbintuitions/JSQuAD',\n    split: 'validation',\n    reference_list_template: '{{ answers.text }}',\n  },\n};\n\nlocal template_ = |||\n  \u4ee5\u4e0b\u306f\u30bf\u30b9\u30af\u3092\u8aac\u660e\u3059\u308b\u6307\u793a\u3068\u3001\u8ffd\u52a0\u306e\u80cc\u666f\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u5165\u529b\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u8981\u6c42\u3092\u9069\u5207\u306b\u6e80\u305f\u3059\u56de\u7b54\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\u3002\n  ### \u6307\u793a\n  \u8cea\u554f\u306b\u5bfe\u3059\u308b\u56de\u7b54\u3092\u6587\u7ae0\u304b\u3089\u4e00\u8a00\u3067\u62bd\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306f\u540d\u8a5e\u3067\u7b54\u3048\u3066\u304f\u3060\u3055\u3044\u3002 \u305d\u308c\u4ee5\u5916\u306b\u306f\u4f55\u3082\u542b\u3081\u306a\u3044\u3053\u3068\u3092\u53b3\u5b88\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n  ### \u5165\u529b\uff1a\n  \u6587\u7ae0\uff1a\u8056\u6b66\u5929\u7687 [SEP] \u6587\u6b66\u5929\u7687\u306e\u7b2c\u4e00\u7687\u5b50\u3068\u3057\u3066\u751f\u307e\u308c\u305f\u304c\u3001\u6176\u96f24\u5e746\u670815\u65e5\uff08707\u5e747\u670818\u65e5\uff09\u306b7\u6b73\u3067\u7236\u3068\u6b7b\u5225\u3001\u6bcd\u30fb\u5bae\u5b50\u3082\u5fc3\u7684\u969c\u5bb3\u306b\u9665\u3063\u305f\u305f\u3081\u3001\u305d\u306e\u5f8c\u306f\u9577\u3089\u304f\u4f1a\u3046\u3053\u3068\u306f\u306a\u304b\u3063\u305f\u3002\u7269\u5fc3\u304c\u3064\u3044\u3066\u4ee5\u5f8c\u306e\u5929\u7687\u304c\u75c5\u6c17\u306e\u5e73\u7652\u3057\u305f\u6bcd\u3068\u306e\u5bfe\u9762\u3092\u679c\u305f\u3057\u305f\u306e\u306f\u9f6237\u306e\u3068\u304d\u3067\u3042\u3063\u305f\u3002\u3053\u306e\u305f\u3081\u3001\u540c\u5e747\u670817\u65e5\uff08707\u5e748\u670818\u65e5\uff09\u3001\u7236\u65b9\u306e\u7956\u6bcd\u30fb\u5143\u660e\u5929\u7687\uff08\u5929\u667a\u5929\u7687\u7687\u5973\uff09\u304c\u4e2d\u7d99\u304e\u306e\u5929\u7687\u3068\u3057\u3066\u5373\u4f4d\u3057\u305f\u3002\u548c\u92857\u5e746\u670825\u65e5\uff08714\u5e748\u67089\u65e5\uff09\u306b\u306f\u9996\u7687\u5b50\u306e\u5143\u670d\u304c\u884c\u308f\u308c\u3066\u540c\u65e5\u6b63\u5f0f\u306b\u7acb\u592a\u5b50\u3055\u308c\u308b\u3082\u3001\u75c5\u5f31\u3067\u3042\u3063\u305f\u3053\u3068\u3001\u7687\u89aa\u52e2\u529b\u3068\u5916\u621a\u3067\u3042\u308b\u85e4\u539f\u6c0f\u3068\u306e\u5bfe\u7acb\u3082\u3042\u308a\u3001\u5373\u4f4d\u306f\u5148\u5ef6\u3070\u3057\u306b\u3055\u308c\u3001\u7fcc\u970a\u4e80\u5143\u5e749\u67082\u65e5\uff08715\u5e7410\u67083\u65e5\uff09\u306b\u4f2f\u6bcd\uff08\u6587\u6b66\u5929\u7687\u306e\u59c9\uff09\u30fb\u5143\u6b63\u5929\u7687\u304c\u300c\u4e2d\u7d99\u304e\u306e\u4e2d\u7d99\u304e\u300d\u3068\u3057\u3066\u7687\u4f4d\u3092\u7d99\u3050\u3053\u3068\u306b\u306a\u3063\u305f\u300224\u6b73\u306e\u3068\u304d\u306b\u5143\u6b63\u5929\u7687\u3088\u308a\u7687\u4f4d\u3092\u8b72\u3089\u308c\u3066\u5373\u4f4d\u3059\u308b\u3053\u3068\u306b\u306a\u308b\u3002\n  \u8cea\u554f\uff1a\u6587\u6b66\u5929\u7687\u306e\u7b2c\u4e00\u7687\u5b50\u3068\u3057\u3066\u751f\u307e\u308c\u305f\u306e\u306f\uff1f\n  ### \u56de\u7b54\uff1a\n  \u8056\u6b66\u5929\u7687\n\n  ### \u5165\u529b\uff1a\n  \u6587\u7ae0\uff1a\u901a\u79f0 [SEP] \u4eba\u540d\u3068\u3057\u3066\u306e\u901a\u79f0\u306f\u901a\u308a\u540d\u3001\u4e8c\u3064\u540d\u3001\u7570\u540d\u3001\u306a\u3069\u3068\u547c\u3070\u308c\u308b\u4e8b\u3082\u3042\u308b\u3002\u8fd1\u4e16\u307e\u3067\u306f\u3001\u672c\u540d\uff08\u5b9f\u540d\uff09\u306f\u300c\u300d\u3068\u547c\u3070\u308c\u3001\u516c\u8a00\u306f\u907f\u3051\u308b\u7fd2\u6163\u304c\u3042\u3063\u305f\u3002\u305d\u306e\u305f\u3081\u3001\u4eba\u3092\u547c\u3076\u6642\u306f\u300c\u4eee\u540d\u300d\u300c\u5b57\u300d\u306a\u3069\u306e\u901a\u79f0\u3001\u5b98\u8077\u540d\u3092\u7528\u3044\u308b\u306e\u304c\u4e00\u822c\u7684\u3060\u3063\u305f\u3002\u4eca\u65e5\u3067\u3082\u300c\u7dcf\u7406\u300d\u300c\u5927\u81e3\u300d\u300c\u793e\u9577\u300d\u300c\u5c02\u52d9\u300d\u306a\u3069\u3068\u547c\u3073\u304b\u3051\u306b\u4f7f\u3046\u306e\u304c\u3053\u308c\u306b\u3042\u305f\u308b\u3002\n  \u8cea\u554f\uff1a\u4eba\u540d\u3068\u3057\u3066\u306e\u901a\u79f0\u306f\u4f55\u3068\u547c\u3070\u308c\u3066\u3044\u308b\u304b\n  ### \u56de\u7b54\uff1a\n  \u901a\u308a\u540d\u3001\u4e8c\u3064\u540d\u3001\u7570\u540d\n\n  ### \u5165\u529b\uff1a\n  \u6587\u7ae0\uff1a\u5742\u672c\u9f8d\u4e00 [SEP] 2014\u5e747\u670810\u65e5\u3001\u6240\u5c5e\u4e8b\u52d9\u6240\u30a8\u30a4\u30d9\u30c3\u30af\u30b9\u30fb\u30df\u30e5\u30fc\u30b8\u30c3\u30af\u30fb\u30af\u30ea\u30a8\u30a4\u30c6\u30a3\u30f4\u304b\u3089\u4e2d\u54bd\u982d\u764c\u3067\u3042\u308b\u3053\u3068\u3001\u7642\u990a\u306b\u5c02\u5ff5\u3059\u308b\u305f\u3081\u306b\u30b3\u30f3\u30b5\u30fc\u30c8\u6d3b\u52d5\u306a\u3069\u3092\u4e2d\u6b62\u3059\u308b\u65e8\u304c\u767a\u8868\u3055\u308c\u305f\u3002\u304b\u3064\u3066\u306f\u30a4\u30f3\u30bf\u30d3\u30e5\u30fc\u306a\u3069\u3067\u5ea6\u3005\u81ea\u8eab\u306e\u5065\u5eb7\u72b6\u614b\u3084\u4f53\u529b\u306b\u81ea\u4fe1\u3092\u8868\u3057\u3066\u304a\u308a\u3001\u30b3\u30f3\u30b5\u30fc\u30c8\u7b49\u516c\u6f14\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3092\u81ea\u8eab\u306e\u5065\u5eb7\u306b\u8d77\u56e0\u3059\u308b\u7406\u7531\u3067\u30ad\u30e3\u30f3\u30bb\u30eb\u3057\u305f\u3053\u3068\u304c\u306a\u304b\u3063\u305f\u3002\n  \u8cea\u554f\uff1a\u5742\u672c\u9f8d\u4e00\u304c\u7642\u990a\u306b\u5c02\u5ff5\u3059\u308b\u305f\u3081\u30b3\u30f3\u30b5\u30fc\u30c8\u6d3b\u52d5\u306a\u3069\u3092\u4e2d\u6b62\u3059\u308b\u3068\u767a\u8868\u3057\u305f\u306e\u306f\u3044\u3064\u304b\u3002\n  ### \u56de\u7b54\uff1a\n  2014\u5e747\u670810\u65e5\n\n  ### \u5165\u529b\uff1a\n  \u6587\u7ae0\uff1a\u30ea\u30ea\u30fc\u30d5 [SEP] \u30d7\u30ec\u30c3\u30b7\u30e3\u30fc\u306e\u6bd4\u8f03\u7684\u304b\u304b\u3089\u306a\u3044\u72b6\u614b\u3067\u6295\u3052\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3001\u82e5\u624b\u6295\u624b\u306e\u30c6\u30b9\u30c8\u306e\u5834\u3068\u3057\u305f\u308a\u3001\u6545\u969c\u660e\u3051\u3084\u767b\u677f\u9593\u9694\u306e\u958b\u3044\u305f\u6295\u624b\u3092\u8abf\u6574\u76ee\u7684\u3067\u767b\u677f\u3055\u305b\u308b\u3053\u3068\u3082\u3042\u308b\u3002\u6557\u6226\u51e6\u7406\u3067\u3042\u3063\u3066\u3082\u597d\u6295\u3059\u308c\u3070\u6b21\u56de\u304b\u3089\u5148\u767a\u3084\u63a5\u6226\u3067\u306e\u4e2d\u7d99\u304e\u306b\u8d77\u7528\u3055\u308c\u308b\u3088\u3046\u306b\u306a\u308b\u5834\u5408\u3082\u3042\u308a\u3001\u5e78\u3044\u6253\u7dda\u306e\u63f4\u8b77\u3092\u53d7\u3051\u3066\u30c1\u30fc\u30e0\u304c\u9006\u8ee2\u3059\u308c\u3070\u52dd\u5229\u6295\u624b\u306b\u8f1d\u304f\u3053\u3068\u3082\u3042\u308b\u3002\n  \u8cea\u554f\uff1a\u6253\u7dda\u306e\u63f4\u8b77\u3092\u53d7\u3051\u3066\u30c1\u30fc\u30e0\u304c\u9006\u8ee2\u3059\u308b\u3068\u3069\u3093\u306a\u6295\u624b\u306b\u306a\u308b\uff1f\n  ### \u56de\u7b54\uff1a\n  \u52dd\u5229\u6295\u624b\n\n  ### \u5165\u529b\uff1a\n  \u6587\u7ae0\uff1a{{ context }}\n  \u8cea\u554f\uff1a{{ question }}\n  ### \u56de\u7b54\uff1a\n|||;\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args,\n    prompt_template: template_,\n    metrics: [\n      { class_path: 'CharF1' },\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 64, stop_sequences: ['\\n\\n'] },\n    batch_size: 1,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/#mgsm_ja","title":"mgsm_ja","text":"<p>Multilingual Grade School Math Benchmark (MGSM) is a benchmark of grade-school math problems. This is a Japanese subset of the benchmark.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Language Models are Multilingual Chain-of-Thought Reasoners <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'juletxara/mgsm',\n    subset: 'ja',\n    reference_template: '{{ answer_number }}',\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'test' } },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 4,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      {{ item.question }}\n      {{ item.answer }}\n      {% endfor %}\n      \u554f\u984c: {{ question }}\n    ||| + '\u30b9\u30c6\u30c3\u30d7\u3054\u3068\u306e\u7b54\u3048:',\n    metrics: [\n      { class_path: 'ExactMatch', init_args: { lm_output_processor: { class_path: 'RegexExtractor', init_args: { pattern: '-?[0-9.,]+' } } } },\n    ],\n    gen_kwargs: { max_new_tokens: 256, stop_sequences: ['\u554f\u984c:'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/#wrime_pos_neg","title":"wrime_pos_neg","text":"<p>WRIME (dataset of Writers\u2019 and Readers\u2019 Intensities of eMotion for their Estimation) is constructed by annotating Internet posts with both the writer\u2019s subjective emotional intensity and the reader\u2019s objective one. This setup converts the original dataset into binary sentiment classification.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Original Repository</li> <li>WRIME: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations</li> <li>A Japanese Dataset for Subjective and Objective Sentiment Polarity Classification in Micro Blog Domain <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'llm-book/wrime-sentiment',\n    reference_template: \"{{ ['\\\"\u30dd\u30b8\u30c6\u30a3\u30d6\\\"', '\\\"\u30cd\u30ac\u30c6\u30a3\u30d6\\\"'][label] }}\",\n    dataset_kwargs: { trust_remote_code: true },\n  },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'BalancedFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 4,\n      },\n    },\n    prompt_template: |||\n      \u6587\u306e\u6975\u6027\u306b\u3064\u3044\u3066\u300c\u30dd\u30b8\u30c6\u30a3\u30d6\u300d\u304b\u300c\u30cd\u30ac\u30c6\u30a3\u30d6\u300d\u304b\u3067\u7b54\u3048\u3066\u304f\u3060\u3055\u3044\u3002\n      {% for item in few_shot_data %}\n      \u6587\uff1a{{ item.sentence }}\n      \u6975\u6027\uff1a\u300c{{ item.references[0] }}\u300d\n      {% endfor %}\n      \u6587\uff1a{{sentence}}\n    ||| + '\u6975\u6027\uff1a\u300c',\n    metrics: [\n      { class_path: 'ExactMatch' },\n    ],\n    gen_kwargs: { max_new_tokens: 8, stop_sequences: ['\u300d'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_generation/#xlsum_ja","title":"xlsum_ja","text":"<p>XLSum is a comprehensive and diverse dataset comprising 1.35 million professionally annotated article-summary pairs from BBC, extracted using a set of carefully designed heuristics. This is a Japanese subset of the dataset.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Original Repository</li> <li>XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages <pre><code>local dataset_base_args = {\n  class_path: 'HFGenerationDataset',\n  init_args: {\n    path: 'csebuetnlp/xlsum',\n    subset: 'japanese',\n    reference_template: '{{ summary }}',\n  },\n};\n\n{\n  // as we deal with LLMs with short context window, we set max_text_length and max_summary_length\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset_base_args { init_args+: { split: 'validation' } },\n    few_shot_generator: {\n      class_path: 'BalancedFewShotGenerator',\n      init_args: {\n        dataset: dataset_base_args { init_args+: { split: 'train' } },\n        num_shots: 1,\n      },\n    },\n    prompt_template: |||\n      \u6587\u7ae0\u3092\uff11\u301c\uff13\u6587\u3067\u8981\u7d04\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n      {% for item in few_shot_data %}\n      \u6587\u7ae0: {{ item.text }}\n      \u8981\u7d04: {{ item.references[0] }}\n      {% endfor %}\n      \u6587\u7ae0: {{ text }}\n    ||| + '\u8981\u7d04:',\n    metrics: [\n      {\n        class_path: 'ROUGE',\n        init_args: { tokenizer: { class_path: 'SacreBleuTokenizer', init_args: { name: 'ja-mecab' } } },\n      },\n    ],\n    gen_kwargs: { max_new_tokens: 100, stop_sequences: ['\\n'] },\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_multiple_choice/","title":"Ja multiple choice","text":""},{"location":"preset_configs/EvalSetup/ja_multiple_choice/#jcommonsenseqa_mc","title":"jcommonsenseqa_mc","text":"<p>JCommonsenseQA is a Japanese version of CommonsenseQA, which is a multiple-choice question answering dataset that requires commonsense reasoning ability. The dataset is built using crowdsourcing with seeds extracted from the knowledge base ConceptNet. This is a setup for multiple choice where the model chooses the correct answer based on the log-probabilities of the choices.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>Original Repository</li> <li>JGLUE: Japanese General Language Understanding Evaluation</li> <li>JGLUE: \u65e5\u672c\u8a9e\u8a00\u8a9e\u7406\u89e3\u30d9\u30f3\u30c1\u30de\u30fc\u30af <pre><code>local dataset_base_args = {\n  path: 'llm-book/JGLUE',\n  subset: 'JCommonsenseQA',\n  choices_templates: ['{{ choice0 }}', '{{ choice1 }}', '{{ choice2 }}', '{{ choice3 }}', '{{ choice4 }}'],\n  answer_index_template: '{{ label }}',\n};\n\n{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: dataset_base_args { split: 'validation' },\n    },\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        dataset: {\n          class_path: 'HFMultipleChoiceDataset',\n          init_args: dataset_base_args { split: 'train' },\n        },\n        num_shots: 0,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      \u554f\u984c\uff1a{{ item.question }}\n      \u56de\u7b54\uff1a\u300c{{ item.choices[item.answer_index] }}\u300d\n      {% endfor %}\n      \u554f\u984c\uff1a{{question}}\n    ||| + '\u56de\u7b54\uff1a\u300c',\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/ja_multiple_choice/#xwinograd_ja","title":"xwinograd_ja","text":"<p>XWinograd is a multilingual collection of Winograd Schemas in six languages that can be used for evaluation of cross-lingual commonsense reasoning capabilities. This is an Japanese subset of the dataset.</p> <p>References:</p> <ul> <li>Hugging Face Dataset</li> <li>It\u2019s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning <pre><code>{\n  class_path: 'MultipleChoice',\n  init_args: {\n    eval_dataset: {\n      class_path: 'HFMultipleChoiceDataset',\n      init_args: {\n        path: 'Muennighoff/xwinograd',\n        subset: 'jp',\n        split: 'test',\n        choices_templates: [\n          '{{ option1 }}{{ sentence.split(\"_\")[1] }}',\n          '{{ option2 }}{{ sentence.split(\"_\")[1] }}',\n        ],\n        answer_index_template: '{{ answer | int - 1 }}',\n        input_templates: { context: '{{ sentence.split(\"_\")[0] }}' },\n      },\n    },\n    prompt_template: '{{ context }}',\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/translation/","title":"Translation","text":""},{"location":"preset_configs/EvalSetup/translation/#wmt20_en_ja","title":"wmt20_en_ja","text":"<p>This dataset is created as a test set for the WMT20 shared task on news translation. This is English to Japanese translation.</p> <p>References:</p> <ul> <li>Data Source</li> <li>2020 Fifth Conference on Machine Translation (WMT20) <pre><code>local dataset = {\n  class_path: 'SacreBleuDataset',\n  init_args: { name: 'wmt20', langpair: 'en-ja' },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset,\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        // Use the eval dataset for few-shot data,\n        // but `RandomFewShotGenerator` will avoid using the same few-shot isntances as the input.\n        dataset: dataset,\n        num_shots: 4,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      En: `{{ item.source }}`\n      Ja: `{{ item.references[0] }}`\n      {% endfor %}\n      En: `{{ source }}`\n    ||| + 'Ja: `',\n    metrics: [\n      { class_path: 'BLEU', init_args: { tokenize_option: 'ja-mecab' } },\n    ],\n    gen_kwargs: { max_new_tokens: 128, stop_sequences: ['`'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/translation/#wmt20_ja_en","title":"wmt20_ja_en","text":"<p>This dataset is created as a test set for the WMT20 shared task on news translation. This is Japanese to English translation.</p> <p>References:</p> <ul> <li>Data Source</li> <li>2020 Fifth Conference on Machine Translation (WMT20) <pre><code>local dataset = {\n  class_path: 'SacreBleuDataset',\n  init_args: { name: 'wmt20', langpair: 'ja-en' },\n};\n\n{\n  class_path: 'Generation',\n  init_args: {\n    eval_dataset: dataset,\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        // Use the eval dataset for few-shot data,\n        // but `RandomFewShotGenerator` will avoid using the same few-shot isntances as the input.\n        dataset: dataset,\n        num_shots: 4,\n      },\n    },\n    prompt_template: |||\n      {% for item in few_shot_data %}\n      Ja: `{{ item.source }}`\n      En: `{{ item.references[0] }}`\n      {% endfor %}\n      Ja: `{{ source }}`\n    ||| + 'En: `',\n    metrics: [\n      { class_path: 'BLEU', init_args: { tokenize_option: 'intl' } },\n    ],\n    gen_kwargs: { max_new_tokens: 128, stop_sequences: ['`'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/translation_chat/","title":"Translation chat","text":""},{"location":"preset_configs/EvalSetup/translation_chat/#wmt20_en_ja_chat","title":"wmt20_en_ja_chat","text":"<p>This dataset is created as a test set for the WMT20 shared task on news translation. This is English to Japanese translation. This is a evaluation setup for chat LLMs.</p> <p>References:</p> <ul> <li>Data Source</li> <li>2020 Fifth Conference on Machine Translation (WMT20) <pre><code>local dataset = {\n  class_path: 'SacreBleuChatDataset',\n  init_args: { name: 'wmt20', langpair: 'en-ja' },\n};\n\n{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: dataset,\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        // Use the eval dataset for few-shot data,\n        // but `RandomFewShotGenerator` will avoid using the same few-shot isntances as the input.\n        dataset: dataset,\n        num_shots: 4,\n      },\n    },\n    metrics: [\n      { class_path: 'BLEU', init_args: { tokenize_option: 'ja-mecab' } },\n    ],\n    gen_kwargs: { max_new_tokens: 128, stop_sequences: ['`'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/EvalSetup/translation_chat/#wmt20_ja_en_chat","title":"wmt20_ja_en_chat","text":"<p>This dataset is created as a test set for the WMT20 shared task on news translation. This is Japanese to English translation. This is a evaluation setup for chat LLMs.</p> <p>References:</p> <ul> <li>Data Source</li> <li>2020 Fifth Conference on Machine Translation (WMT20) <pre><code>local dataset = {\n  class_path: 'SacreBleuChatDataset',\n  init_args: { name: 'wmt20', langpair: 'ja-en' },\n};\n\n{\n  class_path: 'ChatResponse',\n  init_args: {\n    eval_dataset: dataset,\n    few_shot_generator: {\n      class_path: 'RandomFewShotGenerator',\n      init_args: {\n        // Use the eval dataset for few-shot data,\n        // but `RandomFewShotGenerator` will avoid using the same few-shot isntances as the input.\n        dataset: dataset,\n        num_shots: 4,\n      },\n    },\n    metrics: [\n      { class_path: 'BLEU', init_args: { tokenize_option: 'intl' } },\n    ],\n    gen_kwargs: { max_new_tokens: 128, stop_sequences: ['`'] },\n    batch_size: 4,\n  },\n}\n</code></pre></li> </ul>"},{"location":"preset_configs/Metric/","title":"Metric","text":""},{"location":"preset_configs/Metric/#assistant_eval_en_single_turn","title":"assistant_eval_en_single_turn","text":"<p>This is a configuration for evaluting the quality of responses generated by an AI assistant. Originally used to generate scores for MT-bench or Vicuna-bench.</p> <p>Adapted from lm-sys/FastChat. <pre><code>{\n  class_path: 'ChatLLMScore',\n  init_args: {\n    language_model: { class_path: 'OpenAIChatAPI', init_args: { model: 'gpt-4-turbo-2024-04-09' } },\n    valid_score_range: [1, 10],\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: std.stripChars(|||\n          [Instruction]\n          {% if references|length &gt; 0 -%}\n          Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant's answer. Begin your evaluation by comparing the assistant's answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n          {%- else -%}\n          Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".\n          {%- endif %}\n\n          [Question]\n          {{ messages[0][\"content\"] }}\n\n          {% if references|length &gt; 0 -%}\n          [The Start of Reference Answer]\n          {{ references[0] }}\n          [The End of Reference Answer]\n          {% endif -%}\n          [The Start of Assistant's Answer]\n          {% if messages|length == 1 %}{{ lm_output }}{% else %}{{ messages[1][\"content\"] }}{% endif %}\n          [The End of Assistant's Answer]\n        |||, '\\n'),\n      },\n    },\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/Metric/#assistant_eval_ja_single_turn","title":"assistant_eval_ja_single_turn","text":"<p>This is a configuration for evaluting the quality of responses generated by an AI assistant. Originally used to generate scores for the Japanese versions of MT-bench or Vicuna-bench.</p> <p>Translated and adapted from lm-sys/FastChat. <pre><code>{\n  class_path: 'ChatLLMScore',\n  init_args: {\n    language_model: { class_path: 'OpenAIChatAPI', init_args: { model: 'gpt-4-turbo-2024-04-09' } },\n    valid_score_range: [1, 10],\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: std.stripChars(|||\n          [\u6307\u793a]\n          {% if references|length &gt; 0 -%}\n          \u4ee5\u4e0b\u306b\u8868\u793a\u3055\u308c\u308b\u30e6\u30fc\u30b6\u306e\u8cea\u554f\u306b\u5bfe\u3059\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u5fdc\u7b54\u306e\u54c1\u8cea\u3092\u8a55\u4fa1\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u8a55\u4fa1\u306f\u6b63\u78ba\u3055\u3068\u6709\u7528\u6027\u3092\u8003\u616e\u3059\u3079\u304d\u3067\u3059\u3002\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u306e\u8a00\u8a9e\u306f\u3001\u30e6\u30fc\u30b6\u304c\u4f7f\u7528\u3057\u3066\u3044\u308b\u8a00\u8a9e\u3068\u4e00\u81f4\u3057\u3066\u3044\u308b\u3079\u304d\u3067\u3001\u305d\u3046\u3067\u306a\u3044\u5834\u5408\u306f\u6e1b\u70b9\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002\u53c2\u7167\u56de\u7b54\u3068\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u304c\u4e0e\u3048\u3089\u308c\u307e\u3059\u3002\u3042\u306a\u305f\u306e\u8a55\u4fa1\u306f\u3001\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u3068\u53c2\u7167\u56de\u7b54\u3092\u6bd4\u8f03\u3059\u308b\u3053\u3068\u304b\u3089\u59cb\u3081\u3066\u304f\u3060\u3055\u3044\u3002\u30df\u30b9\u3092\u7279\u5b9a\u3057\u3001\u8a02\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u3067\u304d\u308b\u3060\u3051\u5ba2\u89b3\u7684\u3067\u3042\u308b\u3053\u3068\u3002\u8a55\u4fa1\u306e\u8aac\u660e\u3092\u3057\u305f\u5f8c\u3001\"[[rating]]\"\u3068\u3044\u3046\u5f62\u5f0f\u3067\u30011\u304b\u308910\u307e\u3067\u306e\u6574\u6570\u306e\u8a55\u4fa1\u5024\u3092\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\uff08\u4f8b \"rating\uff1a[[5]]\"\uff09\u3002\n          {%- else -%}\n          \u4ee5\u4e0b\u306b\u8868\u793a\u3055\u308c\u308b\u30e6\u30fc\u30b6\u306e\u8cea\u554f\u306b\u5bfe\u3059\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u5fdc\u7b54\u306e\u54c1\u8cea\u3092\u516c\u5e73\u306b\u8a55\u4fa1\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u8a55\u4fa1\u306f\u3001\u5fdc\u7b54\u306e\u6709\u7528\u6027\u3001\u95a2\u9023\u6027\u3001\u6b63\u78ba\u6027\u3001\u6df1\u3055\u3001\u5275\u9020\u6027\u3001\u8a73\u7d30\u5ea6\u306a\u3069\u306e\u8981\u7d20\u3092\u8003\u616e\u3059\u3079\u304d\u3067\u3059\u3002\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u306e\u8a00\u8a9e\u306f\u3001\u30e6\u30fc\u30b6\u304c\u4f7f\u7528\u3057\u3066\u3044\u308b\u8a00\u8a9e\u3068\u4e00\u81f4\u3057\u3066\u3044\u308b\u3079\u304d\u3067\u3001\u305d\u3046\u3067\u306a\u3044\u5834\u5408\u306f\u6e1b\u70b9\u3055\u308c\u308b\u3079\u304d\u3067\u3059\u3002\u8a55\u4fa1\u306f\u77ed\u3044\u8aac\u660e\u304b\u3089\u59cb\u3081\u3066\u304f\u3060\u3055\u3044\u3002\u3067\u304d\u308b\u3060\u3051\u5ba2\u89b3\u7684\u3067\u3042\u308b\u3053\u3068\u3002\u8a55\u4fa1\u306e\u8aac\u660e\u3092\u3057\u305f\u5f8c\u3001\"[[rating]]\"\u3068\u3044\u3046\u5f62\u5f0f\u3067\u30011\u304b\u308910\u307e\u3067\u306e\u6574\u6570\u306e\u8a55\u4fa1\u5024\u3092\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\uff08\u4f8b \"rating\uff1a[[5]]\"\uff09\u3002\n          {%- endif %}\n\n          [\u30e6\u30fc\u30b6\u306e\u8cea\u554f]\n          {{ messages[0][\"content\"] }}\n\n          {% if references|length &gt; 0 -%}\n          [\u53c2\u8003\u56de\u7b54\u306e\u958b\u59cb]\n          {{ references[0] }}\n          [\u53c2\u8003\u56de\u7b54\u306e\u7d42\u4e86]\n          {% endif -%}\n          [\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u958b\u59cb]\n          {% if messages|length == 1 %}{{ lm_output }}{% else %}{{ messages[1][\"content\"] }}{% endif %}\n          [\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u7d42\u4e86]\n        |||, '\\n'),\n      },\n    },\n    system_message: '\u3042\u306a\u305f\u306f\u512a\u79c0\u306a\u52a9\u624b\u3067\u3059\u3002',\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/Metric/#elyza_tasks_100_eval","title":"elyza_tasks_100_eval","text":"<p>This is a config of the ChatLLMScore class designed to evaluate chat assistants with ELYZA-tasks-100. The template is adapted from the blog post ELYZA\u304c\u516c\u958b\u3057\u305f\u65e5\u672c\u8a9eLLM\u300cELYZA-japanese-Llama-2-7b\u300d\u306b\u3064\u3044\u3066\u306e\u89e3\u8aac : (2) \u8a55\u4fa1\u7de8. <pre><code>{\n  class_path: 'ChatLLMScore',\n  init_args: {\n    language_model: { class_path: 'OpenAIChatAPI', init_args: { model: 'gpt-4-turbo-2024-04-09' } },\n    valid_score_range: [1, 5],\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: std.stripChars(|||\n          \u3042\u306a\u305f\u306f\u63a1\u70b9\u8005\u3067\u3059\u3002\n\n          \u554f\u984c, \u6b63\u89e3\u4f8b, \u63a1\u70b9\u57fa\u6e96, \u56de\u7b54 \u304c\u4e0e\u3048\u3089\u308c\u307e\u3059\u3002\n\n          \u63a1\u70b9\u57fa\u6e96\u3068\u6b63\u89e3\u4f8b\u3092\u53c2\u8003\u306b\u3057\u3066\u3001\u56de\u7b54\u30921,2,3,4,5\u306e5\u6bb5\u968e\u3067\u63a1\u70b9\u3057\u3001\u6570\u5b57\u306e\u307f\u3092\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n          # \u554f\u984c\n          {{ messages[-1][\"content\"] }}\n\n          # \u6b63\u89e3\u4f8b\n          {{ references[0] }}\n\n          # \u63a1\u70b9\u57fa\u6e96\n          \u57fa\u672c\u7684\u306a\u63a1\u70b9\u57fa\u6e96\n          - 1\u70b9: \u8aa4\u3063\u3066\u3044\u308b\u3001 \u6307\u793a\u306b\u5f93\u3048\u3066\u3044\u306a\u3044\n          - 2\u70b9: \u8aa4\u3063\u3066\u3044\u308b\u304c\u3001\u65b9\u5411\u6027\u306f\u5408\u3063\u3066\u3044\u308b\n          - 3\u70b9: \u90e8\u5206\u7684\u306b\u8aa4\u3063\u3066\u3044\u308b\u3001 \u90e8\u5206\u7684\u306b\u5408\u3063\u3066\u3044\u308b\n          - 4\u70b9: \u5408\u3063\u3066\u3044\u308b\n          - 5\u70b9: \u5f79\u306b\u7acb\u3064\n\n          \u57fa\u672c\u7684\u306a\u6e1b\u70b9\u9805\u76ee\n          - \u4e0d\u81ea\u7136\u306a\u65e5\u672c\u8a9e: -1\u70b9\n          - \u90e8\u5206\u7684\u306b\u4e8b\u5b9f\u3068\u7570\u306a\u308b\u5185\u5bb9\u3092\u8ff0\u3079\u3066\u3044\u308b: -1\u70b9\n          - \u300c\u502b\u7406\u7684\u306b\u7b54\u3048\u3089\u308c\u307e\u305b\u3093\u300d\u306e\u3088\u3046\u306b\u904e\u5ea6\u306b\u5b89\u5168\u6027\u3092\u6c17\u306b\u3057\u3066\u3057\u307e\u3063\u3066\u3044\u308b: 2\u70b9\u306b\u3059\u308b\n\n          \u554f\u984c\u56fa\u6709\u306e\u63a1\u70b9\u57fa\u6e96\n          {{ eval_aspect }}\n\n          # \u56de\u7b54\n          {{ lm_output }}\n        |||, '\\n'),\n      },\n    },\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/PairwiseJudge/","title":"PairwiseJudge","text":""},{"location":"preset_configs/PairwiseJudge/#assistant_judge_en_single_turn","title":"assistant_judge_en_single_turn","text":"<p>This is a configuration for evaluting the quality of responses generated by an AI assistant. Originally used to generate scores for MT-bench or Vicuna-bench.</p> <p>Adapted from lm-sys/FastChat. <pre><code>{\n  class_path: 'ChatLLMPairwiseJudge',\n  init_args: {\n    language_model: { class_path: 'OpenAIChatAPI', init_args: { model: 'gpt-4-turbo-2024-04-09' } },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: std.stripChars(|||\n          {% set question = model1_item[\"extra_info\"][\"messages\"][0][\"content\"] -%}\n          {% set model1_messages = model1_item[\"extra_info\"][\"messages\"] -%}\n          {% set model2_messages = model2_item[\"extra_info\"][\"messages\"] -%}\n          [Instruction]\n          {% if references|length &gt; 0 -%}\n          Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer, assistant A's answer, and assistant B's answer. Your job is to evaluate which assistant's answer is better. Begin your evaluation by comparing both assistants' answers with the reference answer. Identify and correct any mistakes. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[1]]\" if assistant 1 is better, \"[[2]]\" if assistant 2 is better, and \"[[3]]\" for a tie.\n          {%- else -%}\n          Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[1]]\" if assistant 1 is better, \"[[2]]\" if assistant 2 is better, and \"[[3]]\" for a tie.\n          {%- endif %}\n\n          [Question]\n          {{ question }}\n\n          {% if references|length &gt; 0 -%}\n          [The Start of Reference Answer]\n          {{ references[0] }}\n          [The End of Reference Answer]\n          {% endif -%}\n          [The Start of Assistant 1's Answer]\n          {% if model1_messages|length == 1 %}{{ model1_item[\"lm_output\"] }}{% else %}{{ model1_messages[1][\"content\"] }}{% endif %}\n          [The End of Assistant's Answer]\n          [The Start of Assistant 2's Answer]\n          {% if model2_messages|length == 1 %}{{ model2_item[\"lm_output\"] }}{% else %}{{ model2_messages[1][\"content\"] }}{% endif %}\n          [The End of Assistant's Answer]\n        |||, '\\n'),\n      },\n    },\n  },\n}\n</code></pre></p>"},{"location":"preset_configs/PairwiseJudge/#assistant_judge_ja_single_turn","title":"assistant_judge_ja_single_turn","text":"<p>This is a configuration for evaluting the quality of responses generated by an AI assistant. Originally used to generate scores for the Japanese versions of MT-bench or Vicuna-bench.</p> <p>Translated and adapted from lm-sys/FastChat. <pre><code>{\n  class_path: 'ChatLLMPairwiseJudge',\n  init_args: {\n    language_model: { class_path: 'OpenAIChatAPI', init_args: { model: 'gpt-4-turbo-2024-04-09' } },\n    prompt_template: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: std.stripChars(|||\n          {% set question = model1_item[\"extra_info\"][\"messages\"][0][\"content\"] -%}\n          {% set model1_messages = model1_item[\"extra_info\"][\"messages\"] -%}\n          {% set model2_messages = model2_item[\"extra_info\"][\"messages\"] -%}\n\n          [\u30e6\u30fc\u30b6\u306e\u8cea\u554f]\n          {{ question }}\n\n          {% if references|length &gt; 0 -%}\n          [\u53c2\u8003\u56de\u7b54\u306e\u958b\u59cb]\n          {{ references[0] }}\n          [\u53c2\u8003\u56de\u7b54\u306e\u7d42\u4e86]\n          {% endif -%}\n          [\u30a2\u30b7\u30b9\u30bf\u30f3\u30c81\u306e\u56de\u7b54\u958b\u59cb]\n          {% if model1_messages|length == 1 %}{{ model1_item[\"lm_output\"] }}{% else %}{{ model1_messages[1][\"content\"] }}{% endif %}\n          [\u30a2\u30b7\u30b9\u30bf\u30f3\u30c81\u306e\u56de\u7b54\u7d42\u4e86]\n          [\u30a2\u30b7\u30b9\u30bf\u30f3\u30c82\u306e\u56de\u7b54\u958b\u59cb]\n          {% if model2_messages|length == 1 %}{{ model2_item[\"lm_output\"] }}{% else %}{{ model2_messages[1][\"content\"] }}{% endif %}\n          [\u30a2\u30b7\u30b9\u30bf\u30f3\u30c82\u306e\u56de\u7b54\u7d42\u4e86]\n        |||, '\\n'),\n      },\n    },\n    system_message: {\n      class_path: 'Jinja2PromptTemplate',\n      init_args: {\n        template: std.stripChars(|||\n          {% if references|length &gt; 0 -%}\n          \u3042\u306a\u305f\u306f\u3001\u56de\u7b54\u306e\u8cea\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u305f\u3081\u306e\u5be9\u5224\u54e1\u3067\u3059\u3002\u4ee5\u4e0b\u306b\u793a\u3055\u308c\u308b\u30e6\u30fc\u30b6\u30fc\u306e\u8cea\u554f\u306b\u5bfe\u3059\u308b2\u3064\u306eAI\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u5fdc\u7b54\u306e\u54c1\u8cea\u3092\u8a55\u4fa1\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u5185\u5bb9\u304c\u30e6\u30fc\u30b6\u30fc\u306e\u6307\u793a\u306b\u5f93\u3063\u3066\u304a\u308a\u3001\u30e6\u30fc\u30b6\u30fc\u306e\u8cea\u554f\u306b\u3088\u308a\u3088\u304f\u7b54\u3048\u3066\u3044\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3092\u9078\u3093\u3067\u304f\u3060\u3055\u3044\u3002\u53c2\u7167\u56de\u7b54\u3001\u30a2\u30b7\u30b9\u30bf\u30f3\u30c81\u306e\u56de\u7b54\u3001\u30a2\u30b7\u30b9\u30bf\u30f3\u30c82\u306e\u56de\u7b54\u304c\u4e0e\u3048\u3089\u308c\u308b\u306e\u3067\u3001\u3069\u3061\u3089\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u304c\u512a\u308c\u3066\u3044\u308b\u304b\u3092\u8a55\u4fa1\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u8a55\u4fa1\u306e\u969b\u306b\u306f\u3001\u307e\u305a\u305d\u308c\u305e\u308c\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u56de\u7b54\u3092\u53c2\u7167\u56de\u7b54\u3068\u6bd4\u8f03\u3057\u3001\u56de\u7b54\u306e\u8aa4\u308a\u3092\u898b\u3064\u3051\u3066\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u7acb\u5834\u304c\u504f\u3089\u306a\u3044\u3088\u3046\u306b\u3057\u3001\u56de\u7b54\u306e\u63d0\u793a\u9806\u304c\u3042\u306a\u305f\u306e\u5224\u65ad\u306b\u5f71\u97ff\u3057\u306a\u3044\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u9577\u3055\u304c\u8a55\u4fa1\u306b\u5f71\u97ff\u3057\u306a\u3044\u3053\u3068\u3001\u7279\u5b9a\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u540d\u524d\u3092\u597d\u307e\u306a\u3044\u3053\u3068\u3001\u3067\u304d\u308b\u3060\u3051\u5ba2\u89b3\u7684\u3067\u3042\u308b\u3053\u3068\u3001\u306b\u6c17\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\u3002\u8aac\u660e\u306e\u5f8c\u306b\u3001\u6700\u7d42\u7684\u306a\u5224\u65ad\u3092\u4ee5\u4e0b\u306e\u5f62\u5f0f\u306b\u5f93\u3063\u3066\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\uff1a\u30a2\u30b7\u30b9\u30bf\u30f3\u30c81\u304c\u512a\u308c\u3066\u3044\u308c\u3070[[1]]\u3001\u30a2\u30b7\u30b9\u30bf\u30f3\u30c82\u304c\u512a\u308c\u3066\u3044\u308c\u3070[[2]]\u3001\u540c\u70b9\u306e\u5834\u5408\u306f[[3]]\n          {%- else -%}\n          \u3042\u306a\u305f\u306f\u3001\u56de\u7b54\u306e\u8cea\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u305f\u3081\u306e\u5be9\u5224\u54e1\u3067\u3059\u3002\u4ee5\u4e0b\u306b\u793a\u3055\u308c\u308b\u30e6\u30fc\u30b6\u30fc\u306e\u8cea\u554f\u306b\u5bfe\u3059\u308b2\u3064\u306eAI\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u5fdc\u7b54\u306e\u54c1\u8cea\u3092\u8a55\u4fa1\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u5185\u5bb9\u304c\u30e6\u30fc\u30b6\u30fc\u306e\u6307\u793a\u306b\u5f93\u3063\u3066\u304a\u308a\u3001\u30e6\u30fc\u30b6\u30fc\u306e\u8cea\u554f\u306b\u3088\u308a\u3088\u304f\u7b54\u3048\u3066\u3044\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3092\u9078\u3093\u3067\u304f\u3060\u3055\u3044\u3002\u5177\u4f53\u7684\u306b\u306f\u3001\u56de\u7b54\u306e\u6709\u7528\u6027\u3001\u95a2\u9023\u6027\u3001\u6b63\u78ba\u6027\u3001\u6df1\u3055\u3001\u5275\u9020\u6027\u3001\u8a73\u7d30\u30ec\u30d9\u30eb\u306a\u3069\u306e\u8981\u7d20\u3092\u8003\u616e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u8a55\u4fa1\u306e\u969b\u306b\u306f\u3001\u307e\u305a2\u3064\u306e\u56de\u7b54\u3092\u6bd4\u8f03\u3057\u3001\u7c21\u5358\u306a\u8aac\u660e\u3092\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u7acb\u5834\u304c\u504f\u3089\u306a\u3044\u3088\u3046\u306b\u3057\u3001\u56de\u7b54\u306e\u63d0\u793a\u9806\u304c\u3042\u306a\u305f\u306e\u5224\u65ad\u306b\u5f71\u97ff\u3057\u306a\u3044\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u56de\u7b54\u306e\u9577\u3055\u304c\u8a55\u4fa1\u306b\u5f71\u97ff\u3057\u306a\u3044\u3053\u3068\u3001\u7279\u5b9a\u306e\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u540d\u524d\u3092\u597d\u307e\u306a\u3044\u3053\u3068\u3001\u3067\u304d\u308b\u3060\u3051\u5ba2\u89b3\u7684\u3067\u3042\u308b\u3053\u3068\u3001\u306b\u6c17\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\u3002\u8aac\u660e\u306e\u5f8c\u306b\u3001\u6700\u7d42\u7684\u306a\u5224\u65ad\u3092\u4ee5\u4e0b\u306e\u5f62\u5f0f\u306b\u5f93\u3063\u3066\u51fa\u529b\u3057\u3066\u304f\u3060\u3055\u3044\uff1a\u30a2\u30b7\u30b9\u30bf\u30f3\u30c81\u304c\u512a\u308c\u3066\u3044\u308c\u3070[[1]]\u3001\u30a2\u30b7\u30b9\u30bf\u30f3\u30c82\u304c\u512a\u308c\u3066\u3044\u308c\u3070[[2]]\u3001\u540c\u70b9\u306e\u5834\u5408\u306f[[3]]\n          {%- endif %}\n        |||, '\\n'),\n      },\n    },\n  },\n}\n</code></pre></p>"}]}